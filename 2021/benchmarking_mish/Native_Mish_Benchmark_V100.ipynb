{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  9 23:01:03 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 194kB 15.4MB/s \n",
      "\u001b[?25h  Building wheel for mish-cuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install fastcore --upgrade -qq\n",
    "! pip install fastai --upgrade -qq\n",
    "! pip install git+https://github.com/thomasbrandon/mish-cuda -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import fastai\n",
    "from sys import exit\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from mish_cuda import MishCudaFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(val, spec=\"#0.4G\"):\n",
    "    PREFIXES = np.array([c for c in u\"yzafpnµm kMGTPEZY\"])\n",
    "    exp = np.int8(np.log10(np.abs(val)) // 3 * 3 * np.sign(val))\n",
    "    val /= 10.**exp\n",
    "    prefix = PREFIXES[exp//3 + len(PREFIXES)//2]\n",
    "    return f\"{val:{spec}}{prefix}\"\n",
    "\n",
    "def display_times(times):\n",
    "    return f\"{scale(times.mean())}s ± {scale(times.std())}s, {scale(times.min())}s, {scale(times.max())}s\"\n",
    "\n",
    "def profile_cuda(func, inp, n_repeat=100, warmup=10):\n",
    "    fwd_times,bwd_times = [],[]\n",
    "    for i in range(n_repeat + warmup):\n",
    "        start,end = (torch.cuda.Event(enable_timing=True) for _ in range(2))\n",
    "        start.record()\n",
    "        res = func(inp)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        if i >= warmup: fwd_times.append(start.elapsed_time(end))\n",
    "        start,end = (torch.cuda.Event(enable_timing=True) for _ in range(2))\n",
    "        inp = inp.clone().requires_grad_()\n",
    "        y = func(inp)\n",
    "        l = y.mean()\n",
    "        start.record()\n",
    "        _ = torch.autograd.grad(l, inp)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        if i >= warmup: bwd_times.append(start.elapsed_time(end))\n",
    "    return (np.array(fwd_times)/1000, # Elapsed time is in ms\n",
    "            np.array(bwd_times)/1000)\n",
    "\n",
    "mish_pt = lambda x: x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "def profile(device='cuda', n_repeat=100, warmup=10, size='(16,10,256,256)', baseline=True, types='all'):\n",
    "    if types == 'all': \n",
    "        dtypes = [torch.float16, torch.bfloat16, torch.float32, torch.float64]\n",
    "    else:\n",
    "        if not hasattr(torch, types): exit(\"Invalid data type, expected torch type or 'all', got {types}\")\n",
    "        dtypes = [getattr(torch, types)]\n",
    "    dev = torch.device(type=device)\n",
    "    sz_str = size.replace(' ','')\n",
    "    if not re.match(r\"[\\(\\[]\\d+(,\\d+)*[\\)\\]]\", sz_str):\n",
    "        exit(\"Badly formatted size, should be a list or tuple such as \\\"(1,2,3)\\\".\")\n",
    "    sz = list(map(int, sz_str[1:-1].split(',')))\n",
    "    print(f\"Profiling over {n_repeat} runs after {warmup} warmup runs.\")\n",
    "    for dtype in dtypes:\n",
    "        if len(dtypes) > 1:\n",
    "            print(f\"Testing on {dtype}:\")\n",
    "            ind = ' '\n",
    "        else: ind = ''\n",
    "        inp = torch.randn(*sz, dtype=dtype, device=dev)\n",
    "        timings = []\n",
    "        funcs = {}\n",
    "        funcs.update(relu = torch.nn.functional.relu, \n",
    "                     leaky_relu = torch.nn.functional.leaky_relu,\n",
    "                     softplus = torch.nn.functional.softplus,\n",
    "                     silu_jit = fastai.layers.swish,\n",
    "                     silu_native = torch.nn.functional.silu,\n",
    "                     mish_naive = mish_pt,\n",
    "                     mish_jit = fastai.layers.mish,\n",
    "                     mish_cuda = MishCudaFunction.apply,\n",
    "                     mish_native = torch.nn.functional.mish)\n",
    "        if device=='cpu': funcs.pop('mish_cuda')\n",
    "        max_name = max(map(len, funcs.keys())) + 6\n",
    "        for (name,func) in funcs.items():\n",
    "            if device=='cuda':\n",
    "                if (name=='mish_cuda') and (dtype==torch.bfloat16):\n",
    "                    pass\n",
    "                else: fwd_times,bwd_times = profile_cuda(func, inp, n_repeat, warmup)\n",
    "            print(ind+(name+'_fwd:').ljust(max_name) + display_times(fwd_times))\n",
    "            print(ind+(name+'_bwd:').ljust(max_name) + display_times(bwd_times))\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling over 100 runs after 10 warmup runs.\n",
      "Testing on torch.float16:\n",
      " relu_fwd:        100.4µs ± 4.316µs, 96.26µs, 116.7µs\n",
      " relu_bwd:        179.6µs ± 16.79µs, 161.8µs, 301.1µs\n",
      " leaky_relu_fwd:  95.32µs ± 3.966µs, 92.16µs, 120.8µs\n",
      " leaky_relu_bwd:  178.8µs ± 28.97µs, 158.7µs, 323.6µs\n",
      " softplus_fwd:    105.4µs ± 7.069µs, 100.4µs, 151.6µs\n",
      " softplus_bwd:    237.3µs ± 378.3µs, 171.0µs, 3.813ms\n",
      " silu_jit_fwd:    114.3µs ± 62.76µs, 99.33µs, 638.0µs\n",
      " silu_jit_bwd:    249.7µs ± 288.3µs, 169.0µs, 2.765ms\n",
      " silu_native_fwd: 86.78µs ± 2.632µs, 83.97µs, 100.4µs\n",
      " silu_native_bwd: 158.8µs ± 12.15µs, 147.5µs, 196.6µs\n",
      " mish_naive_fwd:  239.4µs ± 7.391µs, 232.4µs, 272.4µs\n",
      " mish_naive_bwd:  502.3µs ± 341.5µs, 463.9µs, 3.900ms\n",
      " mish_jit_fwd:    187.0µs ± 4.837µs, 182.3µs, 204.8µs\n",
      " mish_jit_bwd:    254.5µs ± 9.229µs, 248.8µs, 328.7µs\n",
      " mish_cuda_fwd:   116.1µs ± 5.289µs, 107.5µs, 136.2µs\n",
      " mish_cuda_bwd:   185.1µs ± 21.07µs, 169.0µs, 291.8µs\n",
      " mish_native_fwd: 84.57µs ± 3.283µs, 81.92µs, 101.4µs\n",
      " mish_native_bwd: 168.2µs ± 16.51µs, 155.6µs, 271.4µs\n",
      "Testing on torch.bfloat16:\n",
      " relu_fwd:        94.56µs ± 65.80µs, 83.97µs, 747.5µs\n",
      " relu_bwd:        197.6µs ± 246.0µs, 143.4µs, 1.988ms\n",
      " leaky_relu_fwd:  84.14µs ± 5.659µs, 80.90µs, 116.7µs\n",
      " leaky_relu_bwd:  155.2µs ± 13.22µs, 143.4µs, 199.7µs\n",
      " softplus_fwd:    97.58µs ± 3.035µs, 95.23µs, 109.6µs\n",
      " softplus_bwd:    152.3µs ± 13.03µs, 146.4µs, 261.1µs\n",
      " silu_jit_fwd:    178.3µs ± 7.874µs, 172.0µs, 213.0µs\n",
      " silu_jit_bwd:    598.2µs ± 412.6µs, 542.7µs, 4.677ms\n",
      " silu_native_fwd: 84.90µs ± 4.708µs, 80.90µs, 113.7µs\n",
      " silu_native_bwd: 187.3µs ± 162.1µs, 144.4µs, 1.535ms\n",
      " mish_naive_fwd:  242.1µs ± 5.280µs, 236.5µs, 262.1µs\n",
      " mish_naive_bwd:  472.0µs ± 70.70µs, 458.8µs, 1.011ms\n",
      " mish_jit_fwd:    258.8µs ± 39.72µs, 247.8µs, 647.2µs\n",
      " mish_jit_bwd:    796.5µs ± 265.8µs, 758.8µs, 3.290ms\n",
      " mish_cuda_fwd:   258.8µs ± 39.72µs, 247.8µs, 647.2µs\n",
      " mish_cuda_bwd:   796.5µs ± 265.8µs, 758.8µs, 3.290ms\n",
      " mish_native_fwd: 87.34µs ± 5.521µs, 82.94µs, 119.8µs\n",
      " mish_native_bwd: 170.9µs ± 21.28µs, 154.6µs, 332.8µs\n",
      "Testing on torch.float32:\n",
      " relu_fwd:        132.7µs ± 5.097µs, 128.0µs, 149.5µs\n",
      " relu_bwd:        235.3µs ± 124.5µs, 216.1µs, 1.331ms\n",
      " leaky_relu_fwd:  134.6µs ± 53.66µs, 123.9µs, 663.6µs\n",
      " leaky_relu_bwd:  291.6µs ± 416.3µs, 216.1µs, 3.420ms\n",
      " softplus_fwd:    127.1µs ± 3.866µs, 123.9µs, 144.4µs\n",
      " softplus_bwd:    219.1µs ± 3.852µs, 217.1µs, 254.0µs\n",
      " silu_jit_fwd:    305.7µs ± 101.5µs, 288.8µs, 1.312ms\n",
      " silu_jit_bwd:    974.2µs ± 201.2µs, 932.9µs, 2.338ms\n",
      " silu_native_fwd: 127.4µs ± 4.018µs, 122.9µs, 144.4µs\n",
      " silu_native_bwd: 217.9µs ± 754.2ns, 216.1µs, 220.2µs\n",
      " mish_naive_fwd:  396.4µs ± 71.03µs, 382.0µs, 1.080ms\n",
      " mish_naive_bwd:  912.2µs ± 748.6µs, 819.2µs, 8.176ms\n",
      " mish_jit_fwd:    408.6µs ± 40.85µs, 395.3µs, 805.9µs\n",
      " mish_jit_bwd:    1.410ms ± 846.5µs, 1.294ms, 9.495ms\n",
      " mish_cuda_fwd:   141.0µs ± 4.121µs, 135.2µs, 158.7µs\n",
      " mish_cuda_bwd:   235.6µs ± 77.05µs, 222.2µs, 969.7µs\n",
      " mish_native_fwd: 128.3µs ± 2.898µs, 124.9µs, 139.3µs\n",
      " mish_native_bwd: 220.5µs ± 952.9ns, 219.1µs, 224.3µs\n",
      "Testing on torch.float64:\n",
      " relu_fwd:        324.0µs ± 4.909µs, 318.5µs, 341.0µs\n",
      " relu_bwd:        492.1µs ± 170.1µs, 470.0µs, 2.169ms\n",
      " leaky_relu_fwd:  319.6µs ± 4.031µs, 314.4µs, 334.8µs\n",
      " leaky_relu_bwd:  472.4µs ± 2.247µs, 469.0µs, 485.4µs\n",
      " softplus_fwd:    312.7µs ± 4.179µs, 307.2µs, 329.7µs\n",
      " softplus_bwd:    470.3µs ± 1.141µs, 468.0µs, 473.1µs\n",
      " silu_jit_fwd:    722.5µs ± 9.100µs, 712.7µs, 756.7µs\n",
      " silu_jit_bwd:    2.447ms ± 172.7µs, 2.416ms, 4.107ms\n",
      " silu_native_fwd: 316.3µs ± 5.669µs, 310.3µs, 348.2µs\n",
      " silu_native_bwd: 473.1µs ± 1.782µs, 470.0µs, 482.3µs\n",
      " mish_naive_fwd:  995.5µs ± 18.37µs, 985.1µs, 1.165ms\n",
      " mish_naive_bwd:  1.999ms ± 79.57µs, 1.979ms, 2.789ms\n",
      " mish_jit_fwd:    1.009ms ± 10.14µs, 997.4µs, 1.050ms\n",
      " mish_jit_bwd:    3.405ms ± 178.1µs, 3.374ms, 5.177ms\n",
      " mish_cuda_fwd:   344.1µs ± 5.212µs, 340.0µs, 371.7µs\n",
      " mish_cuda_bwd:   470.1µs ± 1.002µs, 469.0µs, 476.2µs\n",
      " mish_native_fwd: 334.5µs ± 4.198µs, 328.7µs, 348.2µs\n",
      " mish_native_bwd: 479.5µs ± 1.436µs, 478.2µs, 489.5µs\n"
     ]
    }
   ],
   "source": [
    "profile('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling over 100 runs after 10 warmup runs.\n",
      "Testing on torch.float16:\n",
      " relu_fwd:        301.5µs ± 5.669µs, 295.9µs, 327.7µs\n",
      " relu_bwd:        607.6µs ± 291.4µs, 576.5µs, 3.507ms\n",
      " leaky_relu_fwd:  269.9µs ± 4.267µs, 259.1µs, 282.6µs\n",
      " leaky_relu_bwd:  575.4µs ± 237.3µs, 540.7µs, 2.936ms\n",
      " softplus_fwd:    294.8µs ± 3.801µs, 290.8µs, 314.4µs\n",
      " softplus_bwd:    549.6µs ± 1.638µs, 546.8µs, 555.0µs\n",
      " silu_jit_fwd:    296.4µs ± 8.637µs, 288.8µs, 337.9µs\n",
      " silu_jit_bwd:    593.3µs ± 200.4µs, 558.1µs, 2.018ms\n",
      " silu_native_fwd: 263.1µs ± 17.87µs, 256.0µs, 433.2µs\n",
      " silu_native_bwd: 542.7µs ± 927.0ns, 540.7µs, 544.8µs\n",
      " mish_naive_fwd:  834.5µs ± 4.775µs, 829.4µs, 858.1µs\n",
      " mish_naive_bwd:  1.753ms ± 7.297µs, 1.749ms, 1.824ms\n",
      " mish_jit_fwd:    630.8µs ± 8.030µs, 620.5µs, 661.5µs\n",
      " mish_jit_bwd:    950.3µs ± 14.23µs, 930.8µs, 1.042ms\n",
      " mish_cuda_fwd:   371.4µs ± 6.599µs, 365.6µs, 403.5µs\n",
      " mish_cuda_bwd:   652.3µs ± 30.78µs, 642.0µs, 913.4µs\n",
      " mish_native_fwd: 261.6µs ± 4.253µs, 257.0µs, 283.6µs\n",
      " mish_native_bwd: 587.7µs ± 1.720µs, 583.7µs, 591.9µs\n",
      "Testing on torch.bfloat16:\n",
      " relu_fwd:        264.2µs ± 6.080µs, 259.1µs, 289.8µs\n",
      " relu_bwd:        541.3µs ± 1.738µs, 539.6µs, 553.0µs\n",
      " leaky_relu_fwd:  261.1µs ± 8.169µs, 256.0µs, 323.6µs\n",
      " leaky_relu_bwd:  550.6µs ± 62.12µs, 540.7µs, 1.092ms\n",
      " softplus_fwd:    318.4µs ± 4.707µs, 314.4µs, 335.9µs\n",
      " softplus_bwd:    610.9µs ± 374.5µs, 546.8µs, 3.997ms\n",
      " silu_jit_fwd:    577.5µs ± 12.06µs, 570.4µs, 676.9µs\n",
      " silu_jit_bwd:    2.124ms ± 324.6µs, 2.079ms, 5.055ms\n",
      " silu_native_fwd: 262.8µs ± 10.86µs, 256.0µs, 362.5µs\n",
      " silu_native_bwd: 566.5µs ± 146.8µs, 541.7µs, 1.763ms\n",
      " mish_naive_fwd:  859.4µs ± 6.813µs, 854.0µs, 895.0µs\n",
      " mish_naive_bwd:  1.759ms ± 4.800µs, 1.752ms, 1.770ms\n",
      " mish_jit_fwd:    874.7µs ± 19.50µs, 865.3µs, 1.059ms\n",
      " mish_jit_bwd:    2.918ms ± 42.62µs, 2.910ms, 3.256ms\n",
      " mish_cuda_fwd:   874.7µs ± 19.50µs, 865.3µs, 1.059ms\n",
      " mish_cuda_bwd:   2.918ms ± 42.62µs, 2.910ms, 3.256ms\n",
      " mish_native_fwd: 268.2µs ± 7.362µs, 262.1µs, 303.1µs\n",
      " mish_native_bwd: 591.1µs ± 56.84µs, 580.6µs, 1.119ms\n",
      "Testing on torch.float32:\n",
      " relu_fwd:        440.9µs ± 8.715µs, 433.2µs, 481.3µs\n",
      " relu_bwd:        840.7µs ± 66.79µs, 832.5µs, 1.505ms\n",
      " leaky_relu_fwd:  439.9µs ± 23.01µs, 430.1µs, 657.4µs\n",
      " leaky_relu_bwd:  876.4µs ± 332.3µs, 832.5µs, 4.005ms\n",
      " softplus_fwd:    435.1µs ± 4.595µs, 431.1µs, 458.8µs\n",
      " softplus_bwd:    843.9µs ± 88.30µs, 833.5µs, 1.722ms\n",
      " silu_jit_fwd:    1.051ms ± 26.37µs, 1.038ms, 1.292ms\n",
      " silu_jit_bwd:    3.725ms ± 671.7µs, 3.654ms, 10.41ms\n",
      " silu_native_fwd: 436.3µs ± 26.32µs, 429.1µs, 694.3µs\n",
      " silu_native_bwd: 868.7µs ± 313.9µs, 832.5µs, 3.981ms\n",
      " mish_naive_fwd:  1.445ms ± 17.13µs, 1.436ms, 1.581ms\n",
      " mish_naive_bwd:  3.239ms ± 297.9µs, 3.207ms, 6.203ms\n",
      " mish_jit_fwd:    1.460ms ± 9.043µs, 1.450ms, 1.493ms\n",
      " mish_jit_bwd:    5.089ms ± 144.2µs, 5.067ms, 6.432ms\n",
      " mish_cuda_fwd:   467.5µs ± 5.641µs, 460.8µs, 487.4µs\n",
      " mish_cuda_bwd:   861.1µs ± 14.21µs, 854.0µs, 999.4µs\n",
      " mish_native_fwd: 441.1µs ± 23.93µs, 432.1µs, 651.3µs\n",
      " mish_native_bwd: 877.7µs ± 284.9µs, 835.6µs, 3.063ms\n",
      "Testing on torch.float64:\n",
      " relu_fwd:        1.198ms ± 5.685µs, 1.189ms, 1.218ms\n",
      " relu_bwd:        1.862ms ± 2.409µs, 1.858ms, 1.870ms\n",
      " leaky_relu_fwd:  1.198ms ± 7.886µs, 1.187ms, 1.227ms\n",
      " leaky_relu_bwd:  1.872ms ± 111.3µs, 1.853ms, 2.979ms\n",
      " softplus_fwd:    1.169ms ± 6.337µs, 1.160ms, 1.192ms\n",
      " softplus_bwd:    1.867ms ± 188.0µs, 1.843ms, 3.738ms\n",
      " silu_jit_fwd:    2.752ms ± 10.56µs, 2.731ms, 2.788ms\n",
      " silu_jit_bwd:    9.608ms ± 6.624µs, 9.592ms, 9.623ms\n",
      " silu_native_fwd: 1.186ms ± 6.202µs, 1.178ms, 1.205ms\n",
      " silu_native_bwd: 1.866ms ± 51.68µs, 1.855ms, 2.380ms\n",
      " mish_naive_fwd:  3.860ms ± 10.54µs, 3.842ms, 3.910ms\n",
      " mish_naive_bwd:  7.885ms ± 5.435µs, 7.873ms, 7.898ms\n",
      " mish_jit_fwd:    3.887ms ± 13.44µs, 3.864ms, 3.938ms\n",
      " mish_jit_bwd:    13.43ms ± 8.362µs, 13.41ms, 13.45ms\n",
      " mish_cuda_fwd:   1.271ms ± 5.997µs, 1.263ms, 1.290ms\n",
      " mish_cuda_bwd:   1.913ms ± 592.3µs, 1.842ms, 7.717ms\n",
      " mish_native_fwd: 1.237ms ± 7.004µs, 1.230ms, 1.266ms\n",
      " mish_native_bwd: 1.869ms ± 9.899µs, 1.866ms, 1.966ms\n"
     ]
    }
   ],
   "source": [
    "profile('cuda', size='(64,10,256,256)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
