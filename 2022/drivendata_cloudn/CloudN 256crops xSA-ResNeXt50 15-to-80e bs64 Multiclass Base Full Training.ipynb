{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.55.0-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 47.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba) (58.0.4)\n",
      "Requirement already satisfied: numpy<1.22,>=1.18 in /opt/conda/lib/python3.7/site-packages (from numba) (1.21.2)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.38.0 numba-0.55.0\n"
     ]
    }
   ],
   "source": [
    "! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastcore\n",
      "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting rasterio\n",
      "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.3 MB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting timm\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "\u001b[K     |████████████████████████████████| 431 kB 82.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastcore) (21.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastcore) (21.0)\n",
      "Collecting click>=4.0\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio) (21.2.0)\n",
      "Collecting affine\n",
      "  Downloading affine-2.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.21.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio) (58.0.4)\n",
      "Collecting click-plugins\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio) (2021.10.8)\n",
      "Collecting snuggs>=1.4.1\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=4.0->rasterio) (4.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio) (3.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.11.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.10.0.2)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Collecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.19.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 79.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.2-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 86.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.4.1)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 72.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Collecting termcolor<2.0.0,>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=4.0->rasterio) (3.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (8.4.0)\n",
      "Building wheels for collected packages: promise, subprocess32, termcolor, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=3864b80c5be370fd41114bb480bc4332d983f7c6b159b215acf6de8433965808\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=8ea56b3776e974e58f930924c582ad93b6678181ae37b230caa35108aa81b76b\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=42ee788a1f16068d8affb643b1e99215625852de8f37eccd66d703a2176ecde4\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=05357f158f6b982324af7fce7875e1affe39eaf8fe0c45716ca180efa8904765\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built promise subprocess32 termcolor pathtools\n",
      "Installing collected packages: smmap, termcolor, gitdb, click, yaspin, subprocess32, snuggs, shortuuid, sentry-sdk, protobuf, promise, pathtools, GitPython, docker-pycreds, configparser, cligj, click-plugins, affine, wandb, timm, rasterio, fastcore\n",
      "Successfully installed GitPython-3.1.26 affine-2.3.0 click-8.0.3 click-plugins-1.1.1 cligj-0.7.2 configparser-5.2.0 docker-pycreds-0.4.0 fastcore-1.3.27 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 protobuf-3.19.3 rasterio-1.2.10 sentry-sdk-1.5.2 shortuuid-1.0.8 smmap-5.0.0 snuggs-1.4.7 subprocess32-3.5.4 termcolor-1.1.0 timm-0.5.4 wandb-0.12.9 yaspin-2.1.0\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.6.2-py2.py3-none-any.whl (401 kB)\n",
      "\u001b[K     |████████████████████████████████| 401 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kornia\n",
      "Successfully installed kornia-0.6.2\n",
      "Processing ./fastai.zip\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (21.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (21.0)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: fastcore<1.4,>=1.3.27 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (1.3.27)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (0.11.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 59.4 MB/s eta 0:00:01\u001b[K     |████████████████████████████████| 11.3 MB 59.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (2.25.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (5.4.1)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (8.4.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 74.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy<4\n",
      "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 55.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch<1.11,>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.4) (1.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fastprogress>=0.2.4->fastai==2.5.4) (1.21.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 85.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
      "\u001b[K     |████████████████████████████████| 451 kB 114.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.4) (3.10.0.2)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.4) (2.11.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.4) (4.61.2)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
      "\u001b[K     |████████████████████████████████| 628 kB 53.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.4) (58.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai==2.5.4) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->fastai==2.5.4) (3.0.1)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.4) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.4) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.4) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.4) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai==2.5.4) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<4->fastai==2.5.4) (4.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<4->fastai==2.5.4) (2.0.1)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.5.4) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
      "\u001b[K     |████████████████████████████████| 890 kB 62.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->fastai==2.5.4) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai==2.5.4) (2021.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 45.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: fastai\n",
      "  Building wheel for fastai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastai: filename=fastai-2.5.4-py3-none-any.whl size=188643 sha256=01b839daf271666bc65662f5c2f0f08a997282506f79e77c694bf87d1b3bc542\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4cc_g9yl/wheels/13/5c/04/d01ab4f33d6f45bb2feb4c603b7675a73bd5a1f418de07ef1a\n",
      "Successfully built fastai\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, threadpoolctl, thinc, spacy-loggers, spacy-legacy, scipy, pathy, langcodes, kiwisolver, joblib, fonttools, fastprogress, cycler, spacy, scikit-learn, pandas, matplotlib, fastdownload, fastai\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cycler-0.11.0 cymem-2.0.6 fastai-2.5.4 fastdownload-0.0.5 fastprogress-1.0.0 fonttools-4.28.5 joblib-1.1.0 kiwisolver-1.3.2 langcodes-3.3.0 matplotlib-3.5.1 murmurhash-1.0.6 pandas-1.3.5 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 threadpoolctl-3.0.0 typer-0.4.0 wasabi-0.9.0\n",
      "Processing ./fastxtend.zip\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastxtend==0.0.3) (21.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastxtend==0.0.3) (21.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastxtend==0.0.3) (1.3.5)\n",
      "Requirement already satisfied: fastai>=2.5.1 in /opt/conda/lib/python3.7/site-packages (from fastxtend==0.0.3) (2.5.4)\n",
      "Requirement already satisfied: fastcore>=1.3.26 in /opt/conda/lib/python3.7/site-packages (from fastxtend==0.0.3) (1.3.27)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (1.0.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (3.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (2.25.1)\n",
      "Requirement already satisfied: spacy<4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (3.2.1)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (0.0.5)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (0.11.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (1.7.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (5.4.1)\n",
      "Requirement already satisfied: torch<1.11,>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (1.10.0)\n",
      "Requirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.5.1->fastxtend==0.0.3) (8.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fastprogress>=0.2.4->fastai>=2.5.1->fastxtend==0.0.3) (1.21.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (58.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (3.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (3.10.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (0.7.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (4.61.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->fastxtend==0.0.3) (3.0.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.5.1->fastxtend==0.0.3) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.5.1->fastxtend==0.0.3) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.5.1->fastxtend==0.0.3) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.5.1->fastxtend==0.0.3) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (4.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<4->fastai>=2.5.1->fastxtend==0.0.3) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.5.1->fastxtend==0.0.3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.5.1->fastxtend==0.0.3) (4.28.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.5.1->fastxtend==0.0.3) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.5.1->fastxtend==0.0.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->fastai>=2.5.1->fastxtend==0.0.3) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastxtend==0.0.3) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.5.1->fastxtend==0.0.3) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.5.1->fastxtend==0.0.3) (1.1.0)\n",
      "Building wheels for collected packages: fastxtend\n",
      "  Building wheel for fastxtend (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastxtend: filename=fastxtend-0.0.3-py3-none-any.whl size=17289 sha256=823a4cc8b463379488e36a24d2a830a9ff2eb3793f12640430a2a09ddd85ef69\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-21qicgdl/wheels/ad/31/5a/7bdfd44f597adce54ce3deaa6e27a02024e47fd57fe6bc749c\n",
      "Successfully built fastxtend\n",
      "Installing collected packages: fastxtend\n",
      "Successfully installed fastxtend-0.0.3\n"
     ]
    }
   ],
   "source": [
    "! pip install fastcore rasterio timm wandb --upgrade\n",
    "! pip install kornia --no-deps --upgrade\n",
    "! pip install fastai.zip\n",
    "! pip install fastxtend.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python        : 3.7.11\n",
      "fastai        : 2.5.4\n",
      "fastcore      : 1.3.27\n",
      "fastprogress  : 0.2.7\n",
      "torch         : 1.10.0\n",
      "nvidia driver : 495.46\n",
      "torch cuda    : 11.3 / is available\n",
      "torch cudnn   : 8200 / is enabled\n",
      "\n",
      "=== Hardware === \n",
      "nvidia gpus   : 1\n",
      "torch devices : 1\n",
      "  - gpu0      : NVIDIA GeForce RTX 3090\n",
      "\n",
      "=== Environment === \n",
      "platform      : Linux-5.11.0-46-generic-x86_64-with-debian-buster-sid\n",
      "distro        : #51~20.04.1-Ubuntu SMP Fri Jan 7 06:51:40 UTC 2022\n",
      "conda env     : Unknown\n",
      "python        : /opt/conda/bin/python\n",
      "sys.path      : /workspace\n",
      "/opt/conda/lib/python37.zip\n",
      "/opt/conda/lib/python3.7\n",
      "/opt/conda/lib/python3.7/lib-dynload\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages\n",
      "/opt/conda/lib/python3.7/site-packages/IPython/extensions\n",
      "/root/.ipython\n",
      "\n",
      "Mon Jan 17 23:24:34 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 73%   60C    P8    33W / 350W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n",
      "Optional package(s) to enhance the diagnostics can be installed with:\n",
      "pip install distro\n",
      "Once installed, re-run this utility to get the additional information\n"
     ]
    }
   ],
   "source": [
    "from fastai.test_utils import show_install\n",
    "show_install(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import rasterio\n",
    "import gc\n",
    "from numba import jit\n",
    "import torch.nn.functional as F\n",
    "from fastxtend.augment import tensor_item_tfm\n",
    "from fastxtend.schedulers import fit_flat_varied\n",
    "from fastxtend.utils.lessrandom import *\n",
    "from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Datablocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '' # Colab\n",
    "#PATH = 'data' # Test env\n",
    "\n",
    "LABELS       = f'{PATH}train_labels/'\n",
    "FEATURES     = f'{PATH}train_features/'\n",
    "if FULL:\n",
    "    NPY_LABELS   = f'{PATH}npy_labels/'\n",
    "    NPY_FEATURES = f'{PATH}npy_features/'\n",
    "else:\n",
    "    NPY_LABELS   = f'{PATH}sub_labels/'\n",
    "    NPY_FEATURES = f'{PATH}sub_features/'\n",
    "\n",
    "\n",
    "# from https://github.com/makepath/xarray-spatial/blob/master/xrspatial/multispectral.py\n",
    "@jit(nopython=True, nogil=True)\n",
    "def _normalize_data_cpu(data, min_val, max_val, pixel_max, c, th):\n",
    "    out = np.zeros_like(data)\n",
    "    out[:] = np.nan\n",
    "\n",
    "    range_val = max_val - min_val\n",
    "    rows, cols = data.shape\n",
    "\n",
    "    # check range_val to avoid dividing by zero\n",
    "    if range_val != 0:\n",
    "        for y in range(rows):\n",
    "            for x in range(cols):\n",
    "                val = data[y, x]\n",
    "                norm = (val - min_val) / range_val\n",
    "                # sigmoid contrast enhancement\n",
    "                norm = 1 / (1 + np.exp(c * (th - norm)))\n",
    "                out[y, x] = norm * pixel_max\n",
    "    return out\n",
    "\n",
    "def _normalize_data(data, pixel_max, c, th):\n",
    "    min_val = np.nanmin(data)\n",
    "    max_val = np.nanmax(data)\n",
    "    return _normalize_data_cpu(data, min_val, max_val, pixel_max, c, th)\n",
    "\n",
    "# end xarray-spatial code\n",
    "\n",
    "\n",
    "def _read_tif(fn):\n",
    "    with rasterio.open(f'{fn}/B04.tif') as img:\n",
    "        r_img = img.read(1).astype('float32')\n",
    "    with rasterio.open(f'{fn}/B03.tif') as img:\n",
    "        g_img = img.read(1).astype('float32')\n",
    "    with rasterio.open(f'{fn}/B02.tif') as img:\n",
    "        b_img = img.read(1).astype('float32')\n",
    "    with rasterio.open(f'{fn}/B08.tif') as img:\n",
    "        i_img = img.read(1).astype('float32')\n",
    "    return np.stack([r_img, g_img, b_img, i_img], axis=0)\n",
    "\n",
    "\n",
    "class TensorImageGeo(TensorImage):\n",
    "    @classmethod\n",
    "    def create_tif(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return  cls(torch.from_numpy(_read_tif(fn)))\n",
    "\n",
    "    @classmethod\n",
    "    def create_npy(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(torch.from_numpy(np.load(f'{NPY_FEATURES}{fn}.npy')).float())\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show image using `merge(self._show_args, kwargs)`\"\n",
    "        return show_image(self._true_color(), ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "    # adapted from https://github.com/makepath/xarray-spatial/blob/master/xrspatial/multispectral.py\n",
    "    def _true_color(self, nodata=1, c=10.0, th=0.125, pixel_max = 255):\n",
    "        img = self.detach().clone().cpu().numpy()\n",
    "        img[0, :, :] = _normalize_data(img[0, :, :], pixel_max, c, th)\n",
    "        img[1, :, :] = _normalize_data(img[1, :, :], pixel_max, c, th)\n",
    "        img[2, :, :] = _normalize_data(img[2, :, :], pixel_max, c, th)\n",
    "        return img[0:3, :, :].transpose(1,2,0).astype(np.uint8)\n",
    "\n",
    "\n",
    "class TensorMaskGeo(TensorMask):\n",
    "    _show_args = {'alpha':0.25, 'cmap':'cool', 'interpolation':'nearest'}\n",
    "    @classmethod\n",
    "    def create_tif(cls, fn)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        with rasterio.open(f'{LABELS}{fn}.tif') as img:\n",
    "            lbl = img.read(1)\n",
    "        return cls(torch.from_numpy(lbl))\n",
    "\n",
    "    @classmethod\n",
    "    def create_npy(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(torch.from_numpy(np.load(f'{NPY_LABELS}{fn}.npy')))\n",
    "\n",
    "\n",
    "def _four_corners(img):\n",
    "    imgh, imgw = img.shape[-2], img.shape[-1]\n",
    "    croph, cropw = imgh//2, imgw//2\n",
    "    tl = img[..., 0:croph, 0:cropw]\n",
    "    tr = img[..., 0:croph, imgw-cropw:imgw]\n",
    "    bl = img[..., imgh-croph:imgh, 0:cropw]\n",
    "    br = img[..., imgh-croph:imgh, imgw-cropw:imgw]\n",
    "    return torch.stack([tl, tr, bl, br], 0)\n",
    "\n",
    "\n",
    "class TensorImageGeoCrop(TensorImageGeo):\n",
    "    @classmethod\n",
    "    def create_tif(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(_four_corners(torch.from_numpy(_read_tif(fn))))\n",
    "\n",
    "    @classmethod\n",
    "    def create_npy(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(_four_corners(torch.from_numpy(np.load(f'{NPY_FEATURES}{fn}.npy')).float()))\n",
    "\n",
    "\n",
    "class TensorMaskGeoCrop(TensorMaskGeo):\n",
    "    @classmethod\n",
    "    def create_tif(cls, fn)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        with rasterio.open(f'{LABELS}{fn}.tif') as img:\n",
    "            lbl = img.read(1)\n",
    "        return cls(_four_corners(torch.from_numpy(lbl)))\n",
    "\n",
    "    @classmethod\n",
    "    def create_npy(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(_four_corners(torch.from_numpy(np.load(f'{NPY_LABELS}{fn}.npy'))))\n",
    "\n",
    "\n",
    "\n",
    "class TensorMaskToClass(TensorMaskGeo):\n",
    "    pass\n",
    "\n",
    "class TensorCropMaskToClass(TensorMaskGeoCrop):\n",
    "    pass\n",
    "\n",
    "\n",
    "@ToTensor\n",
    "def encodes(self, o:TensorImageGeo):\n",
    "    return o\n",
    "\n",
    "@ToTensor\n",
    "def encodes(self, o:TensorMaskGeo):\n",
    "    return o\n",
    "\n",
    "\n",
    "class Normalize(DisplayedTransform):\n",
    "    \"Normalize/denorm batch of `TensorImage`\"\n",
    "    parameters,order = L('mean', 'std'),99\n",
    "    def __init__(self, mean=None, std=None, axes=(0,2,3)): store_attr()\n",
    "\n",
    "    @classmethod\n",
    "    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n",
    "\n",
    "    def setups(self, dl:DataLoader):\n",
    "        if self.mean is None or self.std is None:\n",
    "            x,*_ = dl.one_batch()\n",
    "            self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n",
    "\n",
    "    def encodes(self, x:TensorImageGeo): return (x-self.mean) / self.std\n",
    "\n",
    "    def decodes(self, x:TensorImageGeo):\n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return (x*f(self.std) + f(self.mean))\n",
    "\n",
    "\n",
    "class MaskToMultiClass(DisplayedTransform):\n",
    "    order=100\n",
    "    def encodes(self, x:(TensorMaskToClass, TensorCropMaskToClass)):\n",
    "        s = x.shape\n",
    "        x = x.sum(axis=(1,2)) / (s[1]*s[2])\n",
    "        y = torch.zeros([x.shape[0], 20], dtype=x.dtype, device=x.device)\n",
    "        for i in range(len(x)):\n",
    "            z = torch.ones(int(20 * x[i]), dtype=x.dtype, device=x.device)\n",
    "            z = F.pad(z, [0, 20-z.shape[0]])\n",
    "            y[i] = z\n",
    "        return TensorMultiCategory(y)\n",
    "    def decodes(self, x:TensorMultiCategory):\n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return f(x.sum(dim=1))\n",
    "\n",
    "@patch\n",
    "def show(self:TensorMultiCategory, ctx=None, **kwargs):\n",
    "    \"Show self\"\n",
    "    return show_title(str(self.item()), ctx=ctx, **merge({'label': 'text'}, kwargs))\n",
    "\n",
    "\n",
    "class ReshapeCrop(DisplayedTransform):\n",
    "    order = 9 # Runs right before IntToFloatTensor on the GPU\n",
    "    def encodes(self, x:TensorImageGeoCrop):\n",
    "        s = x.shape\n",
    "        return x.view(s[0]*s[1],s[2],s[3],s[4])\n",
    "\n",
    "    def encodes(self, x:TensorMaskGeoCrop):\n",
    "        s = x.shape\n",
    "        return x.view(s[0]*s[1],s[2],s[3])\n",
    "\n",
    "    def decodes(self, x:TensorImageGeoCrop):\n",
    "        xs = []\n",
    "        for i in range(0, find_bs(x), 4):\n",
    "            xs.append(make_grid(x[i:i+4], nrow=2, padding=0))\n",
    "        return torch.stack(xs, dim=0)\n",
    "\n",
    "\n",
    "def GeoImageBlock(cls=TensorImageGeo, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=cls.create_tif)\n",
    "    else:   return TransformBlock(type_tfms=cls.create_npy)\n",
    "\n",
    "def GeoMaskBlock(codes=None, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=TensorMaskGeo.create_tif)\n",
    "    else:   return TransformBlock(type_tfms=TensorMaskGeo.create_npy)\n",
    "\n",
    "def GeoImageCropBlock(cls=TensorImageGeoCrop, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=cls.create_tif, batch_tfms=ReshapeCrop)\n",
    "    else:   return TransformBlock(type_tfms=cls.create_npy, batch_tfms=ReshapeCrop)\n",
    "\n",
    "def GeoMaskCropBlock(cls=TensorMaskGeoCrop, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=cls.create_tif, batch_tfms=ReshapeCrop)\n",
    "    else:   return TransformBlock(type_tfms=cls.create_npy, batch_tfms=ReshapeCrop)\n",
    "\n",
    "def GeoMultiClassBlock(cls=TensorMaskToClass, codes=None, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=cls.create_tif, batch_tfms=MaskToMultiClass)\n",
    "    else:   return TransformBlock(type_tfms=cls.create_npy, batch_tfms=MaskToMultiClass)\n",
    "\n",
    "def GeoCropMultiClassBlock(cls=TensorCropMaskToClass, codes=None, tif=True):\n",
    "    if tif: return TransformBlock(type_tfms=cls.create_tif, batch_tfms=[ReshapeCrop, MaskToMultiClass])\n",
    "    else:   return TransformBlock(type_tfms=cls.create_npy, batch_tfms=[ReshapeCrop, MaskToMultiClass])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.wandb import _make_plt\n",
    "\n",
    "@typedispatch\n",
    "def wandb_process(x:TensorImageGeo, y, samples, outs):\n",
    "    \"Process `sample` and `out` depending on the type of `x/y`\"\n",
    "    res_input, res_pred, res_label = [],[],[]\n",
    "    for s,o in zip(samples, outs):\n",
    "        img = TensorImage(s[0]._true_color())\n",
    "        res_input.append(wandb.Image(img, caption='Input data'))\n",
    "        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground Truth\", res_label)):\n",
    "            fig, ax = _make_plt(img)\n",
    "            # Superimpose label or prediction to input image\n",
    "            ax = img.show(ctx=ax)\n",
    "            ax = t.show(ctx=ax)\n",
    "            res.append(wandb.Image(fig, caption=capt))\n",
    "            plt.close(fig)\n",
    "    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground Truth\":res_label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "class CombineGeoCallback(Callback):\n",
    "    run_train,order = False,MixedPrecision.order+1\n",
    "\n",
    "    def after_pred(self):\n",
    "        preds = []\n",
    "        for i in range(0, find_bs(self.learn.pred), 4):\n",
    "            preds.append(make_grid(self.learn.pred[i:i+4], nrow=2, padding=0))\n",
    "        self.learn.pred = TensorImageGeo(torch.stack(preds, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://nicjac.dev/posts/identify-best-model/ soon to be added to fast.ai\n",
    "class SaveModelCallback(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training and loads it at the end.\"\n",
    "    order = TrackerCallback.order+1\n",
    "    def __init__(self, monitor='valid_loss', comp=None, min_delta=0., fname='model', every_epoch=False, at_end=False,\n",
    "                 with_opt=False, reset_on_fit=True):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n",
    "        assert not (every_epoch and at_end), \"every_epoch and at_end cannot both be set to True\"\n",
    "        # keep track of file path for loggers\n",
    "        self.last_saved_path = None\n",
    "        self.last_saved_metadata = None\n",
    "        \n",
    "        store_attr('fname,every_epoch,at_end,with_opt')\n",
    "\n",
    "    def _save(self, name, metadata): \n",
    "        self.last_saved_path = self.learn.save(name, with_opt=self.with_opt)\n",
    "        self.last_saved_metadata = metadata\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and save if best.\"\n",
    "        if self.every_epoch:\n",
    "            if (self.epoch%self.every_epoch) == 0: self._save(f'{self.fname}_{self.epoch}')\n",
    "        else: #every improvement\n",
    "            super().after_epoch()\n",
    "            if self.new_best:\n",
    "                print(f'Better model found at epoch {self.epoch} with {self.monitor} value: {self.best}.')\n",
    "                self._save(f'{self.fname}', {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log)})\n",
    "\n",
    "    def after_fit(self, **kwargs):\n",
    "        \"Load the best model.\"\n",
    "        if self.at_end: self._save(f'{self.fname}')\n",
    "        elif not self.every_epoch: self.learn.load(f'{self.fname}', with_opt=self.with_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://nicjac.dev/posts/identify-best-model/ soon to be added to fast.ai\n",
    "from fastai.callback.wandb import log_model\n",
    "\n",
    "@patch\n",
    "def after_fit(self:WandbCallback):\n",
    "    if self.log_model:\n",
    "        if self.save_model.last_saved_path is None:\n",
    "            print('WandbCallback could not retrieve a model to upload')\n",
    "        else:\n",
    "            log_model(self.save_model.last_saved_path, metadata=self.save_model.last_saved_metadata)\n",
    "            \n",
    "            for metadata_key in self.save_model.last_saved_metadata:\n",
    "                wandb.run.summary[f'best_{metadata_key}'] = self.save_model.last_saved_metadata[metadata_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.filters import MaxBlurPool2D\n",
    "\n",
    "stem_pool  = partial(MaxBlurPool2D, kernel_size=3, ceil_mode=True)\n",
    "block_pool = partial(MaxBlurPool2D, kernel_size=3, ceil_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class sa_layer(nn.Module):\n",
    "    \"\"\"Constructs a Shuffle Attention module.\n",
    "    Args:\n",
    "        channel: number of input channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, groups=64):\n",
    "        super(sa_layer, self).__init__()\n",
    "        self.groups = groups\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.cweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))\n",
    "        self.cbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))\n",
    "        self.sweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))\n",
    "        self.sbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.gn = nn.GroupNorm(channel // (2 * groups), channel // (2 * groups))\n",
    "\n",
    "    @staticmethod\n",
    "    def channel_shuffle(x, groups):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = x.reshape(b, groups, -1, h, w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        # flatten\n",
    "        x = x.reshape(b, -1, h, w)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = x.reshape(b * self.groups, -1, h, w)\n",
    "        x_0, x_1 = x.chunk(2, dim=1)\n",
    "\n",
    "        # channel attention\n",
    "        xn = self.avg_pool(x_0)\n",
    "        xn = self.cweight * xn + self.cbias\n",
    "        xn = x_0 * self.sigmoid(xn)\n",
    "\n",
    "        # spatial attention\n",
    "        xs = self.gn(x_1)\n",
    "        xs = self.sweight * xs + self.sbias\n",
    "        xs = x_1 * self.sigmoid(xs)\n",
    "\n",
    "        # concatenate along channel axis\n",
    "        out = torch.cat([xn, xs], dim=1)\n",
    "        out = out.reshape(b, -1, h, w)\n",
    "\n",
    "        out = self.channel_shuffle(out, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_basics import *\n",
    "from fastai.layers import ConvLayer, NormType, SimpleSelfAttention\n",
    "from timm.models.layers.eca import EcaModule\n",
    "\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "\n",
    "class ResBlock(Module):\n",
    "    \"Resnet block from `ni` to `nh` with `stride`\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, expansion, ni, nf, stride=1, groups=1, reduction=None, sha=False, eca=False, \n",
    "                 nh1=None, nh2=None, dw=False, g2=1, sa=False, sym=False, norm_type=NormType.Batch, \n",
    "                 act_cls=defaults.activation, ndim=2, ks=3, pool=AvgPool, pool_first=True, block_pool=None, **kwargs):\n",
    "        norm2 = (NormType.BatchZero if norm_type==NormType.Batch else\n",
    "                 NormType.InstanceZero if norm_type==NormType.Instance else norm_type)\n",
    "        if nh2 is None: nh2 = nf\n",
    "        if nh1 is None: nh1 = nh2\n",
    "        nf,ni = nf*expansion,ni*expansion\n",
    "        k0 = dict(norm_type=norm_type, act_cls=act_cls, ndim=ndim, **kwargs)\n",
    "        k1 = dict(norm_type=norm2, act_cls=None, ndim=ndim, **kwargs)\n",
    "        convpath  = [ConvLayer(ni,  nh2, ks, stride=stride, groups=ni if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, ks, groups=g2, **k1)\n",
    "        ] if expansion == 1 else [\n",
    "                     ConvLayer(ni,  nh1, 1, **k0),\n",
    "                     ConvLayer(nh1, nh2, ks, stride=stride, groups=nh1 if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, 1, groups=g2, **k1)]\n",
    "        if reduction: convpath.append(SEModule(nf, reduction=reduction, act_cls=act_cls))\n",
    "        if sha: convpath.append(sa_layer(nf))\n",
    "        if eca: convpath.append(EcaModule(nf))\n",
    "        if sa: convpath.append(SimpleSelfAttention(nf,ks=1,sym=sym))\n",
    "        self.convpath = nn.Sequential(*convpath)\n",
    "        idpath = []\n",
    "        if ni!=nf: idpath.append(ConvLayer(ni, nf, 1, act_cls=None, ndim=ndim, **kwargs))\n",
    "        if stride!=1:\n",
    "            if block_pool==None:\n",
    "                idpath.insert((1,0)[pool_first], pool(stride, ndim=ndim, ceil_mode=True))\n",
    "            else:\n",
    "                idpath.insert((1,0)[pool_first], block_pool())\n",
    "        self.idpath = nn.Sequential(*idpath)\n",
    "        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "\n",
    "    def forward(self, x): return self.act(self.convpath(x) + self.idpath(x))\n",
    "\n",
    "\n",
    "class XResNet(Module):\n",
    "    @delegates(ResBlock)\n",
    "    def __init__(self, block, expansion, layers, p=0.0, c_in=3, n_out=1000, stem_szs=(32,32,64),\n",
    "                 widen=1.0, sa=False, act_cls=defaults.activation, norm_type=NormType.Batch, ndim=2,  \n",
    "                 ks=3, stride=2, stem_pool=partial(MaxPool, ks=3, stride=2, padding=3//2, ndim=2), **kwargs):\n",
    "        store_attr('block,expansion,act_cls,ndim,ks')\n",
    "        if ks % 2 == 0: raise Exception('kernel size has to be odd!')\n",
    "        stem_szs = [c_in, *stem_szs]\n",
    "        stem = [ConvLayer(stem_szs[i], stem_szs[i+1], ks=ks, stride=stride if i==0 else 1,\n",
    "                          act_cls=act_cls, ndim=ndim, norm_type=norm_type)\n",
    "                for i in range(3)]\n",
    "\n",
    "        self.stem=nn.Sequential(\n",
    "            *stem, stem_pool())   \n",
    "\n",
    "        block_szs   = [int(o*widen) for o in [64,128,256,512] +[256]*(len(layers)-4)]\n",
    "        block_szs   = [64//expansion] + block_szs\n",
    "        blocks = self._make_blocks(layers, block_szs, sa, stride, norm_type=norm_type, **kwargs)\n",
    "        \n",
    "        # quick fix to get working\n",
    "        self.layer1 = blocks[0] \n",
    "        self.layer2 = blocks[1]\n",
    "        self.layer3 = blocks[2]\n",
    "        self.layer4 = blocks[3]\n",
    "\n",
    "        self.head = nn.Sequential(AdaptiveAvgPool(sz=1, ndim=ndim), Flatten(), nn.Dropout(p),\n",
    "                                  nn.Linear(block_szs[-1]*expansion, n_out))\n",
    "\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_blocks(self, layers, block_szs, sa, stride, **kwargs):\n",
    "        return [self._make_layer(ni=block_szs[i], nf=block_szs[i+1], blocks=l,\n",
    "                                 stride=1 if i==0 else stride, sa=sa and i==len(layers)-4, **kwargs)\n",
    "                for i,l in enumerate(layers)]\n",
    "\n",
    "    def _make_layer(self, ni, nf, blocks, stride, sa, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            *[self.block(self.expansion, ni if i==0 else nf, nf, stride=stride if i==0 else 1,\n",
    "                      sa=sa and i==(blocks-1), act_cls=self.act_cls, ndim=self.ndim, ks=self.ks, **kwargs)\n",
    "              for i in range(blocks)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        c1 = self.layer1(x)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "\n",
    "        return self.head(c4)\n",
    "\n",
    "\n",
    "class XResNetDeepLabHead(Module):\n",
    "    @delegates(ResBlock)\n",
    "    def __init__(self, block, expansion, layers, p=0.0, c_in=3, n_out=1000, stem_szs=(32,32,64),\n",
    "                 widen=1.0, sa=False, act_cls=defaults.activation, norm_type=NormType.Batch, ndim=2,  \n",
    "                 ks=3, stride=2, stem_pool=partial(MaxPool, ks=3, stride=2, padding=3//2, ndim=2), **kwargs):\n",
    "        store_attr('block,expansion,act_cls,ndim,ks')\n",
    "        if ks % 2 == 0: raise Exception('kernel size has to be odd!')\n",
    "        stem_szs = [c_in, *stem_szs]\n",
    "        stem = [ConvLayer(stem_szs[i], stem_szs[i+1], ks=ks, stride=stride if i==0 else 1,\n",
    "                          act_cls=act_cls, ndim=ndim, norm_type=norm_type)\n",
    "                for i in range(3)]\n",
    "\n",
    "        self.stem=nn.Sequential(\n",
    "            *stem, stem_pool())   \n",
    "\n",
    "        block_szs   = [int(o*widen) for o in [64,128,256,512] +[256]*(len(layers)-4)]\n",
    "        block_szs   = [64//expansion] + block_szs\n",
    "        blocks = self._make_blocks(layers, block_szs, sa, stride, norm_type=norm_type, **kwargs)\n",
    "        \n",
    "        # quick fix to get working\n",
    "        self.layer1 = blocks[0] \n",
    "        self.layer2 = blocks[1]\n",
    "        self.layer3 = blocks[2]\n",
    "        self.layer4 = blocks[3]\n",
    "\n",
    "        # self.head = nn.Sequential(AdaptiveAvgPool(sz=1, ndim=ndim), Flatten(), nn.Dropout(p),\n",
    "        #                           nn.Linear(block_szs[-1]*expansion, n_out))\n",
    "\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_blocks(self, layers, block_szs, sa, stride, **kwargs):\n",
    "        return [self._make_layer(ni=block_szs[i], nf=block_szs[i+1], blocks=l,\n",
    "                                 stride=1 if i==0 else stride, sa=sa and i==len(layers)-4, **kwargs)\n",
    "                for i,l in enumerate(layers)]\n",
    "\n",
    "    def _make_layer(self, ni, nf, blocks, stride, sa, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            *[self.block(self.expansion, ni if i==0 else nf, nf, stride=stride if i==0 else 1,\n",
    "                      sa=sa and i==(blocks-1), act_cls=self.act_cls, ndim=self.ndim, ks=self.ks, **kwargs)\n",
    "              for i in range(blocks)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        c1 = self.layer1(x)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "\n",
    "        return c4, c1 #, self.head(c4)\n",
    "\n",
    "\n",
    "def _xresnet(pretrained, expansion, layers, **kwargs):\n",
    "    # TODO pretrain all sizes. Currently will fail with non-xrn50\n",
    "    url = 'https://s3.amazonaws.com/fast-ai-modelzoo/xrn50_940.pth'\n",
    "    if deeplabhead: res = XResNetDeepLabHead(ResBlock, expansion, layers, **kwargs)\n",
    "    else: res = XResNet(ResBlock, expansion, layers, **kwargs)\n",
    "    if pretrained: raise ValueError('No pretrained models.')\n",
    "    return res\n",
    "\n",
    "def xresnet18 (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet50 (pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 4,  6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def SEBlock(expansion, ni, nf, groups=1, reduction=16, stride=1, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh1=nf*2, nh2=nf*expansion, **kwargs)\n",
    "\n",
    "def SEResNeXtBlock(expansion, ni, nf, groups=32, reduction=16, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh2=w, **kwargs)\n",
    "\n",
    "def SABlock(expansion, ni, nf, groups=1, sha=True, stride=1, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, sha=sha, nh1=nf*2, nh2=nf*expansion, **kwargs)\n",
    "\n",
    "def SAResNeXtBlock(expansion, ni, nf, groups=32, sha=True, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, sha=sha, nh2=w, **kwargs)\n",
    "\n",
    "def ECABlock(expansion, ni, nf, groups=1, eca=True, stride=1, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, eca=eca, nh1=nf*2, nh2=nf*expansion, **kwargs)\n",
    "\n",
    "def ECAResNeXtBlock(expansion, ni, nf, groups=32, eca=True, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, eca=eca, nh2=w, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def xse_resnet34(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(SEBlock,  1, g1, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "\n",
    "def xse_resnext34(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(SEResNeXtBlock, 1, g1, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "\n",
    "def xse_resnet50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(SEBlock, 4, g1, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "\n",
    "def xse_resnext50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet \n",
    "    return Model(SEResNeXtBlock, 4, g1, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "\n",
    "def xsa_resnet50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(SABlock, 4, g1, n_out=n_out, sha=True, **kwargs)\n",
    "\n",
    "def xsa_resnext50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(SAResNeXtBlock, 4, g1, n_out=n_out, groups=32, sha=True, **kwargs)\n",
    "\n",
    "def xeca_resnet34(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(ECABlock, 1, g1, n_out=n_out, eca=True, **kwargs)\n",
    "\n",
    "def xeca_resnext34(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(ECAResNeXtBlock, 1, g1, n_out=n_out, groups=32, eca=True, **kwargs)\n",
    "\n",
    "def xeca_resnet50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):  \n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return XResNet(ECABlock, 4, g1, n_out=n_out, eca=True, **kwargs)\n",
    "\n",
    "def xeca_resnext50(n_out=1000, pretrained=False, deeplabhead=False, **kwargs):\n",
    "    Model = XResNetDeepLabHead if deeplabhead else XResNet\n",
    "    return Model(ECAResNeXtBlock, 4, g1, n_out=n_out, groups=32, eca=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeeplabV3+\n",
    "Implementation modified from https://github.com/yassouali/pytorch-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "-> The Atrous Spatial Pyramid Pooling\n",
    "'''\n",
    "\n",
    "def assp_branch(in_channels, out_channels, kernel_size, dilation, act_cls=defaults.activation):\n",
    "    padding = 0 if kernel_size == 1 else dilation\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls())\n",
    "\n",
    "class ASSP(nn.Module):\n",
    "    def __init__(self, in_channels, output_stride, p=0.5, act_cls=defaults.activation):\n",
    "        super(ASSP, self).__init__()\n",
    "\n",
    "        assert output_stride in [8, 16], 'Only output strides of 8 or 16 are suported'\n",
    "        if output_stride == 16: dilations = [1, 6, 12, 18]\n",
    "        elif output_stride == 8: dilations = [1, 12, 24, 36]\n",
    "        \n",
    "        self.aspp1 = assp_branch(in_channels, 256, 1, dilation=dilations[0], act_cls=act_cls)\n",
    "        self.aspp2 = assp_branch(in_channels, 256, 3, dilation=dilations[1], act_cls=act_cls)\n",
    "        self.aspp3 = assp_branch(in_channels, 256, 3, dilation=dilations[2], act_cls=act_cls)\n",
    "        self.aspp4 = assp_branch(in_channels, 256, 3, dilation=dilations[3], act_cls=act_cls)\n",
    "\n",
    "        self.avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(in_channels, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls())\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256*5, 256, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        init_cnn(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = F.interpolate(self.avg_pool(x), size=(x.size(2), x.size(3)), mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = self.conv1(torch.cat((x1, x2, x3, x4, x5), dim=1))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(self.act(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "''' \n",
    "-> Decoder\n",
    "'''\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, low_level_channels, num_classes, p=0.1, act_cls=defaults.activation):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(low_level_channels, 48, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "\n",
    "        # Table 2, best performance with two 3x3 convs\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(48+256, 256, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls(),\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(256, num_classes, 1, stride=1),\n",
    "        )\n",
    "        init_cnn(self)\n",
    "\n",
    "    def forward(self, x, low_level_features):\n",
    "        low_level_features = self.conv1(low_level_features)\n",
    "        low_level_features = self.act(self.bn1(low_level_features))\n",
    "        H, W = low_level_features.size(2), low_level_features.size(3)\n",
    "\n",
    "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        x = self.output(torch.cat((low_level_features, x), dim=1))\n",
    "        return x\n",
    "\n",
    "'''\n",
    "-> Deeplab V3 +\n",
    "'''\n",
    "\n",
    "class DeepLab(nn.Module):\n",
    "    def __init__(self, num_classes, backbone, in_channels=2048, output_stride=16, assp_p=0.5, decode_p=0.1,\n",
    "                 freeze_bn=False, freeze_backbone=False, act_cls=defaults.activation, **_):\n",
    "                \n",
    "        super(DeepLab, self).__init__()\n",
    "        low_level_channels = int(in_channels/8)\n",
    "\n",
    "        self.backbone = backbone()\n",
    "        self.ASSP = ASSP(in_channels=in_channels, output_stride=output_stride, p=assp_p, act_cls=act_cls)\n",
    "        self.decoder = Decoder(low_level_channels, num_classes, p=decode_p, act_cls=act_cls)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.size(2), x.size(3)\n",
    "        x, low_level_features = self.backbone(x)\n",
    "        x = self.ASSP(x)\n",
    "        x = self.decoder(x, low_level_features)\n",
    "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def channel_drop(x:TensorImage, replace, channel, dim):\n",
    "    return x.index_fill_(dim, channel, replace)\n",
    "\n",
    "class ChannelDrop(RandTransform):\n",
    "    order = 65 # After Noise \n",
    "    \"Drop entire channel by replacing it with random solid value or `replace_val`\"\n",
    "    def __init__(self, p=0.1, replace=None, channels=3, dim=-3):\n",
    "        super().__init__(p=p)\n",
    "        self.replace = replace\n",
    "        self.channels = channels-1\n",
    "        self.dim = dim\n",
    "\n",
    "    def before_call(self, b, split_idx):\n",
    "        super().before_call(b, split_idx)\n",
    "        self.value = random.uniform(0,1) if self.replace is None else self.replace\n",
    "        self.channel = random.randint(0, self.channels)\n",
    "\n",
    "    def encodes(self, x:TensorImage):\n",
    "        return x.channel_drop(self.value, torch.tensor(self.channel, device=x.device, dtype=torch.long), self.dim)\n",
    "\n",
    "@patch\n",
    "def random_noise(x:TensorImage, std):\n",
    "    return x + torch.normal(mean=0, std=std, size=x.shape, device=x.device, dtype=x.dtype)\n",
    "\n",
    "class RandomNoise(RandTransform):\n",
    "    \"Add random normal noise.\"\n",
    "    order = 60\n",
    "    def __init__(self, p=0.1, std=0.1):\n",
    "        super().__init__(p=p)\n",
    "        self.std = std\n",
    "\n",
    "    def before_call(self, b, split_idx):\n",
    "        super().before_call(b, split_idx)\n",
    "        self.rand_std = random.uniform(0,self.std)\n",
    "\n",
    "    def encodes(self, x:TensorImage):\n",
    "        return x.random_noise(self.rand_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastxtend.basics import is_listish\n",
    "\n",
    "# Internal Cell\n",
    "def init_loss(l, reduction='none', **kwargs):\n",
    "    \"Initiatize loss class or partial loss function\"\n",
    "    return partialler(l, reduction=reduction) if isinstance(l, types.FunctionType) else l(reduction=reduction, **kwargs)\n",
    "\n",
    "# Cell\n",
    "class MultiLoss(Module):\n",
    "    \"Combine multiple `loss_funcs` via `reduction`, with optional weighting. Log loss_funcs to metrics via LossMetricsCallback, optionally using `loss_names`.\"\n",
    "    def __init__(self, loss_funcs=[], weights=[], loss_kwargs=[], loss_names=[], reduction='mean', loss_reduction='none'):\n",
    "        store_attr(but='loss_funcs,weights,loss_kwargs,loss_names')\n",
    "        if len(weights)==0: self.weights = [1]*len(loss_funcs)\n",
    "        else: self.weights = weights\n",
    "        if len(loss_kwargs)==0: loss_kwargs = [{}]*len(loss_funcs)\n",
    "        self.loss_funcs = [init_loss(l, loss_reduction, **k) for l, k in zip(loss_funcs, loss_kwargs)]\n",
    "        if len(loss_names)==0: loss_names = [l.__name__ for l in loss_funcs]\n",
    "        self.loss_names = L(loss_names)\n",
    "        self._reduction,self._loss = reduction,{}\n",
    "\n",
    "    def forward(self, pred, targ):\n",
    "        for i, loss_func in enumerate(self.loss_funcs):\n",
    "            l = TensorBase(self.weights[i]*loss_func(pred, targ))\n",
    "            if i == 0: loss = TensorBase(torch.zeros_like(targ)).float()\n",
    "            loss += l\n",
    "            self._loss[i] = l\n",
    "        \n",
    "        return loss.mean() if self._reduction=='mean' else loss.sum() if self._reduction=='sum' else loss\n",
    "\n",
    "    @property\n",
    "    def losses(self): return self._loss\n",
    "\n",
    "    @property\n",
    "    def reduction(self): return self._reduction\n",
    "\n",
    "    @reduction.setter\n",
    "    def reduction(self, r): self._reduction = r\n",
    "\n",
    "    @delegates(Module.to)\n",
    "    def to(self, *args, **kwargs):\n",
    "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
    "        if is_listish(self.weights) or not isinstance(self.weights, torch.Tensor): self.weights = torch.Tensor(self.weights)\n",
    "        if self.weights.device != device: self.weights = self.weights.to(device=device)\n",
    "        super().to(*args, **kwargs)\n",
    "\n",
    "    def activation(self, pred):\n",
    "        return getattr(self.loss_funcs[0], 'activation', noop)(pred)\n",
    "\n",
    "    def decodes(self, pred):\n",
    "        return getattr(self.loss_funcs[0], 'decodes', noop)(pred)\n",
    "\n",
    "# Internal Cell\n",
    "class MultiAvgLoss(AvgLoss):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, i, name):\n",
    "        store_attr(but='name')\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.loss_func.losses[self.i].mean())*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def name(self): return self._name\n",
    "\n",
    "# Internal Cell\n",
    "class MultiAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, i, name, beta=0.98):\n",
    "        store_attr(but='name')\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss_func.losses[self.i].mean(), gather=False), self.val, self.beta)\n",
    "\n",
    "    @property\n",
    "    def name(self): return self._name\n",
    "\n",
    "# Cell\n",
    "class MultiLossCallback(Callback):\n",
    "    run_valid,order = False,Recorder.order+1\n",
    "    \"\"\"A fastai friendly metric implemented as a callback so that we can handle use cases where we don't\n",
    "    want to count tokens marked to be ignored or else not count batches where there are no targs\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=0.98):\n",
    "        self.beta, = beta,\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not isinstance(self.loss_func, MultiLoss):\n",
    "            raise ValueError(\"`LossMetricsCallback` requires loss to be `MultiLoss` class\")\n",
    "\n",
    "        train_mets,valid_mets = L(),L()\n",
    "        for i in range(len(self.loss_func.loss_funcs)):\n",
    "            train_mets += MultiAvgSmoothLoss(i, self.loss_func.loss_names[i], self.beta)\n",
    "            valid_mets += MultiAvgLoss(i, self.loss_func.loss_names[i])\n",
    "        train_mets.map(Self.reset())\n",
    "\n",
    "        self.recorder._train_mets = train_mets + self.recorder._train_mets\n",
    "        self.recorder._valid_mets = valid_mets + self.recorder._valid_mets\n",
    "        self.recorder._train_smooth_mets = train_mets + self.recorder._train_smooth_mets\n",
    "\n",
    "        train_idx = self.recorder.metric_names.argfirst(lambda o:o=='train_loss')+1\n",
    "        valid_idx = self.recorder.metric_names.argfirst(lambda o:o=='valid_loss')+1\n",
    "        pre_names = self.recorder.metric_names[:train_idx]\n",
    "        mid_names = self.recorder.metric_names[train_idx:valid_idx]\n",
    "        post_names = self.recorder.metric_names[valid_idx:]\n",
    "\n",
    "        train_names = self.loss_func.loss_names.map('train_{}')\n",
    "        valid_names = self.loss_func.loss_names.map('valid_{}')\n",
    "\n",
    "        self.recorder.metric_names = pre_names + train_names + mid_names + valid_names + post_names\n",
    "        self.recorder._train_smooth_names = train_names + self.recorder._train_smooth_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(Module):\n",
    "    \"Dice loss for segmentation\"\n",
    "    def __init__(self, axis=1, smooth=1e-6, reduction=\"mean\", square_in_union=False):\n",
    "        store_attr()\n",
    "\n",
    "    def forward(self, pred, targ):\n",
    "        targ = self._one_hot(targ, pred.shape[self.axis])\n",
    "        pred, targ = TensorBase(pred), TensorBase(targ)\n",
    "        assert pred.shape == targ.shape, 'input and target dimensions differ, DiceLoss expects non one-hot targs'\n",
    "        pred = self.activation(pred)\n",
    "        sum_dims = list(range(2, len(pred.shape)))\n",
    "        inter = torch.sum(pred*targ, dim=sum_dims)\n",
    "        union = (torch.sum(pred**2+targ, dim=sum_dims) if self.square_in_union\n",
    "            else torch.sum(pred+targ, dim=sum_dims))\n",
    "        dice_score = (2. * inter + self.smooth)/(union + self.smooth)\n",
    "        loss = (1-dice_score)\n",
    "        return loss.mean() if self.reduction=='mean' else loss.sum() if self.reduction=='sum' else loss\n",
    "\n",
    "    @staticmethod\n",
    "    def _one_hot(x, classes, axis=1):\n",
    "        \"Creates one binay mask per class\"\n",
    "        return torch.stack([torch.where(x==c, 1, 0) for c in range(classes)], axis=axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datablocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudn_stats = ([2.74e+03,2.84e+03,2.85e+03,3.66e+03], [2.79e+03,2.9e+03,3.16e+03,2.42e+03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multcls_dls(bs, item_tfms=None, batch_tfms=None, valid_fold=4):\n",
    "    df = pd.read_csv(f'{PATH}cloudn_folds.csv')\n",
    "    df['valid'] = df.fold == valid_fold\n",
    "    \n",
    "    if batch_tfms is None: batch_tfms = []\n",
    "    batch_tfms += [Normalize.from_stats(*cloudn_stats)]\n",
    "\n",
    "    block = DataBlock(blocks=(GeoImageBlock(tif=False), GeoMultiClassBlock(tif=False)),\n",
    "                      get_x=[ColReader('chip_id')],\n",
    "                      get_y=[ColReader('chip_id')],\n",
    "                      splitter=ColSplitter('valid'),\n",
    "                      item_tfms=RandomCrop(256),\n",
    "                      batch_tfms=batch_tfms,\n",
    "                      n_inp=1)\n",
    "\n",
    "    vblock = DataBlock(blocks=(GeoImageCropBlock(tif=False), GeoCropMultiClassBlock(tif=False)),\n",
    "                       get_x=[ColReader('chip_id')],\n",
    "                       get_y=[ColReader('chip_id')],\n",
    "                       splitter=ColSplitter('valid'),\n",
    "                       item_tfms=item_tfms,\n",
    "                       batch_tfms=batch_tfms,\n",
    "                       n_inp=1)\n",
    "    \n",
    "    dls = block.dataloaders(df, bs=bs, num_workers=min(8, num_cpus()))\n",
    "    vdls = vblock.dataloaders(df, bs=bs//4, num_workers=min(8, num_cpus()))\n",
    "\n",
    "    dls.valid = vdls.valid\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segcrops_dls(bs, item_tfms=None, batch_tfms=None, valid_fold=4):\n",
    "    if FULL: df = pd.read_csv(f'{PATH}cloudn_folds.csv')\n",
    "    else: df = pd.read_csv(f'{PATH}cloudn_folds_sub.csv')\n",
    "    df['valid'] = df.fold == valid_fold\n",
    "    \n",
    "    if batch_tfms is None: batch_tfms = []\n",
    "    batch_tfms += [Normalize.from_stats(*cloudn_stats)]\n",
    "\n",
    "    block = DataBlock(blocks=(GeoImageBlock(tif=False), GeoMaskBlock(tif=False)),\n",
    "                      get_x=[ColReader('chip_id')],\n",
    "                      get_y=[ColReader('chip_id')],\n",
    "                      splitter=ColSplitter('valid'),\n",
    "                      item_tfms=RandomCrop(256),\n",
    "                      batch_tfms=batch_tfms,\n",
    "                      n_inp=1)\n",
    "\n",
    "    vblock = DataBlock(blocks=(GeoImageCropBlock(tif=False), GeoMaskBlock(tif=False)),\n",
    "                       get_x=[ColReader('chip_id')],\n",
    "                       get_y=[ColReader('chip_id')],\n",
    "                       splitter=ColSplitter('valid'),\n",
    "                       batch_tfms=Normalize.from_stats(*cloudn_stats),\n",
    "                       n_inp=1)\n",
    "    \n",
    "    dls = block.dataloaders(df, bs=bs, num_workers=min(8, num_cpus()))\n",
    "    vdls = vblock.dataloaders(df, bs=bs//4, num_workers=min(8, num_cpus()))\n",
    "\n",
    "    dls.valid = vdls.valid\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_train(run_name, bs, epochs=10, lr=8e-3, wd=1e-2, pct_start=0.75, item_tfms=None, batch_tfms=None, save=True,\n",
    "                     act_cls=nn.Mish, drop=0.0, next_lr=None, change_by=None, change_time=1, group='xecaRN50 MultiClass 10e', \n",
    "                     arch=xeca_resnet50, stem_pool=stem_pool, block_pool=block_pool, sa=False, seeds=[1618, 314, 2998, 6626, 42]):\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        valid_fold=i\n",
    "        print(f'Fold {i}')\n",
    "        \n",
    "        with less_random(seeds[i]):\n",
    "            dls = get_multcls_dls(bs, item_tfms=item_tfms, batch_tfms=batch_tfms, valid_fold=valid_fold)\n",
    "\n",
    "        run = wandb.init(project=\"cloudn-submissions\", name=f'{run_name} Fold {i}', entity='bwarner', group=group,\n",
    "                        notes=f'bs={bs}, epochs={epochs}, lr={lr}, wd={wd}, pct_start={pct_start}, valid_fold={valid_fold}')\n",
    "\n",
    "        cbs=[GradientClip()]\n",
    "        if save: cbs += [SaveModelCallback(monitor='f1_score', comp=np.greater, fname=f'{run_name} Fold {i}')]\n",
    "\n",
    "        with less_random(seeds[i]):\n",
    "            model = arch(c_in=4, n_out=20, act_cls=act_cls, stem_pool=stem_pool, block_pool=block_pool, p=drop, sa=sa)\n",
    "            learn = Learner(dls, model, loss_func=BCEWithLogitsLossFlat(), opt_func=ranger, \n",
    "                            metrics=[F1ScoreMulti(), PrecisionMulti(), RecallMulti()],\n",
    "                            cbs=cbs+[WandbCallback(log=None, log_preds=False, log_model=True)]).to_fp16()\n",
    "            \n",
    "        with less_random(seeds[i]):\n",
    "            learn.fit_flat_cos(epochs, lr, pct_start=pct_start, wd=wd)\n",
    "\n",
    "        if save:\n",
    "            # rename weights so they will match the deeplab expected model names\n",
    "            state = torch.load(f'models/{run_name} Fold {i}.pth', map_location='cpu')\n",
    "            for key in list(state.keys()):\n",
    "                state[key.replace('model', 'model.backbone')] = state.pop(key)\n",
    "            torch.save(state, f'models/{run_name} Rename Fold {i}.pth', pickle_module=pickle, pickle_protocol=2)\n",
    "\n",
    "        run.finish()\n",
    "        learn,dls = None,None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.336749</td>\n",
       "      <td>0.310142</td>\n",
       "      <td>0.881735</td>\n",
       "      <td>0.915409</td>\n",
       "      <td>0.852252</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.288155</td>\n",
       "      <td>0.893410</td>\n",
       "      <td>0.917953</td>\n",
       "      <td>0.870822</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276528</td>\n",
       "      <td>0.278260</td>\n",
       "      <td>0.898883</td>\n",
       "      <td>0.928527</td>\n",
       "      <td>0.871235</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265052</td>\n",
       "      <td>0.265648</td>\n",
       "      <td>0.907341</td>\n",
       "      <td>0.904498</td>\n",
       "      <td>0.910742</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270410</td>\n",
       "      <td>0.283613</td>\n",
       "      <td>0.893663</td>\n",
       "      <td>0.932170</td>\n",
       "      <td>0.858821</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.263718</td>\n",
       "      <td>0.247111</td>\n",
       "      <td>0.914428</td>\n",
       "      <td>0.917598</td>\n",
       "      <td>0.911461</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.244727</td>\n",
       "      <td>0.247019</td>\n",
       "      <td>0.918282</td>\n",
       "      <td>0.896258</td>\n",
       "      <td>0.941719</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.242850</td>\n",
       "      <td>0.232683</td>\n",
       "      <td>0.924152</td>\n",
       "      <td>0.906658</td>\n",
       "      <td>0.942446</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.233029</td>\n",
       "      <td>0.229362</td>\n",
       "      <td>0.923660</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.938069</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.234941</td>\n",
       "      <td>0.223137</td>\n",
       "      <td>0.924635</td>\n",
       "      <td>0.930670</td>\n",
       "      <td>0.919001</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.228124</td>\n",
       "      <td>0.241397</td>\n",
       "      <td>0.919746</td>\n",
       "      <td>0.924757</td>\n",
       "      <td>0.915347</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.222737</td>\n",
       "      <td>0.239147</td>\n",
       "      <td>0.924721</td>\n",
       "      <td>0.889872</td>\n",
       "      <td>0.962775</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.212627</td>\n",
       "      <td>0.213563</td>\n",
       "      <td>0.929974</td>\n",
       "      <td>0.929407</td>\n",
       "      <td>0.930725</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.200787</td>\n",
       "      <td>0.205194</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.939268</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197627</td>\n",
       "      <td>0.205822</td>\n",
       "      <td>0.933663</td>\n",
       "      <td>0.926143</td>\n",
       "      <td>0.941459</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with f1_score value: 0.881734707768806.\n",
      "Better model found at epoch 1 with f1_score value: 0.8934095168826328.\n",
      "Better model found at epoch 2 with f1_score value: 0.8988832918677184.\n",
      "Better model found at epoch 3 with f1_score value: 0.9073413751307668.\n",
      "Better model found at epoch 5 with f1_score value: 0.9144275126971253.\n",
      "Better model found at epoch 6 with f1_score value: 0.9182818167392135.\n",
      "Better model found at epoch 7 with f1_score value: 0.9241516274507395.\n",
      "Better model found at epoch 9 with f1_score value: 0.9246349611276325.\n",
      "Better model found at epoch 11 with f1_score value: 0.9247206987765126.\n",
      "Better model found at epoch 12 with f1_score value: 0.9299740134882624.\n",
      "Better model found at epoch 13 with f1_score value: 0.9329688712455301.\n",
      "Better model found at epoch 14 with f1_score value: 0.9336633903074414.\n",
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.336339</td>\n",
       "      <td>0.314929</td>\n",
       "      <td>0.896472</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.939739</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.300612</td>\n",
       "      <td>0.284494</td>\n",
       "      <td>0.899546</td>\n",
       "      <td>0.902093</td>\n",
       "      <td>0.897577</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276992</td>\n",
       "      <td>0.255716</td>\n",
       "      <td>0.913594</td>\n",
       "      <td>0.922984</td>\n",
       "      <td>0.904665</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.276346</td>\n",
       "      <td>0.261422</td>\n",
       "      <td>0.909689</td>\n",
       "      <td>0.930618</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.262229</td>\n",
       "      <td>0.266458</td>\n",
       "      <td>0.907939</td>\n",
       "      <td>0.915446</td>\n",
       "      <td>0.901022</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.259181</td>\n",
       "      <td>0.263014</td>\n",
       "      <td>0.910476</td>\n",
       "      <td>0.863224</td>\n",
       "      <td>0.963586</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.250738</td>\n",
       "      <td>0.224922</td>\n",
       "      <td>0.927552</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.941919</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.254064</td>\n",
       "      <td>0.284875</td>\n",
       "      <td>0.900088</td>\n",
       "      <td>0.838875</td>\n",
       "      <td>0.973493</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.234216</td>\n",
       "      <td>0.214730</td>\n",
       "      <td>0.931164</td>\n",
       "      <td>0.939833</td>\n",
       "      <td>0.922777</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.226353</td>\n",
       "      <td>0.233997</td>\n",
       "      <td>0.924834</td>\n",
       "      <td>0.899297</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.230028</td>\n",
       "      <td>0.214339</td>\n",
       "      <td>0.930579</td>\n",
       "      <td>0.933040</td>\n",
       "      <td>0.928399</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0.221520</td>\n",
       "      <td>0.929869</td>\n",
       "      <td>0.900993</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.220843</td>\n",
       "      <td>0.210891</td>\n",
       "      <td>0.932320</td>\n",
       "      <td>0.904321</td>\n",
       "      <td>0.962803</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.204854</td>\n",
       "      <td>0.204296</td>\n",
       "      <td>0.935774</td>\n",
       "      <td>0.926120</td>\n",
       "      <td>0.945742</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.203946</td>\n",
       "      <td>0.221733</td>\n",
       "      <td>0.931453</td>\n",
       "      <td>0.903378</td>\n",
       "      <td>0.962231</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with f1_score value: 0.8964716576403123.\n",
      "Better model found at epoch 1 with f1_score value: 0.8995461907124467.\n",
      "Better model found at epoch 2 with f1_score value: 0.9135944894153699.\n",
      "Better model found at epoch 6 with f1_score value: 0.9275515846900817.\n",
      "Better model found at epoch 8 with f1_score value: 0.9311640815179677.\n",
      "Better model found at epoch 12 with f1_score value: 0.9323201338277078.\n",
      "Better model found at epoch 13 with f1_score value: 0.9357738774084094.\n",
      "Fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.342163</td>\n",
       "      <td>0.295875</td>\n",
       "      <td>0.899001</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.290541</td>\n",
       "      <td>0.284803</td>\n",
       "      <td>0.902711</td>\n",
       "      <td>0.896391</td>\n",
       "      <td>0.909527</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283909</td>\n",
       "      <td>0.283661</td>\n",
       "      <td>0.901002</td>\n",
       "      <td>0.941682</td>\n",
       "      <td>0.863855</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270404</td>\n",
       "      <td>0.290408</td>\n",
       "      <td>0.896932</td>\n",
       "      <td>0.922753</td>\n",
       "      <td>0.873506</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265449</td>\n",
       "      <td>0.251996</td>\n",
       "      <td>0.913099</td>\n",
       "      <td>0.941173</td>\n",
       "      <td>0.887216</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.262647</td>\n",
       "      <td>0.266641</td>\n",
       "      <td>0.905418</td>\n",
       "      <td>0.914852</td>\n",
       "      <td>0.896814</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.246129</td>\n",
       "      <td>0.252886</td>\n",
       "      <td>0.908835</td>\n",
       "      <td>0.951030</td>\n",
       "      <td>0.870732</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.235942</td>\n",
       "      <td>0.236113</td>\n",
       "      <td>0.919586</td>\n",
       "      <td>0.944205</td>\n",
       "      <td>0.896604</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.239893</td>\n",
       "      <td>0.233954</td>\n",
       "      <td>0.926076</td>\n",
       "      <td>0.938233</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.234653</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.926650</td>\n",
       "      <td>0.936322</td>\n",
       "      <td>0.917388</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>0.272857</td>\n",
       "      <td>0.904925</td>\n",
       "      <td>0.948293</td>\n",
       "      <td>0.865434</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.227034</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.923072</td>\n",
       "      <td>0.948763</td>\n",
       "      <td>0.899178</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.217397</td>\n",
       "      <td>0.212425</td>\n",
       "      <td>0.934063</td>\n",
       "      <td>0.940123</td>\n",
       "      <td>0.928155</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.217615</td>\n",
       "      <td>0.216851</td>\n",
       "      <td>0.931992</td>\n",
       "      <td>0.912464</td>\n",
       "      <td>0.952565</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.200818</td>\n",
       "      <td>0.204319</td>\n",
       "      <td>0.933714</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>0.920410</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with f1_score value: 0.8990006498426976.\n",
      "Better model found at epoch 1 with f1_score value: 0.9027113110738568.\n",
      "Better model found at epoch 4 with f1_score value: 0.9130994058711007.\n",
      "Better model found at epoch 7 with f1_score value: 0.9195859247945896.\n",
      "Better model found at epoch 8 with f1_score value: 0.9260756878240631.\n",
      "Better model found at epoch 9 with f1_score value: 0.9266499437862675.\n",
      "Better model found at epoch 12 with f1_score value: 0.9340634861337925.\n",
      "Fold 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.331932</td>\n",
       "      <td>0.306103</td>\n",
       "      <td>0.892214</td>\n",
       "      <td>0.874556</td>\n",
       "      <td>0.910926</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.297355</td>\n",
       "      <td>0.269806</td>\n",
       "      <td>0.906395</td>\n",
       "      <td>0.887115</td>\n",
       "      <td>0.927296</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275573</td>\n",
       "      <td>0.268193</td>\n",
       "      <td>0.904621</td>\n",
       "      <td>0.925359</td>\n",
       "      <td>0.885060</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.278884</td>\n",
       "      <td>0.322799</td>\n",
       "      <td>0.875621</td>\n",
       "      <td>0.954202</td>\n",
       "      <td>0.810141</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.263481</td>\n",
       "      <td>0.245887</td>\n",
       "      <td>0.913349</td>\n",
       "      <td>0.932387</td>\n",
       "      <td>0.895745</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253051</td>\n",
       "      <td>0.228076</td>\n",
       "      <td>0.920799</td>\n",
       "      <td>0.932978</td>\n",
       "      <td>0.909298</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.251165</td>\n",
       "      <td>0.219948</td>\n",
       "      <td>0.927422</td>\n",
       "      <td>0.929948</td>\n",
       "      <td>0.925603</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.250116</td>\n",
       "      <td>0.234285</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.904136</td>\n",
       "      <td>0.946205</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.234789</td>\n",
       "      <td>0.220404</td>\n",
       "      <td>0.928298</td>\n",
       "      <td>0.919705</td>\n",
       "      <td>0.938237</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>0.224383</td>\n",
       "      <td>0.925047</td>\n",
       "      <td>0.928036</td>\n",
       "      <td>0.922575</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.222287</td>\n",
       "      <td>0.223920</td>\n",
       "      <td>0.925731</td>\n",
       "      <td>0.928231</td>\n",
       "      <td>0.924091</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.215964</td>\n",
       "      <td>0.213127</td>\n",
       "      <td>0.930519</td>\n",
       "      <td>0.930827</td>\n",
       "      <td>0.930482</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.212597</td>\n",
       "      <td>0.215127</td>\n",
       "      <td>0.926396</td>\n",
       "      <td>0.941101</td>\n",
       "      <td>0.912385</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.210992</td>\n",
       "      <td>0.203149</td>\n",
       "      <td>0.934814</td>\n",
       "      <td>0.919334</td>\n",
       "      <td>0.951304</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.206422</td>\n",
       "      <td>0.200715</td>\n",
       "      <td>0.936235</td>\n",
       "      <td>0.931272</td>\n",
       "      <td>0.941577</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with f1_score value: 0.8922141606849147.\n",
      "Better model found at epoch 1 with f1_score value: 0.9063945657431993.\n",
      "Better model found at epoch 4 with f1_score value: 0.9133494728832208.\n",
      "Better model found at epoch 5 with f1_score value: 0.9207988604642843.\n",
      "Better model found at epoch 6 with f1_score value: 0.9274215600011386.\n",
      "Better model found at epoch 8 with f1_score value: 0.9282975635461698.\n",
      "Better model found at epoch 11 with f1_score value: 0.9305191977031037.\n",
      "Better model found at epoch 13 with f1_score value: 0.9348138564568039.\n",
      "Better model found at epoch 14 with f1_score value: 0.9362351900428045.\n",
      "Fold 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.318884</td>\n",
       "      <td>0.316103</td>\n",
       "      <td>0.891375</td>\n",
       "      <td>0.860283</td>\n",
       "      <td>0.925915</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.282526</td>\n",
       "      <td>0.315173</td>\n",
       "      <td>0.885464</td>\n",
       "      <td>0.927515</td>\n",
       "      <td>0.847496</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280154</td>\n",
       "      <td>0.270862</td>\n",
       "      <td>0.906346</td>\n",
       "      <td>0.908649</td>\n",
       "      <td>0.904618</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263402</td>\n",
       "      <td>0.268123</td>\n",
       "      <td>0.910010</td>\n",
       "      <td>0.883595</td>\n",
       "      <td>0.938434</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256217</td>\n",
       "      <td>0.294182</td>\n",
       "      <td>0.888763</td>\n",
       "      <td>0.925753</td>\n",
       "      <td>0.856356</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241908</td>\n",
       "      <td>0.309298</td>\n",
       "      <td>0.898440</td>\n",
       "      <td>0.844088</td>\n",
       "      <td>0.961782</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.246645</td>\n",
       "      <td>0.274495</td>\n",
       "      <td>0.904456</td>\n",
       "      <td>0.908908</td>\n",
       "      <td>0.900865</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.239814</td>\n",
       "      <td>0.252842</td>\n",
       "      <td>0.917649</td>\n",
       "      <td>0.935745</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.227787</td>\n",
       "      <td>0.300498</td>\n",
       "      <td>0.884354</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.829847</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.228341</td>\n",
       "      <td>0.237238</td>\n",
       "      <td>0.921514</td>\n",
       "      <td>0.916479</td>\n",
       "      <td>0.926815</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.229906</td>\n",
       "      <td>0.242395</td>\n",
       "      <td>0.920106</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.230025</td>\n",
       "      <td>0.236677</td>\n",
       "      <td>0.923023</td>\n",
       "      <td>0.928929</td>\n",
       "      <td>0.917520</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.220186</td>\n",
       "      <td>0.225612</td>\n",
       "      <td>0.927571</td>\n",
       "      <td>0.939482</td>\n",
       "      <td>0.915984</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.202497</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>0.931705</td>\n",
       "      <td>0.925577</td>\n",
       "      <td>0.938136</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.216595</td>\n",
       "      <td>0.931122</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>0.940590</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with f1_score value: 0.8913753050184038.\n",
      "Better model found at epoch 2 with f1_score value: 0.9063455743916133.\n",
      "Better model found at epoch 3 with f1_score value: 0.9100098848962679.\n",
      "Better model found at epoch 7 with f1_score value: 0.9176493234056528.\n",
      "Better model found at epoch 9 with f1_score value: 0.9215137573709884.\n",
      "Better model found at epoch 11 with f1_score value: 0.9230227640311055.\n",
      "Better model found at epoch 12 with f1_score value: 0.9275706266581002.\n",
      "Better model found at epoch 13 with f1_score value: 0.9317050917037368.\n"
     ]
    }
   ],
   "source": [
    "bs=64\n",
    "epochs=15\n",
    "lr=1e-3\n",
    "\n",
    "multiclass_train('xSA-ResNeXt50 15e bs64 Multiclass Base', bs=bs, epochs=epochs, lr=lr, arch=xsa_resnext50,\n",
    "                 group='xSA-RNXt50 Multiclass-to-Seg 15-to-80e', wd=1e-4, save=True,\n",
    "                 batch_tfms=aug_transforms(flip_vert=True, max_rotate=45, max_zoom=1.2, max_warp=0.1, max_lighting=0,\n",
    "                 xtra_tfms=[ChannelDrop(channels=4), RandomNoise()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_train(run_name, bs, epochs=80, lr=1e-3, wd=1e-2, pct_start=0.75, item_tfms=None, batch_tfms=None, valid_fold=4, save=False,\n",
    "              deep_act_cls=nn.Mish, assp_p=0.5, decode_p=0.1, next_lr=None, change_by=None, change_time=1, group='xecaRN50 MultiClass 10e', \n",
    "              arch=xeca_resnet50, stem_pool=stem_pool, block_pool=block_pool, sa=False, arch_act_cls=nn.Mish, \n",
    "              load_name='xSA-ResNeXt50 15e bs64 Multiclass Base Rename', seeds=[1618, 314, 2998, 6626, 42]):\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        valid_fold=i\n",
    "        print(f'Fold {i}')\n",
    "        \n",
    "        with less_random(seeds[i]):\n",
    "            dls = get_segcrops_dls(bs, item_tfms=item_tfms, batch_tfms=batch_tfms, valid_fold=valid_fold)\n",
    "\n",
    "        run = wandb.init(project=\"cloudn-submissions\", name=f'{run_name} Fold {i}', entity='bwarner', group=group,\n",
    "                        notes=f'bs={bs}, epochs={epochs}, lr={lr}, wd={wd}, pct_start={pct_start}, valid_fold={valid_fold}')\n",
    "\n",
    "        loss = MultiLoss([LabelSmoothingCrossEntropy, DiceLoss], loss_kwargs=[{'axis':1},{}], \n",
    "                    weights=[1., 1.], loss_names=['ls_cross_entropy', 'dice_loss'], loss_reduction='mean')\n",
    "\n",
    "        cbs=[MultiLossCallback(), CombineGeoCallback(), GradientClip()]\n",
    "        if save: cbs += [SaveModelCallback(monitor='jaccard_coeff', comp=np.greater, fname=f'{run_name} Fold {i}')]\n",
    "\n",
    "        with less_random(seeds[i]):\n",
    "            backbone = partial(arch, c_in=4, n_out=20, act_cls=arch_act_cls, stem_pool=stem_pool, block_pool=block_pool, sa=sa, deeplabhead=True)\n",
    "            model = DeepLab(2, backbone, assp_p=assp_p, decode_p=decode_p, act_cls=deep_act_cls)\n",
    "            learn = Learner(dls, model, loss_func=loss, opt_func=ranger, \n",
    "                            metrics=[JaccardCoeff(), Dice()],\n",
    "                            cbs=cbs+[WandbCallback(log=None, log_preds=True, log_model=True, n_preds=12)]).to_fp16()\n",
    "\n",
    "        learn.load(f'{load_name} Fold {i}', strict=False)\n",
    "\n",
    "        with less_random(seeds[i]):\n",
    "            learn.fit_flat_cos(epochs, lr, pct_start=pct_start, wd=wd)\n",
    "\n",
    "        run.finish()\n",
    "        learn,dls = None,None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ls_cross_entropy</th>\n",
       "      <th>train_dice_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_ls_cross_entropy</th>\n",
       "      <th>valid_dice_loss</th>\n",
       "      <th>jaccard_coeff</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.894273</td>\n",
       "      <td>0.440704</td>\n",
       "      <td>0.453569</td>\n",
       "      <td>0.808032</td>\n",
       "      <td>0.393841</td>\n",
       "      <td>0.414191</td>\n",
       "      <td>0.828222</td>\n",
       "      <td>0.906041</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.843211</td>\n",
       "      <td>0.410527</td>\n",
       "      <td>0.432684</td>\n",
       "      <td>0.820440</td>\n",
       "      <td>0.403849</td>\n",
       "      <td>0.416591</td>\n",
       "      <td>0.818156</td>\n",
       "      <td>0.899985</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.809475</td>\n",
       "      <td>0.389652</td>\n",
       "      <td>0.419823</td>\n",
       "      <td>0.764665</td>\n",
       "      <td>0.387566</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.837072</td>\n",
       "      <td>0.911311</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.786480</td>\n",
       "      <td>0.376224</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.732765</td>\n",
       "      <td>0.370336</td>\n",
       "      <td>0.362429</td>\n",
       "      <td>0.860298</td>\n",
       "      <td>0.924903</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.788923</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.407832</td>\n",
       "      <td>0.753549</td>\n",
       "      <td>0.370358</td>\n",
       "      <td>0.383191</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.922194</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.787699</td>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.408488</td>\n",
       "      <td>0.786193</td>\n",
       "      <td>0.386135</td>\n",
       "      <td>0.400059</td>\n",
       "      <td>0.837142</td>\n",
       "      <td>0.911353</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.760494</td>\n",
       "      <td>0.360945</td>\n",
       "      <td>0.399549</td>\n",
       "      <td>0.721894</td>\n",
       "      <td>0.357114</td>\n",
       "      <td>0.364780</td>\n",
       "      <td>0.867408</td>\n",
       "      <td>0.928997</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.759062</td>\n",
       "      <td>0.362797</td>\n",
       "      <td>0.396265</td>\n",
       "      <td>0.729572</td>\n",
       "      <td>0.368438</td>\n",
       "      <td>0.361134</td>\n",
       "      <td>0.862062</td>\n",
       "      <td>0.925922</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.754486</td>\n",
       "      <td>0.360574</td>\n",
       "      <td>0.393912</td>\n",
       "      <td>0.740319</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.376420</td>\n",
       "      <td>0.865728</td>\n",
       "      <td>0.928033</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.747655</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.391713</td>\n",
       "      <td>0.707835</td>\n",
       "      <td>0.356585</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.875795</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.744896</td>\n",
       "      <td>0.353858</td>\n",
       "      <td>0.391038</td>\n",
       "      <td>0.714537</td>\n",
       "      <td>0.360443</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.866975</td>\n",
       "      <td>0.928748</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.741333</td>\n",
       "      <td>0.352409</td>\n",
       "      <td>0.388924</td>\n",
       "      <td>0.732996</td>\n",
       "      <td>0.364887</td>\n",
       "      <td>0.368109</td>\n",
       "      <td>0.866258</td>\n",
       "      <td>0.928337</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.741638</td>\n",
       "      <td>0.350332</td>\n",
       "      <td>0.391306</td>\n",
       "      <td>0.706552</td>\n",
       "      <td>0.359423</td>\n",
       "      <td>0.347129</td>\n",
       "      <td>0.871273</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.732565</td>\n",
       "      <td>0.346487</td>\n",
       "      <td>0.386078</td>\n",
       "      <td>0.700031</td>\n",
       "      <td>0.359446</td>\n",
       "      <td>0.340585</td>\n",
       "      <td>0.878083</td>\n",
       "      <td>0.935084</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.737206</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.695841</td>\n",
       "      <td>0.344957</td>\n",
       "      <td>0.350884</td>\n",
       "      <td>0.879006</td>\n",
       "      <td>0.935607</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.736371</td>\n",
       "      <td>0.346544</td>\n",
       "      <td>0.389828</td>\n",
       "      <td>0.706207</td>\n",
       "      <td>0.349106</td>\n",
       "      <td>0.357101</td>\n",
       "      <td>0.878711</td>\n",
       "      <td>0.935440</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.730961</td>\n",
       "      <td>0.347249</td>\n",
       "      <td>0.383712</td>\n",
       "      <td>0.719617</td>\n",
       "      <td>0.347888</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.729566</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>0.384962</td>\n",
       "      <td>0.705391</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.353743</td>\n",
       "      <td>0.876469</td>\n",
       "      <td>0.934168</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.722289</td>\n",
       "      <td>0.339186</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>0.692065</td>\n",
       "      <td>0.341468</td>\n",
       "      <td>0.350597</td>\n",
       "      <td>0.883159</td>\n",
       "      <td>0.937955</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.727459</td>\n",
       "      <td>0.344199</td>\n",
       "      <td>0.383260</td>\n",
       "      <td>0.692531</td>\n",
       "      <td>0.341114</td>\n",
       "      <td>0.351416</td>\n",
       "      <td>0.883470</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.727832</td>\n",
       "      <td>0.344873</td>\n",
       "      <td>0.382959</td>\n",
       "      <td>0.701665</td>\n",
       "      <td>0.346289</td>\n",
       "      <td>0.355376</td>\n",
       "      <td>0.877576</td>\n",
       "      <td>0.934797</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.715390</td>\n",
       "      <td>0.336906</td>\n",
       "      <td>0.378484</td>\n",
       "      <td>0.687215</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.337095</td>\n",
       "      <td>0.882986</td>\n",
       "      <td>0.937857</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.728393</td>\n",
       "      <td>0.344495</td>\n",
       "      <td>0.383897</td>\n",
       "      <td>0.717453</td>\n",
       "      <td>0.353551</td>\n",
       "      <td>0.363902</td>\n",
       "      <td>0.873766</td>\n",
       "      <td>0.932631</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.722568</td>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.381899</td>\n",
       "      <td>0.689063</td>\n",
       "      <td>0.347477</td>\n",
       "      <td>0.341586</td>\n",
       "      <td>0.881672</td>\n",
       "      <td>0.937116</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.718867</td>\n",
       "      <td>0.340921</td>\n",
       "      <td>0.377946</td>\n",
       "      <td>0.691711</td>\n",
       "      <td>0.347323</td>\n",
       "      <td>0.344388</td>\n",
       "      <td>0.879636</td>\n",
       "      <td>0.935964</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.720438</td>\n",
       "      <td>0.340706</td>\n",
       "      <td>0.379731</td>\n",
       "      <td>0.688828</td>\n",
       "      <td>0.342351</td>\n",
       "      <td>0.346477</td>\n",
       "      <td>0.882741</td>\n",
       "      <td>0.937719</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.724405</td>\n",
       "      <td>0.343721</td>\n",
       "      <td>0.380684</td>\n",
       "      <td>0.689392</td>\n",
       "      <td>0.346722</td>\n",
       "      <td>0.342671</td>\n",
       "      <td>0.884183</td>\n",
       "      <td>0.938532</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.717503</td>\n",
       "      <td>0.338422</td>\n",
       "      <td>0.379080</td>\n",
       "      <td>0.692319</td>\n",
       "      <td>0.343358</td>\n",
       "      <td>0.348961</td>\n",
       "      <td>0.879263</td>\n",
       "      <td>0.935753</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.708275</td>\n",
       "      <td>0.333415</td>\n",
       "      <td>0.374860</td>\n",
       "      <td>0.687715</td>\n",
       "      <td>0.350303</td>\n",
       "      <td>0.337411</td>\n",
       "      <td>0.884902</td>\n",
       "      <td>0.938937</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.716963</td>\n",
       "      <td>0.338443</td>\n",
       "      <td>0.378520</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.345559</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>0.880809</td>\n",
       "      <td>0.936628</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.715092</td>\n",
       "      <td>0.338873</td>\n",
       "      <td>0.376219</td>\n",
       "      <td>0.682844</td>\n",
       "      <td>0.342592</td>\n",
       "      <td>0.340252</td>\n",
       "      <td>0.887653</td>\n",
       "      <td>0.940483</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.719182</td>\n",
       "      <td>0.340086</td>\n",
       "      <td>0.379095</td>\n",
       "      <td>0.683872</td>\n",
       "      <td>0.335848</td>\n",
       "      <td>0.348025</td>\n",
       "      <td>0.885229</td>\n",
       "      <td>0.939121</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.714802</td>\n",
       "      <td>0.336866</td>\n",
       "      <td>0.377936</td>\n",
       "      <td>0.711279</td>\n",
       "      <td>0.357199</td>\n",
       "      <td>0.354080</td>\n",
       "      <td>0.865702</td>\n",
       "      <td>0.928017</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.714049</td>\n",
       "      <td>0.337065</td>\n",
       "      <td>0.376984</td>\n",
       "      <td>0.688006</td>\n",
       "      <td>0.337612</td>\n",
       "      <td>0.350394</td>\n",
       "      <td>0.884840</td>\n",
       "      <td>0.938902</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.709131</td>\n",
       "      <td>0.333665</td>\n",
       "      <td>0.375466</td>\n",
       "      <td>0.679240</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.338149</td>\n",
       "      <td>0.885196</td>\n",
       "      <td>0.939102</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.711498</td>\n",
       "      <td>0.334658</td>\n",
       "      <td>0.376840</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>0.350302</td>\n",
       "      <td>0.339094</td>\n",
       "      <td>0.877058</td>\n",
       "      <td>0.934503</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.702370</td>\n",
       "      <td>0.329795</td>\n",
       "      <td>0.372575</td>\n",
       "      <td>0.680982</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.337902</td>\n",
       "      <td>0.885279</td>\n",
       "      <td>0.939149</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.704127</td>\n",
       "      <td>0.331405</td>\n",
       "      <td>0.372722</td>\n",
       "      <td>0.680953</td>\n",
       "      <td>0.341026</td>\n",
       "      <td>0.339927</td>\n",
       "      <td>0.888529</td>\n",
       "      <td>0.940975</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.701767</td>\n",
       "      <td>0.328311</td>\n",
       "      <td>0.373455</td>\n",
       "      <td>0.677024</td>\n",
       "      <td>0.342682</td>\n",
       "      <td>0.334342</td>\n",
       "      <td>0.889603</td>\n",
       "      <td>0.941576</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.712642</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.374330</td>\n",
       "      <td>0.683332</td>\n",
       "      <td>0.336758</td>\n",
       "      <td>0.346574</td>\n",
       "      <td>0.887031</td>\n",
       "      <td>0.940134</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.702025</td>\n",
       "      <td>0.330240</td>\n",
       "      <td>0.371785</td>\n",
       "      <td>0.674551</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.339452</td>\n",
       "      <td>0.889990</td>\n",
       "      <td>0.941793</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.706553</td>\n",
       "      <td>0.331527</td>\n",
       "      <td>0.375026</td>\n",
       "      <td>0.682085</td>\n",
       "      <td>0.343966</td>\n",
       "      <td>0.338119</td>\n",
       "      <td>0.883996</td>\n",
       "      <td>0.938427</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.706754</td>\n",
       "      <td>0.332951</td>\n",
       "      <td>0.373803</td>\n",
       "      <td>0.683621</td>\n",
       "      <td>0.338949</td>\n",
       "      <td>0.344672</td>\n",
       "      <td>0.885285</td>\n",
       "      <td>0.939153</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.702693</td>\n",
       "      <td>0.329734</td>\n",
       "      <td>0.372958</td>\n",
       "      <td>0.683072</td>\n",
       "      <td>0.347147</td>\n",
       "      <td>0.335925</td>\n",
       "      <td>0.887433</td>\n",
       "      <td>0.940360</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.704077</td>\n",
       "      <td>0.330331</td>\n",
       "      <td>0.373746</td>\n",
       "      <td>0.689205</td>\n",
       "      <td>0.358389</td>\n",
       "      <td>0.330817</td>\n",
       "      <td>0.880777</td>\n",
       "      <td>0.936609</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.707831</td>\n",
       "      <td>0.333095</td>\n",
       "      <td>0.374737</td>\n",
       "      <td>0.677578</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.340299</td>\n",
       "      <td>0.886768</td>\n",
       "      <td>0.939986</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.709535</td>\n",
       "      <td>0.334425</td>\n",
       "      <td>0.375110</td>\n",
       "      <td>0.677058</td>\n",
       "      <td>0.341137</td>\n",
       "      <td>0.335921</td>\n",
       "      <td>0.889301</td>\n",
       "      <td>0.941407</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.698368</td>\n",
       "      <td>0.326081</td>\n",
       "      <td>0.372287</td>\n",
       "      <td>0.693308</td>\n",
       "      <td>0.342791</td>\n",
       "      <td>0.350517</td>\n",
       "      <td>0.883935</td>\n",
       "      <td>0.938392</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.705029</td>\n",
       "      <td>0.332164</td>\n",
       "      <td>0.372865</td>\n",
       "      <td>0.683188</td>\n",
       "      <td>0.338295</td>\n",
       "      <td>0.344892</td>\n",
       "      <td>0.886556</td>\n",
       "      <td>0.939867</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.705528</td>\n",
       "      <td>0.332408</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.685316</td>\n",
       "      <td>0.346546</td>\n",
       "      <td>0.338770</td>\n",
       "      <td>0.881539</td>\n",
       "      <td>0.937040</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.699912</td>\n",
       "      <td>0.328992</td>\n",
       "      <td>0.370920</td>\n",
       "      <td>0.710334</td>\n",
       "      <td>0.355460</td>\n",
       "      <td>0.354874</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.935949</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.702343</td>\n",
       "      <td>0.330213</td>\n",
       "      <td>0.372130</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.335741</td>\n",
       "      <td>0.336589</td>\n",
       "      <td>0.889229</td>\n",
       "      <td>0.941367</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.702182</td>\n",
       "      <td>0.330575</td>\n",
       "      <td>0.371607</td>\n",
       "      <td>0.677680</td>\n",
       "      <td>0.337056</td>\n",
       "      <td>0.340624</td>\n",
       "      <td>0.890157</td>\n",
       "      <td>0.941887</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.702056</td>\n",
       "      <td>0.328042</td>\n",
       "      <td>0.374013</td>\n",
       "      <td>0.687044</td>\n",
       "      <td>0.344940</td>\n",
       "      <td>0.342104</td>\n",
       "      <td>0.883058</td>\n",
       "      <td>0.937898</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.697174</td>\n",
       "      <td>0.327305</td>\n",
       "      <td>0.369868</td>\n",
       "      <td>0.674774</td>\n",
       "      <td>0.339839</td>\n",
       "      <td>0.334935</td>\n",
       "      <td>0.888185</td>\n",
       "      <td>0.940782</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.693696</td>\n",
       "      <td>0.326772</td>\n",
       "      <td>0.366923</td>\n",
       "      <td>0.678463</td>\n",
       "      <td>0.337472</td>\n",
       "      <td>0.340991</td>\n",
       "      <td>0.888990</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.698821</td>\n",
       "      <td>0.327565</td>\n",
       "      <td>0.371256</td>\n",
       "      <td>0.703982</td>\n",
       "      <td>0.350856</td>\n",
       "      <td>0.353126</td>\n",
       "      <td>0.875911</td>\n",
       "      <td>0.933851</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.697014</td>\n",
       "      <td>0.326428</td>\n",
       "      <td>0.370585</td>\n",
       "      <td>0.672104</td>\n",
       "      <td>0.335896</td>\n",
       "      <td>0.336209</td>\n",
       "      <td>0.889359</td>\n",
       "      <td>0.941440</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.695107</td>\n",
       "      <td>0.327397</td>\n",
       "      <td>0.367710</td>\n",
       "      <td>0.672410</td>\n",
       "      <td>0.333090</td>\n",
       "      <td>0.339320</td>\n",
       "      <td>0.889358</td>\n",
       "      <td>0.941439</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.690710</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.368610</td>\n",
       "      <td>0.681259</td>\n",
       "      <td>0.340966</td>\n",
       "      <td>0.340294</td>\n",
       "      <td>0.886658</td>\n",
       "      <td>0.939924</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.689960</td>\n",
       "      <td>0.323930</td>\n",
       "      <td>0.366030</td>\n",
       "      <td>0.693805</td>\n",
       "      <td>0.350461</td>\n",
       "      <td>0.343343</td>\n",
       "      <td>0.885405</td>\n",
       "      <td>0.939220</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.687430</td>\n",
       "      <td>0.319919</td>\n",
       "      <td>0.367511</td>\n",
       "      <td>0.686964</td>\n",
       "      <td>0.346388</td>\n",
       "      <td>0.340576</td>\n",
       "      <td>0.886952</td>\n",
       "      <td>0.940090</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.696914</td>\n",
       "      <td>0.327799</td>\n",
       "      <td>0.369114</td>\n",
       "      <td>0.678381</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.340216</td>\n",
       "      <td>0.887937</td>\n",
       "      <td>0.940643</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.689318</td>\n",
       "      <td>0.323489</td>\n",
       "      <td>0.365829</td>\n",
       "      <td>0.669035</td>\n",
       "      <td>0.337516</td>\n",
       "      <td>0.331518</td>\n",
       "      <td>0.891160</td>\n",
       "      <td>0.942448</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.681404</td>\n",
       "      <td>0.318497</td>\n",
       "      <td>0.362908</td>\n",
       "      <td>0.668075</td>\n",
       "      <td>0.339779</td>\n",
       "      <td>0.328296</td>\n",
       "      <td>0.892542</td>\n",
       "      <td>0.943221</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.682080</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.365080</td>\n",
       "      <td>0.682993</td>\n",
       "      <td>0.345732</td>\n",
       "      <td>0.337262</td>\n",
       "      <td>0.886185</td>\n",
       "      <td>0.939659</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.684301</td>\n",
       "      <td>0.318010</td>\n",
       "      <td>0.366292</td>\n",
       "      <td>0.667939</td>\n",
       "      <td>0.335745</td>\n",
       "      <td>0.332194</td>\n",
       "      <td>0.890512</td>\n",
       "      <td>0.942085</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.681748</td>\n",
       "      <td>0.317358</td>\n",
       "      <td>0.364389</td>\n",
       "      <td>0.668225</td>\n",
       "      <td>0.332786</td>\n",
       "      <td>0.335439</td>\n",
       "      <td>0.893166</td>\n",
       "      <td>0.943569</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.680269</td>\n",
       "      <td>0.317252</td>\n",
       "      <td>0.363017</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.353521</td>\n",
       "      <td>0.340923</td>\n",
       "      <td>0.878636</td>\n",
       "      <td>0.935398</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.678891</td>\n",
       "      <td>0.315823</td>\n",
       "      <td>0.363067</td>\n",
       "      <td>0.689682</td>\n",
       "      <td>0.349001</td>\n",
       "      <td>0.340682</td>\n",
       "      <td>0.885488</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.678912</td>\n",
       "      <td>0.318392</td>\n",
       "      <td>0.360521</td>\n",
       "      <td>0.677783</td>\n",
       "      <td>0.341436</td>\n",
       "      <td>0.336347</td>\n",
       "      <td>0.887496</td>\n",
       "      <td>0.940395</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.673932</td>\n",
       "      <td>0.314584</td>\n",
       "      <td>0.359347</td>\n",
       "      <td>0.687016</td>\n",
       "      <td>0.350128</td>\n",
       "      <td>0.336889</td>\n",
       "      <td>0.886859</td>\n",
       "      <td>0.940038</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.679107</td>\n",
       "      <td>0.317981</td>\n",
       "      <td>0.361126</td>\n",
       "      <td>0.666071</td>\n",
       "      <td>0.333207</td>\n",
       "      <td>0.332864</td>\n",
       "      <td>0.894137</td>\n",
       "      <td>0.944110</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.679637</td>\n",
       "      <td>0.316501</td>\n",
       "      <td>0.363136</td>\n",
       "      <td>0.682858</td>\n",
       "      <td>0.345270</td>\n",
       "      <td>0.337588</td>\n",
       "      <td>0.887069</td>\n",
       "      <td>0.940155</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.682323</td>\n",
       "      <td>0.318435</td>\n",
       "      <td>0.363888</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0.337428</td>\n",
       "      <td>0.332065</td>\n",
       "      <td>0.890377</td>\n",
       "      <td>0.942010</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.682539</td>\n",
       "      <td>0.319624</td>\n",
       "      <td>0.362915</td>\n",
       "      <td>0.694136</td>\n",
       "      <td>0.357448</td>\n",
       "      <td>0.336689</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>0.936864</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.682407</td>\n",
       "      <td>0.317773</td>\n",
       "      <td>0.364634</td>\n",
       "      <td>0.690595</td>\n",
       "      <td>0.352608</td>\n",
       "      <td>0.337986</td>\n",
       "      <td>0.884873</td>\n",
       "      <td>0.938920</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.314269</td>\n",
       "      <td>0.362531</td>\n",
       "      <td>0.685952</td>\n",
       "      <td>0.350279</td>\n",
       "      <td>0.335673</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.359633</td>\n",
       "      <td>0.665511</td>\n",
       "      <td>0.334569</td>\n",
       "      <td>0.330942</td>\n",
       "      <td>0.892561</td>\n",
       "      <td>0.943231</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.678257</td>\n",
       "      <td>0.315329</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>0.672997</td>\n",
       "      <td>0.339582</td>\n",
       "      <td>0.333415</td>\n",
       "      <td>0.889350</td>\n",
       "      <td>0.941435</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_coeff value: 0.8282222889657994.\n",
      "Better model found at epoch 2 with jaccard_coeff value: 0.83707213655886.\n",
      "Better model found at epoch 3 with jaccard_coeff value: 0.8602975696065156.\n",
      "Better model found at epoch 6 with jaccard_coeff value: 0.8674078706622291.\n",
      "Better model found at epoch 9 with jaccard_coeff value: 0.8757951701882729.\n",
      "Better model found at epoch 13 with jaccard_coeff value: 0.8780829259708557.\n",
      "Better model found at epoch 14 with jaccard_coeff value: 0.8790056406700795.\n",
      "Better model found at epoch 18 with jaccard_coeff value: 0.8831592992055081.\n",
      "Better model found at epoch 19 with jaccard_coeff value: 0.8834701264271617.\n",
      "Better model found at epoch 26 with jaccard_coeff value: 0.8841827913652879.\n",
      "Better model found at epoch 28 with jaccard_coeff value: 0.8849020349894925.\n",
      "Better model found at epoch 30 with jaccard_coeff value: 0.8876527206095074.\n",
      "Better model found at epoch 37 with jaccard_coeff value: 0.8885291519183003.\n",
      "Better model found at epoch 38 with jaccard_coeff value: 0.8896027315136933.\n",
      "Better model found at epoch 40 with jaccard_coeff value: 0.889989701224646.\n",
      "Better model found at epoch 52 with jaccard_coeff value: 0.8901572241135595.\n",
      "Better model found at epoch 63 with jaccard_coeff value: 0.8911598853652254.\n",
      "Better model found at epoch 64 with jaccard_coeff value: 0.892542440968194.\n",
      "Better model found at epoch 67 with jaccard_coeff value: 0.8931663626392005.\n",
      "Better model found at epoch 72 with jaccard_coeff value: 0.8941373339438337.\n",
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ls_cross_entropy</th>\n",
       "      <th>train_dice_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_ls_cross_entropy</th>\n",
       "      <th>valid_dice_loss</th>\n",
       "      <th>jaccard_coeff</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.883143</td>\n",
       "      <td>0.433834</td>\n",
       "      <td>0.449309</td>\n",
       "      <td>0.799498</td>\n",
       "      <td>0.402191</td>\n",
       "      <td>0.397307</td>\n",
       "      <td>0.832780</td>\n",
       "      <td>0.908761</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835021</td>\n",
       "      <td>0.406466</td>\n",
       "      <td>0.428555</td>\n",
       "      <td>0.770694</td>\n",
       "      <td>0.387045</td>\n",
       "      <td>0.383650</td>\n",
       "      <td>0.840100</td>\n",
       "      <td>0.913102</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.810684</td>\n",
       "      <td>0.391017</td>\n",
       "      <td>0.419667</td>\n",
       "      <td>0.760693</td>\n",
       "      <td>0.385682</td>\n",
       "      <td>0.375012</td>\n",
       "      <td>0.852254</td>\n",
       "      <td>0.920234</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.787396</td>\n",
       "      <td>0.377064</td>\n",
       "      <td>0.410332</td>\n",
       "      <td>0.769935</td>\n",
       "      <td>0.405512</td>\n",
       "      <td>0.364423</td>\n",
       "      <td>0.834818</td>\n",
       "      <td>0.909974</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.777335</td>\n",
       "      <td>0.373458</td>\n",
       "      <td>0.403877</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>0.365357</td>\n",
       "      <td>0.371358</td>\n",
       "      <td>0.867008</td>\n",
       "      <td>0.928767</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.770257</td>\n",
       "      <td>0.370087</td>\n",
       "      <td>0.400169</td>\n",
       "      <td>0.720155</td>\n",
       "      <td>0.360340</td>\n",
       "      <td>0.359816</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.928427</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.767200</td>\n",
       "      <td>0.365237</td>\n",
       "      <td>0.401963</td>\n",
       "      <td>0.730919</td>\n",
       "      <td>0.355294</td>\n",
       "      <td>0.375625</td>\n",
       "      <td>0.866182</td>\n",
       "      <td>0.928293</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.766284</td>\n",
       "      <td>0.366651</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>0.871123</td>\n",
       "      <td>0.931123</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.753006</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.392106</td>\n",
       "      <td>0.728245</td>\n",
       "      <td>0.366014</td>\n",
       "      <td>0.362231</td>\n",
       "      <td>0.860531</td>\n",
       "      <td>0.925038</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>0.390966</td>\n",
       "      <td>0.700305</td>\n",
       "      <td>0.358032</td>\n",
       "      <td>0.342272</td>\n",
       "      <td>0.879259</td>\n",
       "      <td>0.935751</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.747563</td>\n",
       "      <td>0.355706</td>\n",
       "      <td>0.391856</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.353827</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.873177</td>\n",
       "      <td>0.932295</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.737289</td>\n",
       "      <td>0.350282</td>\n",
       "      <td>0.387007</td>\n",
       "      <td>0.699462</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.351362</td>\n",
       "      <td>0.876276</td>\n",
       "      <td>0.934059</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.745915</td>\n",
       "      <td>0.355462</td>\n",
       "      <td>0.390453</td>\n",
       "      <td>0.690520</td>\n",
       "      <td>0.343198</td>\n",
       "      <td>0.347322</td>\n",
       "      <td>0.883785</td>\n",
       "      <td>0.938308</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.732383</td>\n",
       "      <td>0.345467</td>\n",
       "      <td>0.386916</td>\n",
       "      <td>0.697280</td>\n",
       "      <td>0.346196</td>\n",
       "      <td>0.351084</td>\n",
       "      <td>0.882705</td>\n",
       "      <td>0.937699</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.738655</td>\n",
       "      <td>0.351447</td>\n",
       "      <td>0.387207</td>\n",
       "      <td>0.705643</td>\n",
       "      <td>0.345632</td>\n",
       "      <td>0.360011</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>0.936396</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.734592</td>\n",
       "      <td>0.348529</td>\n",
       "      <td>0.386063</td>\n",
       "      <td>0.693676</td>\n",
       "      <td>0.337773</td>\n",
       "      <td>0.355903</td>\n",
       "      <td>0.883874</td>\n",
       "      <td>0.938358</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.732907</td>\n",
       "      <td>0.349686</td>\n",
       "      <td>0.383221</td>\n",
       "      <td>0.689406</td>\n",
       "      <td>0.341475</td>\n",
       "      <td>0.347931</td>\n",
       "      <td>0.884074</td>\n",
       "      <td>0.938470</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.733651</td>\n",
       "      <td>0.348379</td>\n",
       "      <td>0.385272</td>\n",
       "      <td>0.701096</td>\n",
       "      <td>0.342756</td>\n",
       "      <td>0.358339</td>\n",
       "      <td>0.883822</td>\n",
       "      <td>0.938329</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.721623</td>\n",
       "      <td>0.341666</td>\n",
       "      <td>0.379957</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.341153</td>\n",
       "      <td>0.348879</td>\n",
       "      <td>0.880193</td>\n",
       "      <td>0.936279</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.730604</td>\n",
       "      <td>0.346528</td>\n",
       "      <td>0.384077</td>\n",
       "      <td>0.696017</td>\n",
       "      <td>0.343032</td>\n",
       "      <td>0.352985</td>\n",
       "      <td>0.881977</td>\n",
       "      <td>0.937288</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.730255</td>\n",
       "      <td>0.345003</td>\n",
       "      <td>0.385252</td>\n",
       "      <td>0.703056</td>\n",
       "      <td>0.340990</td>\n",
       "      <td>0.362067</td>\n",
       "      <td>0.871770</td>\n",
       "      <td>0.931492</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.727558</td>\n",
       "      <td>0.347572</td>\n",
       "      <td>0.379986</td>\n",
       "      <td>0.691515</td>\n",
       "      <td>0.340090</td>\n",
       "      <td>0.351425</td>\n",
       "      <td>0.883339</td>\n",
       "      <td>0.938056</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.730241</td>\n",
       "      <td>0.347133</td>\n",
       "      <td>0.383108</td>\n",
       "      <td>0.693160</td>\n",
       "      <td>0.348182</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.940049</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.718521</td>\n",
       "      <td>0.340311</td>\n",
       "      <td>0.378210</td>\n",
       "      <td>0.700393</td>\n",
       "      <td>0.351740</td>\n",
       "      <td>0.348654</td>\n",
       "      <td>0.874735</td>\n",
       "      <td>0.933183</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.725775</td>\n",
       "      <td>0.343575</td>\n",
       "      <td>0.382199</td>\n",
       "      <td>0.683090</td>\n",
       "      <td>0.334627</td>\n",
       "      <td>0.348464</td>\n",
       "      <td>0.888609</td>\n",
       "      <td>0.941019</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.722804</td>\n",
       "      <td>0.341341</td>\n",
       "      <td>0.381463</td>\n",
       "      <td>0.702236</td>\n",
       "      <td>0.346401</td>\n",
       "      <td>0.355835</td>\n",
       "      <td>0.873884</td>\n",
       "      <td>0.932698</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.715388</td>\n",
       "      <td>0.336948</td>\n",
       "      <td>0.378441</td>\n",
       "      <td>0.713203</td>\n",
       "      <td>0.346483</td>\n",
       "      <td>0.366720</td>\n",
       "      <td>0.873340</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.719669</td>\n",
       "      <td>0.340367</td>\n",
       "      <td>0.379303</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.348410</td>\n",
       "      <td>0.346526</td>\n",
       "      <td>0.878539</td>\n",
       "      <td>0.935343</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.717468</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.380868</td>\n",
       "      <td>0.691427</td>\n",
       "      <td>0.352114</td>\n",
       "      <td>0.339313</td>\n",
       "      <td>0.880432</td>\n",
       "      <td>0.936415</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.721608</td>\n",
       "      <td>0.341606</td>\n",
       "      <td>0.380003</td>\n",
       "      <td>0.695625</td>\n",
       "      <td>0.336471</td>\n",
       "      <td>0.359154</td>\n",
       "      <td>0.886725</td>\n",
       "      <td>0.939962</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.717150</td>\n",
       "      <td>0.340229</td>\n",
       "      <td>0.376921</td>\n",
       "      <td>0.686273</td>\n",
       "      <td>0.337661</td>\n",
       "      <td>0.348613</td>\n",
       "      <td>0.888598</td>\n",
       "      <td>0.941013</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.712932</td>\n",
       "      <td>0.337719</td>\n",
       "      <td>0.375213</td>\n",
       "      <td>0.677554</td>\n",
       "      <td>0.340035</td>\n",
       "      <td>0.337519</td>\n",
       "      <td>0.890764</td>\n",
       "      <td>0.942227</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.709045</td>\n",
       "      <td>0.334779</td>\n",
       "      <td>0.374266</td>\n",
       "      <td>0.682899</td>\n",
       "      <td>0.340425</td>\n",
       "      <td>0.342474</td>\n",
       "      <td>0.884683</td>\n",
       "      <td>0.938814</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.714547</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>0.377879</td>\n",
       "      <td>0.696949</td>\n",
       "      <td>0.343804</td>\n",
       "      <td>0.353144</td>\n",
       "      <td>0.885697</td>\n",
       "      <td>0.939384</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.705881</td>\n",
       "      <td>0.331326</td>\n",
       "      <td>0.374555</td>\n",
       "      <td>0.675760</td>\n",
       "      <td>0.335289</td>\n",
       "      <td>0.340471</td>\n",
       "      <td>0.890638</td>\n",
       "      <td>0.942156</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.708423</td>\n",
       "      <td>0.334054</td>\n",
       "      <td>0.374369</td>\n",
       "      <td>0.687433</td>\n",
       "      <td>0.346090</td>\n",
       "      <td>0.341343</td>\n",
       "      <td>0.887413</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.709060</td>\n",
       "      <td>0.333281</td>\n",
       "      <td>0.375779</td>\n",
       "      <td>0.695867</td>\n",
       "      <td>0.344616</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.886291</td>\n",
       "      <td>0.939718</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.713677</td>\n",
       "      <td>0.336908</td>\n",
       "      <td>0.376769</td>\n",
       "      <td>0.685877</td>\n",
       "      <td>0.338158</td>\n",
       "      <td>0.347719</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.940011</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.713105</td>\n",
       "      <td>0.336883</td>\n",
       "      <td>0.376221</td>\n",
       "      <td>0.704363</td>\n",
       "      <td>0.344025</td>\n",
       "      <td>0.360338</td>\n",
       "      <td>0.873709</td>\n",
       "      <td>0.932598</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.704231</td>\n",
       "      <td>0.332773</td>\n",
       "      <td>0.371457</td>\n",
       "      <td>0.695766</td>\n",
       "      <td>0.354704</td>\n",
       "      <td>0.341062</td>\n",
       "      <td>0.874036</td>\n",
       "      <td>0.932785</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.714577</td>\n",
       "      <td>0.337973</td>\n",
       "      <td>0.376604</td>\n",
       "      <td>0.680429</td>\n",
       "      <td>0.331721</td>\n",
       "      <td>0.348708</td>\n",
       "      <td>0.888030</td>\n",
       "      <td>0.940695</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.709428</td>\n",
       "      <td>0.335644</td>\n",
       "      <td>0.373784</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.332917</td>\n",
       "      <td>0.342296</td>\n",
       "      <td>0.889105</td>\n",
       "      <td>0.941298</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.701990</td>\n",
       "      <td>0.331880</td>\n",
       "      <td>0.370109</td>\n",
       "      <td>0.674738</td>\n",
       "      <td>0.335874</td>\n",
       "      <td>0.338864</td>\n",
       "      <td>0.888010</td>\n",
       "      <td>0.940684</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.705562</td>\n",
       "      <td>0.332810</td>\n",
       "      <td>0.372751</td>\n",
       "      <td>0.687276</td>\n",
       "      <td>0.340427</td>\n",
       "      <td>0.346849</td>\n",
       "      <td>0.883727</td>\n",
       "      <td>0.938275</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.705535</td>\n",
       "      <td>0.331073</td>\n",
       "      <td>0.374463</td>\n",
       "      <td>0.702573</td>\n",
       "      <td>0.345951</td>\n",
       "      <td>0.356623</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>0.940232</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.331246</td>\n",
       "      <td>0.371470</td>\n",
       "      <td>0.685123</td>\n",
       "      <td>0.347677</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.888374</td>\n",
       "      <td>0.940888</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.702542</td>\n",
       "      <td>0.329305</td>\n",
       "      <td>0.373238</td>\n",
       "      <td>0.672841</td>\n",
       "      <td>0.328936</td>\n",
       "      <td>0.343905</td>\n",
       "      <td>0.892961</td>\n",
       "      <td>0.943454</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.700332</td>\n",
       "      <td>0.328005</td>\n",
       "      <td>0.372328</td>\n",
       "      <td>0.687718</td>\n",
       "      <td>0.347505</td>\n",
       "      <td>0.340213</td>\n",
       "      <td>0.883570</td>\n",
       "      <td>0.938187</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.704101</td>\n",
       "      <td>0.331941</td>\n",
       "      <td>0.372160</td>\n",
       "      <td>0.667875</td>\n",
       "      <td>0.330147</td>\n",
       "      <td>0.337728</td>\n",
       "      <td>0.893888</td>\n",
       "      <td>0.943971</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.700746</td>\n",
       "      <td>0.329910</td>\n",
       "      <td>0.370836</td>\n",
       "      <td>0.695297</td>\n",
       "      <td>0.350984</td>\n",
       "      <td>0.344314</td>\n",
       "      <td>0.885111</td>\n",
       "      <td>0.939055</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.697919</td>\n",
       "      <td>0.327976</td>\n",
       "      <td>0.369943</td>\n",
       "      <td>0.669618</td>\n",
       "      <td>0.338510</td>\n",
       "      <td>0.331108</td>\n",
       "      <td>0.892150</td>\n",
       "      <td>0.943001</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>0.331479</td>\n",
       "      <td>0.368870</td>\n",
       "      <td>0.683434</td>\n",
       "      <td>0.341703</td>\n",
       "      <td>0.341732</td>\n",
       "      <td>0.888212</td>\n",
       "      <td>0.940797</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.700882</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>0.372053</td>\n",
       "      <td>0.674245</td>\n",
       "      <td>0.344336</td>\n",
       "      <td>0.329909</td>\n",
       "      <td>0.888710</td>\n",
       "      <td>0.941076</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.697979</td>\n",
       "      <td>0.328593</td>\n",
       "      <td>0.369385</td>\n",
       "      <td>0.672905</td>\n",
       "      <td>0.336777</td>\n",
       "      <td>0.336128</td>\n",
       "      <td>0.893268</td>\n",
       "      <td>0.943625</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>0.328913</td>\n",
       "      <td>0.367639</td>\n",
       "      <td>0.681821</td>\n",
       "      <td>0.333260</td>\n",
       "      <td>0.348560</td>\n",
       "      <td>0.891698</td>\n",
       "      <td>0.942749</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.694343</td>\n",
       "      <td>0.324367</td>\n",
       "      <td>0.369976</td>\n",
       "      <td>0.687929</td>\n",
       "      <td>0.351299</td>\n",
       "      <td>0.336630</td>\n",
       "      <td>0.887844</td>\n",
       "      <td>0.940590</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.689460</td>\n",
       "      <td>0.325498</td>\n",
       "      <td>0.363963</td>\n",
       "      <td>0.665566</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>0.334199</td>\n",
       "      <td>0.894674</td>\n",
       "      <td>0.944409</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.692854</td>\n",
       "      <td>0.325682</td>\n",
       "      <td>0.367172</td>\n",
       "      <td>0.674442</td>\n",
       "      <td>0.334288</td>\n",
       "      <td>0.340153</td>\n",
       "      <td>0.891448</td>\n",
       "      <td>0.942609</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.697998</td>\n",
       "      <td>0.329656</td>\n",
       "      <td>0.368342</td>\n",
       "      <td>0.680859</td>\n",
       "      <td>0.337653</td>\n",
       "      <td>0.343206</td>\n",
       "      <td>0.886056</td>\n",
       "      <td>0.939586</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.686916</td>\n",
       "      <td>0.320004</td>\n",
       "      <td>0.366913</td>\n",
       "      <td>0.665536</td>\n",
       "      <td>0.333425</td>\n",
       "      <td>0.332111</td>\n",
       "      <td>0.895279</td>\n",
       "      <td>0.944747</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.690718</td>\n",
       "      <td>0.322613</td>\n",
       "      <td>0.368105</td>\n",
       "      <td>0.668839</td>\n",
       "      <td>0.332322</td>\n",
       "      <td>0.336517</td>\n",
       "      <td>0.893424</td>\n",
       "      <td>0.943712</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.697787</td>\n",
       "      <td>0.328133</td>\n",
       "      <td>0.369654</td>\n",
       "      <td>0.678928</td>\n",
       "      <td>0.337319</td>\n",
       "      <td>0.341609</td>\n",
       "      <td>0.888375</td>\n",
       "      <td>0.940888</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.693476</td>\n",
       "      <td>0.324145</td>\n",
       "      <td>0.369331</td>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.352062</td>\n",
       "      <td>0.348280</td>\n",
       "      <td>0.874917</td>\n",
       "      <td>0.933286</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.690868</td>\n",
       "      <td>0.323680</td>\n",
       "      <td>0.367187</td>\n",
       "      <td>0.686942</td>\n",
       "      <td>0.345510</td>\n",
       "      <td>0.341432</td>\n",
       "      <td>0.878008</td>\n",
       "      <td>0.935042</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.684584</td>\n",
       "      <td>0.318553</td>\n",
       "      <td>0.366031</td>\n",
       "      <td>0.663655</td>\n",
       "      <td>0.330991</td>\n",
       "      <td>0.332663</td>\n",
       "      <td>0.894674</td>\n",
       "      <td>0.944410</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.683688</td>\n",
       "      <td>0.317343</td>\n",
       "      <td>0.366344</td>\n",
       "      <td>0.665027</td>\n",
       "      <td>0.333103</td>\n",
       "      <td>0.331923</td>\n",
       "      <td>0.894357</td>\n",
       "      <td>0.944233</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.684256</td>\n",
       "      <td>0.319346</td>\n",
       "      <td>0.364910</td>\n",
       "      <td>0.688832</td>\n",
       "      <td>0.345443</td>\n",
       "      <td>0.343389</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>0.938732</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.686583</td>\n",
       "      <td>0.321524</td>\n",
       "      <td>0.365060</td>\n",
       "      <td>0.669744</td>\n",
       "      <td>0.332998</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.893140</td>\n",
       "      <td>0.943554</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.682716</td>\n",
       "      <td>0.319375</td>\n",
       "      <td>0.363340</td>\n",
       "      <td>0.686284</td>\n",
       "      <td>0.347159</td>\n",
       "      <td>0.339126</td>\n",
       "      <td>0.880634</td>\n",
       "      <td>0.936529</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.684695</td>\n",
       "      <td>0.319854</td>\n",
       "      <td>0.364841</td>\n",
       "      <td>0.665144</td>\n",
       "      <td>0.331506</td>\n",
       "      <td>0.333638</td>\n",
       "      <td>0.894230</td>\n",
       "      <td>0.944162</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.675972</td>\n",
       "      <td>0.313163</td>\n",
       "      <td>0.362809</td>\n",
       "      <td>0.671014</td>\n",
       "      <td>0.337275</td>\n",
       "      <td>0.333739</td>\n",
       "      <td>0.890653</td>\n",
       "      <td>0.942164</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.681726</td>\n",
       "      <td>0.317384</td>\n",
       "      <td>0.364343</td>\n",
       "      <td>0.664492</td>\n",
       "      <td>0.330037</td>\n",
       "      <td>0.334456</td>\n",
       "      <td>0.895430</td>\n",
       "      <td>0.944831</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.678327</td>\n",
       "      <td>0.315985</td>\n",
       "      <td>0.362342</td>\n",
       "      <td>0.670792</td>\n",
       "      <td>0.335822</td>\n",
       "      <td>0.334970</td>\n",
       "      <td>0.891674</td>\n",
       "      <td>0.942735</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.675778</td>\n",
       "      <td>0.316009</td>\n",
       "      <td>0.359768</td>\n",
       "      <td>0.661736</td>\n",
       "      <td>0.330138</td>\n",
       "      <td>0.331597</td>\n",
       "      <td>0.896201</td>\n",
       "      <td>0.945260</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.681877</td>\n",
       "      <td>0.320159</td>\n",
       "      <td>0.361718</td>\n",
       "      <td>0.675074</td>\n",
       "      <td>0.338114</td>\n",
       "      <td>0.336960</td>\n",
       "      <td>0.891906</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.677765</td>\n",
       "      <td>0.316360</td>\n",
       "      <td>0.361406</td>\n",
       "      <td>0.676018</td>\n",
       "      <td>0.340667</td>\n",
       "      <td>0.335352</td>\n",
       "      <td>0.892006</td>\n",
       "      <td>0.942921</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.681035</td>\n",
       "      <td>0.319314</td>\n",
       "      <td>0.361721</td>\n",
       "      <td>0.666083</td>\n",
       "      <td>0.333480</td>\n",
       "      <td>0.332603</td>\n",
       "      <td>0.892932</td>\n",
       "      <td>0.943438</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.683586</td>\n",
       "      <td>0.320009</td>\n",
       "      <td>0.363577</td>\n",
       "      <td>0.681361</td>\n",
       "      <td>0.344822</td>\n",
       "      <td>0.336539</td>\n",
       "      <td>0.890685</td>\n",
       "      <td>0.942182</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.677327</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.364127</td>\n",
       "      <td>0.662804</td>\n",
       "      <td>0.331436</td>\n",
       "      <td>0.331368</td>\n",
       "      <td>0.895891</td>\n",
       "      <td>0.945087</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.677765</td>\n",
       "      <td>0.313022</td>\n",
       "      <td>0.364744</td>\n",
       "      <td>0.680946</td>\n",
       "      <td>0.340618</td>\n",
       "      <td>0.340329</td>\n",
       "      <td>0.889816</td>\n",
       "      <td>0.941696</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_coeff value: 0.8327797048380502.\n",
      "Better model found at epoch 1 with jaccard_coeff value: 0.8400995122017069.\n",
      "Better model found at epoch 2 with jaccard_coeff value: 0.8522539011039554.\n",
      "Better model found at epoch 4 with jaccard_coeff value: 0.8670075968148014.\n",
      "Better model found at epoch 7 with jaccard_coeff value: 0.8711231252731187.\n",
      "Better model found at epoch 9 with jaccard_coeff value: 0.8792586836336503.\n",
      "Better model found at epoch 12 with jaccard_coeff value: 0.8837851341936729.\n",
      "Better model found at epoch 15 with jaccard_coeff value: 0.8838740472834229.\n",
      "Better model found at epoch 16 with jaccard_coeff value: 0.8840735644453379.\n",
      "Better model found at epoch 22 with jaccard_coeff value: 0.8868805206212182.\n",
      "Better model found at epoch 24 with jaccard_coeff value: 0.888608729214288.\n",
      "Better model found at epoch 31 with jaccard_coeff value: 0.8907643283941726.\n",
      "Better model found at epoch 46 with jaccard_coeff value: 0.8929605315981256.\n",
      "Better model found at epoch 48 with jaccard_coeff value: 0.8938881634464059.\n",
      "Better model found at epoch 56 with jaccard_coeff value: 0.8946739707968864.\n",
      "Better model found at epoch 59 with jaccard_coeff value: 0.8952794062461168.\n",
      "Better model found at epoch 71 with jaccard_coeff value: 0.895430295462715.\n",
      "Better model found at epoch 73 with jaccard_coeff value: 0.8962013044453832.\n",
      "Fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ls_cross_entropy</th>\n",
       "      <th>train_dice_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_ls_cross_entropy</th>\n",
       "      <th>valid_dice_loss</th>\n",
       "      <th>jaccard_coeff</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.898989</td>\n",
       "      <td>0.445086</td>\n",
       "      <td>0.453903</td>\n",
       "      <td>0.839630</td>\n",
       "      <td>0.434530</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.808396</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.430109</td>\n",
       "      <td>0.775513</td>\n",
       "      <td>0.392899</td>\n",
       "      <td>0.382614</td>\n",
       "      <td>0.838820</td>\n",
       "      <td>0.912346</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.801885</td>\n",
       "      <td>0.385266</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>0.771136</td>\n",
       "      <td>0.371381</td>\n",
       "      <td>0.399756</td>\n",
       "      <td>0.851715</td>\n",
       "      <td>0.919920</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.795537</td>\n",
       "      <td>0.382434</td>\n",
       "      <td>0.413103</td>\n",
       "      <td>0.739694</td>\n",
       "      <td>0.361286</td>\n",
       "      <td>0.378408</td>\n",
       "      <td>0.862817</td>\n",
       "      <td>0.926357</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.778425</td>\n",
       "      <td>0.374260</td>\n",
       "      <td>0.404165</td>\n",
       "      <td>0.725119</td>\n",
       "      <td>0.360561</td>\n",
       "      <td>0.364557</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.930061</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.765388</td>\n",
       "      <td>0.368731</td>\n",
       "      <td>0.396657</td>\n",
       "      <td>0.723475</td>\n",
       "      <td>0.355090</td>\n",
       "      <td>0.368385</td>\n",
       "      <td>0.862584</td>\n",
       "      <td>0.926223</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>0.360234</td>\n",
       "      <td>0.395718</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.362542</td>\n",
       "      <td>0.356443</td>\n",
       "      <td>0.864916</td>\n",
       "      <td>0.927566</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.754485</td>\n",
       "      <td>0.359135</td>\n",
       "      <td>0.395350</td>\n",
       "      <td>0.711237</td>\n",
       "      <td>0.355141</td>\n",
       "      <td>0.356096</td>\n",
       "      <td>0.871404</td>\n",
       "      <td>0.931284</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.757383</td>\n",
       "      <td>0.362514</td>\n",
       "      <td>0.394869</td>\n",
       "      <td>0.725846</td>\n",
       "      <td>0.357365</td>\n",
       "      <td>0.368481</td>\n",
       "      <td>0.866379</td>\n",
       "      <td>0.928406</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.748823</td>\n",
       "      <td>0.357309</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.711830</td>\n",
       "      <td>0.363353</td>\n",
       "      <td>0.348477</td>\n",
       "      <td>0.870499</td>\n",
       "      <td>0.930767</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.745553</td>\n",
       "      <td>0.353895</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>0.717175</td>\n",
       "      <td>0.359108</td>\n",
       "      <td>0.358067</td>\n",
       "      <td>0.868714</td>\n",
       "      <td>0.929745</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.742235</td>\n",
       "      <td>0.353186</td>\n",
       "      <td>0.389048</td>\n",
       "      <td>0.706840</td>\n",
       "      <td>0.355639</td>\n",
       "      <td>0.351201</td>\n",
       "      <td>0.870824</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.350894</td>\n",
       "      <td>0.384867</td>\n",
       "      <td>0.702322</td>\n",
       "      <td>0.352946</td>\n",
       "      <td>0.349376</td>\n",
       "      <td>0.876032</td>\n",
       "      <td>0.933920</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.748481</td>\n",
       "      <td>0.357295</td>\n",
       "      <td>0.391186</td>\n",
       "      <td>0.698538</td>\n",
       "      <td>0.344836</td>\n",
       "      <td>0.353702</td>\n",
       "      <td>0.875163</td>\n",
       "      <td>0.933426</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.731917</td>\n",
       "      <td>0.348964</td>\n",
       "      <td>0.382953</td>\n",
       "      <td>0.704533</td>\n",
       "      <td>0.343614</td>\n",
       "      <td>0.360919</td>\n",
       "      <td>0.876022</td>\n",
       "      <td>0.933915</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.739884</td>\n",
       "      <td>0.352760</td>\n",
       "      <td>0.387125</td>\n",
       "      <td>0.703922</td>\n",
       "      <td>0.348306</td>\n",
       "      <td>0.355617</td>\n",
       "      <td>0.876550</td>\n",
       "      <td>0.934215</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.732817</td>\n",
       "      <td>0.346289</td>\n",
       "      <td>0.386528</td>\n",
       "      <td>0.704434</td>\n",
       "      <td>0.357864</td>\n",
       "      <td>0.346570</td>\n",
       "      <td>0.873056</td>\n",
       "      <td>0.932226</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.729207</td>\n",
       "      <td>0.343874</td>\n",
       "      <td>0.385333</td>\n",
       "      <td>0.713175</td>\n",
       "      <td>0.362683</td>\n",
       "      <td>0.350491</td>\n",
       "      <td>0.876071</td>\n",
       "      <td>0.933942</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.728668</td>\n",
       "      <td>0.344351</td>\n",
       "      <td>0.384316</td>\n",
       "      <td>0.714617</td>\n",
       "      <td>0.356260</td>\n",
       "      <td>0.358357</td>\n",
       "      <td>0.869888</td>\n",
       "      <td>0.930417</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.726134</td>\n",
       "      <td>0.343058</td>\n",
       "      <td>0.383075</td>\n",
       "      <td>0.697775</td>\n",
       "      <td>0.350961</td>\n",
       "      <td>0.346814</td>\n",
       "      <td>0.874102</td>\n",
       "      <td>0.932822</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.724825</td>\n",
       "      <td>0.341745</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.726654</td>\n",
       "      <td>0.383197</td>\n",
       "      <td>0.343457</td>\n",
       "      <td>0.860152</td>\n",
       "      <td>0.924819</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.720133</td>\n",
       "      <td>0.341421</td>\n",
       "      <td>0.378712</td>\n",
       "      <td>0.684788</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.346246</td>\n",
       "      <td>0.883953</td>\n",
       "      <td>0.938403</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.727722</td>\n",
       "      <td>0.345264</td>\n",
       "      <td>0.382458</td>\n",
       "      <td>0.686930</td>\n",
       "      <td>0.343381</td>\n",
       "      <td>0.343549</td>\n",
       "      <td>0.882201</td>\n",
       "      <td>0.937414</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.340768</td>\n",
       "      <td>0.380056</td>\n",
       "      <td>0.701295</td>\n",
       "      <td>0.356117</td>\n",
       "      <td>0.345178</td>\n",
       "      <td>0.875083</td>\n",
       "      <td>0.933381</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.719517</td>\n",
       "      <td>0.339960</td>\n",
       "      <td>0.379557</td>\n",
       "      <td>0.683459</td>\n",
       "      <td>0.342186</td>\n",
       "      <td>0.341273</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>0.938170</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>0.341090</td>\n",
       "      <td>0.381545</td>\n",
       "      <td>0.709490</td>\n",
       "      <td>0.363731</td>\n",
       "      <td>0.345760</td>\n",
       "      <td>0.871669</td>\n",
       "      <td>0.931435</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.722198</td>\n",
       "      <td>0.339849</td>\n",
       "      <td>0.382349</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>0.359336</td>\n",
       "      <td>0.347695</td>\n",
       "      <td>0.876531</td>\n",
       "      <td>0.934204</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.727044</td>\n",
       "      <td>0.344463</td>\n",
       "      <td>0.382582</td>\n",
       "      <td>0.695633</td>\n",
       "      <td>0.343238</td>\n",
       "      <td>0.352395</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.933550</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.717912</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.377996</td>\n",
       "      <td>0.680306</td>\n",
       "      <td>0.342778</td>\n",
       "      <td>0.337527</td>\n",
       "      <td>0.888238</td>\n",
       "      <td>0.940811</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.717693</td>\n",
       "      <td>0.339222</td>\n",
       "      <td>0.378471</td>\n",
       "      <td>0.714184</td>\n",
       "      <td>0.350792</td>\n",
       "      <td>0.363392</td>\n",
       "      <td>0.882089</td>\n",
       "      <td>0.937351</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.722415</td>\n",
       "      <td>0.341083</td>\n",
       "      <td>0.381333</td>\n",
       "      <td>0.699641</td>\n",
       "      <td>0.340829</td>\n",
       "      <td>0.358812</td>\n",
       "      <td>0.885344</td>\n",
       "      <td>0.939186</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.712310</td>\n",
       "      <td>0.338717</td>\n",
       "      <td>0.373594</td>\n",
       "      <td>0.696864</td>\n",
       "      <td>0.353524</td>\n",
       "      <td>0.343340</td>\n",
       "      <td>0.874175</td>\n",
       "      <td>0.932864</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.712733</td>\n",
       "      <td>0.335088</td>\n",
       "      <td>0.377645</td>\n",
       "      <td>0.696808</td>\n",
       "      <td>0.355538</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>0.877961</td>\n",
       "      <td>0.935015</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.714200</td>\n",
       "      <td>0.335481</td>\n",
       "      <td>0.378720</td>\n",
       "      <td>0.686285</td>\n",
       "      <td>0.343544</td>\n",
       "      <td>0.342741</td>\n",
       "      <td>0.887309</td>\n",
       "      <td>0.940290</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.712116</td>\n",
       "      <td>0.335038</td>\n",
       "      <td>0.377078</td>\n",
       "      <td>0.700244</td>\n",
       "      <td>0.352135</td>\n",
       "      <td>0.348109</td>\n",
       "      <td>0.871646</td>\n",
       "      <td>0.931422</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.705019</td>\n",
       "      <td>0.330572</td>\n",
       "      <td>0.374447</td>\n",
       "      <td>0.701843</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.348560</td>\n",
       "      <td>0.866808</td>\n",
       "      <td>0.928652</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.717521</td>\n",
       "      <td>0.339302</td>\n",
       "      <td>0.378219</td>\n",
       "      <td>0.689732</td>\n",
       "      <td>0.347209</td>\n",
       "      <td>0.342524</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.939440</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.333294</td>\n",
       "      <td>0.372507</td>\n",
       "      <td>0.679589</td>\n",
       "      <td>0.340255</td>\n",
       "      <td>0.339334</td>\n",
       "      <td>0.889383</td>\n",
       "      <td>0.941453</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.712857</td>\n",
       "      <td>0.335477</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>0.703952</td>\n",
       "      <td>0.360387</td>\n",
       "      <td>0.343565</td>\n",
       "      <td>0.878915</td>\n",
       "      <td>0.935556</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.716278</td>\n",
       "      <td>0.339640</td>\n",
       "      <td>0.376638</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>0.336488</td>\n",
       "      <td>0.352909</td>\n",
       "      <td>0.881657</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.705790</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.374490</td>\n",
       "      <td>0.670576</td>\n",
       "      <td>0.337111</td>\n",
       "      <td>0.333466</td>\n",
       "      <td>0.891351</td>\n",
       "      <td>0.942555</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.707787</td>\n",
       "      <td>0.333952</td>\n",
       "      <td>0.373835</td>\n",
       "      <td>0.707931</td>\n",
       "      <td>0.355780</td>\n",
       "      <td>0.352150</td>\n",
       "      <td>0.878488</td>\n",
       "      <td>0.935314</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.335813</td>\n",
       "      <td>0.376196</td>\n",
       "      <td>0.690419</td>\n",
       "      <td>0.337057</td>\n",
       "      <td>0.353362</td>\n",
       "      <td>0.885446</td>\n",
       "      <td>0.939243</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.703757</td>\n",
       "      <td>0.333904</td>\n",
       "      <td>0.369853</td>\n",
       "      <td>0.668209</td>\n",
       "      <td>0.329519</td>\n",
       "      <td>0.338690</td>\n",
       "      <td>0.893634</td>\n",
       "      <td>0.943830</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.702202</td>\n",
       "      <td>0.330776</td>\n",
       "      <td>0.371426</td>\n",
       "      <td>0.689775</td>\n",
       "      <td>0.346198</td>\n",
       "      <td>0.343577</td>\n",
       "      <td>0.887773</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.694445</td>\n",
       "      <td>0.324589</td>\n",
       "      <td>0.369856</td>\n",
       "      <td>0.690437</td>\n",
       "      <td>0.350322</td>\n",
       "      <td>0.340115</td>\n",
       "      <td>0.878783</td>\n",
       "      <td>0.935481</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.703395</td>\n",
       "      <td>0.329809</td>\n",
       "      <td>0.373585</td>\n",
       "      <td>0.705919</td>\n",
       "      <td>0.352981</td>\n",
       "      <td>0.352937</td>\n",
       "      <td>0.867466</td>\n",
       "      <td>0.929030</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.704911</td>\n",
       "      <td>0.330758</td>\n",
       "      <td>0.374153</td>\n",
       "      <td>0.671860</td>\n",
       "      <td>0.333106</td>\n",
       "      <td>0.338753</td>\n",
       "      <td>0.889761</td>\n",
       "      <td>0.941665</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.696802</td>\n",
       "      <td>0.328686</td>\n",
       "      <td>0.368116</td>\n",
       "      <td>0.674380</td>\n",
       "      <td>0.342344</td>\n",
       "      <td>0.332037</td>\n",
       "      <td>0.888487</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.698344</td>\n",
       "      <td>0.327857</td>\n",
       "      <td>0.370487</td>\n",
       "      <td>0.668192</td>\n",
       "      <td>0.336242</td>\n",
       "      <td>0.331950</td>\n",
       "      <td>0.893017</td>\n",
       "      <td>0.943486</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.702228</td>\n",
       "      <td>0.328906</td>\n",
       "      <td>0.373322</td>\n",
       "      <td>0.674388</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.343288</td>\n",
       "      <td>0.889941</td>\n",
       "      <td>0.941766</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.700064</td>\n",
       "      <td>0.325799</td>\n",
       "      <td>0.374264</td>\n",
       "      <td>0.674535</td>\n",
       "      <td>0.331137</td>\n",
       "      <td>0.343398</td>\n",
       "      <td>0.892422</td>\n",
       "      <td>0.943153</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.689050</td>\n",
       "      <td>0.323450</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.724810</td>\n",
       "      <td>0.376432</td>\n",
       "      <td>0.348378</td>\n",
       "      <td>0.857259</td>\n",
       "      <td>0.923144</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.696992</td>\n",
       "      <td>0.326952</td>\n",
       "      <td>0.370040</td>\n",
       "      <td>0.678566</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>0.338189</td>\n",
       "      <td>0.888388</td>\n",
       "      <td>0.940896</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.690540</td>\n",
       "      <td>0.323023</td>\n",
       "      <td>0.367517</td>\n",
       "      <td>0.674674</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>0.334296</td>\n",
       "      <td>0.891532</td>\n",
       "      <td>0.942656</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.701975</td>\n",
       "      <td>0.329655</td>\n",
       "      <td>0.372321</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.334456</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>0.892177</td>\n",
       "      <td>0.943017</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.694194</td>\n",
       "      <td>0.324890</td>\n",
       "      <td>0.369304</td>\n",
       "      <td>0.672138</td>\n",
       "      <td>0.339216</td>\n",
       "      <td>0.332922</td>\n",
       "      <td>0.889817</td>\n",
       "      <td>0.941696</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.693071</td>\n",
       "      <td>0.325491</td>\n",
       "      <td>0.367579</td>\n",
       "      <td>0.681569</td>\n",
       "      <td>0.332770</td>\n",
       "      <td>0.348799</td>\n",
       "      <td>0.892970</td>\n",
       "      <td>0.943459</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.695370</td>\n",
       "      <td>0.324381</td>\n",
       "      <td>0.370990</td>\n",
       "      <td>0.700448</td>\n",
       "      <td>0.354733</td>\n",
       "      <td>0.345715</td>\n",
       "      <td>0.873472</td>\n",
       "      <td>0.932463</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.697445</td>\n",
       "      <td>0.327479</td>\n",
       "      <td>0.369967</td>\n",
       "      <td>0.669491</td>\n",
       "      <td>0.334597</td>\n",
       "      <td>0.334894</td>\n",
       "      <td>0.891343</td>\n",
       "      <td>0.942551</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.323479</td>\n",
       "      <td>0.363363</td>\n",
       "      <td>0.711997</td>\n",
       "      <td>0.359420</td>\n",
       "      <td>0.352577</td>\n",
       "      <td>0.868071</td>\n",
       "      <td>0.929377</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.694017</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.370217</td>\n",
       "      <td>0.667476</td>\n",
       "      <td>0.333653</td>\n",
       "      <td>0.333823</td>\n",
       "      <td>0.894080</td>\n",
       "      <td>0.944078</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.693444</td>\n",
       "      <td>0.324250</td>\n",
       "      <td>0.369193</td>\n",
       "      <td>0.683850</td>\n",
       "      <td>0.343781</td>\n",
       "      <td>0.340069</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.682278</td>\n",
       "      <td>0.317687</td>\n",
       "      <td>0.364591</td>\n",
       "      <td>0.685071</td>\n",
       "      <td>0.351558</td>\n",
       "      <td>0.333513</td>\n",
       "      <td>0.889651</td>\n",
       "      <td>0.941603</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.683484</td>\n",
       "      <td>0.317691</td>\n",
       "      <td>0.365793</td>\n",
       "      <td>0.666408</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.327680</td>\n",
       "      <td>0.893423</td>\n",
       "      <td>0.943712</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.690425</td>\n",
       "      <td>0.323871</td>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.671769</td>\n",
       "      <td>0.333482</td>\n",
       "      <td>0.338287</td>\n",
       "      <td>0.892196</td>\n",
       "      <td>0.943027</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.682670</td>\n",
       "      <td>0.318726</td>\n",
       "      <td>0.363944</td>\n",
       "      <td>0.681661</td>\n",
       "      <td>0.345934</td>\n",
       "      <td>0.335727</td>\n",
       "      <td>0.890243</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.688925</td>\n",
       "      <td>0.323235</td>\n",
       "      <td>0.365690</td>\n",
       "      <td>0.705247</td>\n",
       "      <td>0.343372</td>\n",
       "      <td>0.361874</td>\n",
       "      <td>0.880826</td>\n",
       "      <td>0.936638</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.681091</td>\n",
       "      <td>0.316453</td>\n",
       "      <td>0.364638</td>\n",
       "      <td>0.671502</td>\n",
       "      <td>0.339783</td>\n",
       "      <td>0.331720</td>\n",
       "      <td>0.892417</td>\n",
       "      <td>0.943150</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.681016</td>\n",
       "      <td>0.317117</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.676703</td>\n",
       "      <td>0.340456</td>\n",
       "      <td>0.336247</td>\n",
       "      <td>0.891437</td>\n",
       "      <td>0.942603</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.684884</td>\n",
       "      <td>0.318338</td>\n",
       "      <td>0.366547</td>\n",
       "      <td>0.685122</td>\n",
       "      <td>0.343319</td>\n",
       "      <td>0.341803</td>\n",
       "      <td>0.891267</td>\n",
       "      <td>0.942508</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.685514</td>\n",
       "      <td>0.319915</td>\n",
       "      <td>0.365599</td>\n",
       "      <td>0.684227</td>\n",
       "      <td>0.346918</td>\n",
       "      <td>0.337309</td>\n",
       "      <td>0.890746</td>\n",
       "      <td>0.942216</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.684023</td>\n",
       "      <td>0.318983</td>\n",
       "      <td>0.365039</td>\n",
       "      <td>0.680283</td>\n",
       "      <td>0.344154</td>\n",
       "      <td>0.336129</td>\n",
       "      <td>0.883752</td>\n",
       "      <td>0.938289</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.682529</td>\n",
       "      <td>0.318514</td>\n",
       "      <td>0.364015</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.339240</td>\n",
       "      <td>0.334260</td>\n",
       "      <td>0.888494</td>\n",
       "      <td>0.940955</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.677631</td>\n",
       "      <td>0.315577</td>\n",
       "      <td>0.362054</td>\n",
       "      <td>0.685676</td>\n",
       "      <td>0.340775</td>\n",
       "      <td>0.344901</td>\n",
       "      <td>0.886160</td>\n",
       "      <td>0.939645</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>0.315004</td>\n",
       "      <td>0.362111</td>\n",
       "      <td>0.676232</td>\n",
       "      <td>0.341940</td>\n",
       "      <td>0.334292</td>\n",
       "      <td>0.889518</td>\n",
       "      <td>0.941529</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.674437</td>\n",
       "      <td>0.313080</td>\n",
       "      <td>0.361357</td>\n",
       "      <td>0.682138</td>\n",
       "      <td>0.341082</td>\n",
       "      <td>0.341056</td>\n",
       "      <td>0.885258</td>\n",
       "      <td>0.939137</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.677893</td>\n",
       "      <td>0.315611</td>\n",
       "      <td>0.362282</td>\n",
       "      <td>0.671489</td>\n",
       "      <td>0.339318</td>\n",
       "      <td>0.332170</td>\n",
       "      <td>0.889651</td>\n",
       "      <td>0.941604</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.677952</td>\n",
       "      <td>0.314756</td>\n",
       "      <td>0.363196</td>\n",
       "      <td>0.672298</td>\n",
       "      <td>0.337124</td>\n",
       "      <td>0.335174</td>\n",
       "      <td>0.891824</td>\n",
       "      <td>0.942819</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.673427</td>\n",
       "      <td>0.313596</td>\n",
       "      <td>0.359832</td>\n",
       "      <td>0.666940</td>\n",
       "      <td>0.335044</td>\n",
       "      <td>0.331897</td>\n",
       "      <td>0.893288</td>\n",
       "      <td>0.943637</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_coeff value: 0.8083961781139939.\n",
      "Better model found at epoch 1 with jaccard_coeff value: 0.8388195318435332.\n",
      "Better model found at epoch 2 with jaccard_coeff value: 0.8517147960486074.\n",
      "Better model found at epoch 3 with jaccard_coeff value: 0.862816800232352.\n",
      "Better model found at epoch 4 with jaccard_coeff value: 0.8692650853300034.\n",
      "Better model found at epoch 7 with jaccard_coeff value: 0.8714037739405738.\n",
      "Better model found at epoch 12 with jaccard_coeff value: 0.8760318098686927.\n",
      "Better model found at epoch 15 with jaccard_coeff value: 0.8765502685478693.\n",
      "Better model found at epoch 21 with jaccard_coeff value: 0.8839532052054068.\n",
      "Better model found at epoch 28 with jaccard_coeff value: 0.8882379723540135.\n",
      "Better model found at epoch 37 with jaccard_coeff value: 0.8893826109832446.\n",
      "Better model found at epoch 40 with jaccard_coeff value: 0.8913514831350379.\n",
      "Better model found at epoch 43 with jaccard_coeff value: 0.8936338380341151.\n",
      "Better model found at epoch 61 with jaccard_coeff value: 0.8940795371337782.\n",
      "Fold 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ls_cross_entropy</th>\n",
       "      <th>train_dice_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_ls_cross_entropy</th>\n",
       "      <th>valid_dice_loss</th>\n",
       "      <th>jaccard_coeff</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.899072</td>\n",
       "      <td>0.442295</td>\n",
       "      <td>0.456777</td>\n",
       "      <td>0.815743</td>\n",
       "      <td>0.398656</td>\n",
       "      <td>0.417087</td>\n",
       "      <td>0.822744</td>\n",
       "      <td>0.902753</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.850483</td>\n",
       "      <td>0.411655</td>\n",
       "      <td>0.438827</td>\n",
       "      <td>0.765283</td>\n",
       "      <td>0.382074</td>\n",
       "      <td>0.383208</td>\n",
       "      <td>0.846483</td>\n",
       "      <td>0.916860</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.810104</td>\n",
       "      <td>0.388480</td>\n",
       "      <td>0.421624</td>\n",
       "      <td>0.747886</td>\n",
       "      <td>0.384359</td>\n",
       "      <td>0.363526</td>\n",
       "      <td>0.852337</td>\n",
       "      <td>0.920283</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.798326</td>\n",
       "      <td>0.383152</td>\n",
       "      <td>0.415174</td>\n",
       "      <td>0.735205</td>\n",
       "      <td>0.365466</td>\n",
       "      <td>0.369738</td>\n",
       "      <td>0.864138</td>\n",
       "      <td>0.927118</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.777636</td>\n",
       "      <td>0.372741</td>\n",
       "      <td>0.404895</td>\n",
       "      <td>0.712231</td>\n",
       "      <td>0.367513</td>\n",
       "      <td>0.344718</td>\n",
       "      <td>0.865216</td>\n",
       "      <td>0.927738</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.367913</td>\n",
       "      <td>0.400192</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>0.340761</td>\n",
       "      <td>0.357062</td>\n",
       "      <td>0.877013</td>\n",
       "      <td>0.934477</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.771683</td>\n",
       "      <td>0.370969</td>\n",
       "      <td>0.400713</td>\n",
       "      <td>0.696219</td>\n",
       "      <td>0.346859</td>\n",
       "      <td>0.349359</td>\n",
       "      <td>0.877261</td>\n",
       "      <td>0.934618</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.767871</td>\n",
       "      <td>0.365263</td>\n",
       "      <td>0.402609</td>\n",
       "      <td>0.688384</td>\n",
       "      <td>0.345177</td>\n",
       "      <td>0.343207</td>\n",
       "      <td>0.880898</td>\n",
       "      <td>0.936678</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.759407</td>\n",
       "      <td>0.361741</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>0.690505</td>\n",
       "      <td>0.344150</td>\n",
       "      <td>0.346355</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.935811</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.756870</td>\n",
       "      <td>0.361906</td>\n",
       "      <td>0.394964</td>\n",
       "      <td>0.688058</td>\n",
       "      <td>0.338281</td>\n",
       "      <td>0.349777</td>\n",
       "      <td>0.882033</td>\n",
       "      <td>0.937319</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.745686</td>\n",
       "      <td>0.353910</td>\n",
       "      <td>0.391776</td>\n",
       "      <td>0.689167</td>\n",
       "      <td>0.346706</td>\n",
       "      <td>0.342461</td>\n",
       "      <td>0.877537</td>\n",
       "      <td>0.934775</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.744423</td>\n",
       "      <td>0.353281</td>\n",
       "      <td>0.391142</td>\n",
       "      <td>0.782543</td>\n",
       "      <td>0.396016</td>\n",
       "      <td>0.386526</td>\n",
       "      <td>0.845750</td>\n",
       "      <td>0.916430</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.736478</td>\n",
       "      <td>0.347297</td>\n",
       "      <td>0.389181</td>\n",
       "      <td>0.675079</td>\n",
       "      <td>0.336879</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.888329</td>\n",
       "      <td>0.940863</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.744330</td>\n",
       "      <td>0.355459</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>0.688731</td>\n",
       "      <td>0.339250</td>\n",
       "      <td>0.349481</td>\n",
       "      <td>0.883830</td>\n",
       "      <td>0.938333</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.746019</td>\n",
       "      <td>0.355004</td>\n",
       "      <td>0.391015</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.347512</td>\n",
       "      <td>0.371508</td>\n",
       "      <td>0.874421</td>\n",
       "      <td>0.933004</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.741735</td>\n",
       "      <td>0.350449</td>\n",
       "      <td>0.391286</td>\n",
       "      <td>0.677322</td>\n",
       "      <td>0.341607</td>\n",
       "      <td>0.335715</td>\n",
       "      <td>0.883448</td>\n",
       "      <td>0.938118</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.740470</td>\n",
       "      <td>0.350794</td>\n",
       "      <td>0.389676</td>\n",
       "      <td>0.702327</td>\n",
       "      <td>0.349943</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.883984</td>\n",
       "      <td>0.938420</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.736655</td>\n",
       "      <td>0.349493</td>\n",
       "      <td>0.387162</td>\n",
       "      <td>0.696733</td>\n",
       "      <td>0.351473</td>\n",
       "      <td>0.345260</td>\n",
       "      <td>0.883020</td>\n",
       "      <td>0.937876</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.724461</td>\n",
       "      <td>0.341651</td>\n",
       "      <td>0.382810</td>\n",
       "      <td>0.681384</td>\n",
       "      <td>0.348132</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>0.883887</td>\n",
       "      <td>0.938365</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.738346</td>\n",
       "      <td>0.349858</td>\n",
       "      <td>0.388488</td>\n",
       "      <td>0.688211</td>\n",
       "      <td>0.335820</td>\n",
       "      <td>0.352391</td>\n",
       "      <td>0.885510</td>\n",
       "      <td>0.939279</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.730515</td>\n",
       "      <td>0.344681</td>\n",
       "      <td>0.385834</td>\n",
       "      <td>0.717876</td>\n",
       "      <td>0.370455</td>\n",
       "      <td>0.347421</td>\n",
       "      <td>0.866055</td>\n",
       "      <td>0.928220</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.726310</td>\n",
       "      <td>0.344582</td>\n",
       "      <td>0.381728</td>\n",
       "      <td>0.669997</td>\n",
       "      <td>0.330707</td>\n",
       "      <td>0.339289</td>\n",
       "      <td>0.889924</td>\n",
       "      <td>0.941756</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.729012</td>\n",
       "      <td>0.344694</td>\n",
       "      <td>0.384318</td>\n",
       "      <td>0.684380</td>\n",
       "      <td>0.342906</td>\n",
       "      <td>0.341474</td>\n",
       "      <td>0.887165</td>\n",
       "      <td>0.940209</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.733897</td>\n",
       "      <td>0.345746</td>\n",
       "      <td>0.388151</td>\n",
       "      <td>0.684486</td>\n",
       "      <td>0.351868</td>\n",
       "      <td>0.332618</td>\n",
       "      <td>0.883273</td>\n",
       "      <td>0.938019</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.724307</td>\n",
       "      <td>0.341429</td>\n",
       "      <td>0.382878</td>\n",
       "      <td>0.673951</td>\n",
       "      <td>0.332451</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.886575</td>\n",
       "      <td>0.939878</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.726319</td>\n",
       "      <td>0.346465</td>\n",
       "      <td>0.379854</td>\n",
       "      <td>0.665916</td>\n",
       "      <td>0.328779</td>\n",
       "      <td>0.337137</td>\n",
       "      <td>0.891846</td>\n",
       "      <td>0.942831</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.722459</td>\n",
       "      <td>0.341851</td>\n",
       "      <td>0.380608</td>\n",
       "      <td>0.700676</td>\n",
       "      <td>0.347797</td>\n",
       "      <td>0.352879</td>\n",
       "      <td>0.878654</td>\n",
       "      <td>0.935408</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.725220</td>\n",
       "      <td>0.341398</td>\n",
       "      <td>0.383821</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>0.329170</td>\n",
       "      <td>0.335531</td>\n",
       "      <td>0.890293</td>\n",
       "      <td>0.941963</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.726848</td>\n",
       "      <td>0.342563</td>\n",
       "      <td>0.384285</td>\n",
       "      <td>0.696238</td>\n",
       "      <td>0.357857</td>\n",
       "      <td>0.338381</td>\n",
       "      <td>0.877043</td>\n",
       "      <td>0.934494</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.721680</td>\n",
       "      <td>0.340292</td>\n",
       "      <td>0.381388</td>\n",
       "      <td>0.694590</td>\n",
       "      <td>0.338487</td>\n",
       "      <td>0.356104</td>\n",
       "      <td>0.880523</td>\n",
       "      <td>0.936466</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.715797</td>\n",
       "      <td>0.336299</td>\n",
       "      <td>0.379498</td>\n",
       "      <td>0.678323</td>\n",
       "      <td>0.342888</td>\n",
       "      <td>0.335435</td>\n",
       "      <td>0.883355</td>\n",
       "      <td>0.938065</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.716676</td>\n",
       "      <td>0.339542</td>\n",
       "      <td>0.377134</td>\n",
       "      <td>0.662195</td>\n",
       "      <td>0.332462</td>\n",
       "      <td>0.329733</td>\n",
       "      <td>0.892293</td>\n",
       "      <td>0.943081</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.713220</td>\n",
       "      <td>0.335308</td>\n",
       "      <td>0.377912</td>\n",
       "      <td>0.663902</td>\n",
       "      <td>0.337327</td>\n",
       "      <td>0.326575</td>\n",
       "      <td>0.892963</td>\n",
       "      <td>0.943455</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.720730</td>\n",
       "      <td>0.339438</td>\n",
       "      <td>0.381292</td>\n",
       "      <td>0.665233</td>\n",
       "      <td>0.334191</td>\n",
       "      <td>0.331041</td>\n",
       "      <td>0.891056</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.717191</td>\n",
       "      <td>0.337456</td>\n",
       "      <td>0.379735</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.325876</td>\n",
       "      <td>0.339997</td>\n",
       "      <td>0.894563</td>\n",
       "      <td>0.944347</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.717197</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>0.381657</td>\n",
       "      <td>0.682390</td>\n",
       "      <td>0.349466</td>\n",
       "      <td>0.332924</td>\n",
       "      <td>0.885977</td>\n",
       "      <td>0.939542</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.719828</td>\n",
       "      <td>0.339088</td>\n",
       "      <td>0.380739</td>\n",
       "      <td>0.670541</td>\n",
       "      <td>0.326959</td>\n",
       "      <td>0.343582</td>\n",
       "      <td>0.893467</td>\n",
       "      <td>0.943736</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.708954</td>\n",
       "      <td>0.332775</td>\n",
       "      <td>0.376179</td>\n",
       "      <td>0.707993</td>\n",
       "      <td>0.352427</td>\n",
       "      <td>0.355566</td>\n",
       "      <td>0.877530</td>\n",
       "      <td>0.934771</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.722455</td>\n",
       "      <td>0.340710</td>\n",
       "      <td>0.381746</td>\n",
       "      <td>0.675977</td>\n",
       "      <td>0.337855</td>\n",
       "      <td>0.338122</td>\n",
       "      <td>0.889095</td>\n",
       "      <td>0.941292</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.715911</td>\n",
       "      <td>0.338521</td>\n",
       "      <td>0.377391</td>\n",
       "      <td>0.683134</td>\n",
       "      <td>0.349825</td>\n",
       "      <td>0.333309</td>\n",
       "      <td>0.886256</td>\n",
       "      <td>0.939698</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.711288</td>\n",
       "      <td>0.336603</td>\n",
       "      <td>0.374686</td>\n",
       "      <td>0.663952</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.333389</td>\n",
       "      <td>0.893842</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.714746</td>\n",
       "      <td>0.335381</td>\n",
       "      <td>0.379365</td>\n",
       "      <td>0.671010</td>\n",
       "      <td>0.336407</td>\n",
       "      <td>0.334604</td>\n",
       "      <td>0.884365</td>\n",
       "      <td>0.938635</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.718951</td>\n",
       "      <td>0.340150</td>\n",
       "      <td>0.378801</td>\n",
       "      <td>0.672866</td>\n",
       "      <td>0.340229</td>\n",
       "      <td>0.332637</td>\n",
       "      <td>0.886461</td>\n",
       "      <td>0.939814</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.332503</td>\n",
       "      <td>0.374164</td>\n",
       "      <td>0.654866</td>\n",
       "      <td>0.326159</td>\n",
       "      <td>0.328708</td>\n",
       "      <td>0.898073</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.704210</td>\n",
       "      <td>0.328226</td>\n",
       "      <td>0.375984</td>\n",
       "      <td>0.668545</td>\n",
       "      <td>0.344403</td>\n",
       "      <td>0.324142</td>\n",
       "      <td>0.894128</td>\n",
       "      <td>0.944105</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.717076</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.379048</td>\n",
       "      <td>0.651748</td>\n",
       "      <td>0.320054</td>\n",
       "      <td>0.331695</td>\n",
       "      <td>0.899095</td>\n",
       "      <td>0.946867</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.338122</td>\n",
       "      <td>0.377601</td>\n",
       "      <td>0.659157</td>\n",
       "      <td>0.327147</td>\n",
       "      <td>0.332010</td>\n",
       "      <td>0.894887</td>\n",
       "      <td>0.944528</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.712594</td>\n",
       "      <td>0.334792</td>\n",
       "      <td>0.377802</td>\n",
       "      <td>0.658423</td>\n",
       "      <td>0.324079</td>\n",
       "      <td>0.334344</td>\n",
       "      <td>0.896520</td>\n",
       "      <td>0.945437</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.709556</td>\n",
       "      <td>0.332153</td>\n",
       "      <td>0.377402</td>\n",
       "      <td>0.653583</td>\n",
       "      <td>0.326285</td>\n",
       "      <td>0.327298</td>\n",
       "      <td>0.896556</td>\n",
       "      <td>0.945457</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.708111</td>\n",
       "      <td>0.332484</td>\n",
       "      <td>0.375627</td>\n",
       "      <td>0.657412</td>\n",
       "      <td>0.329034</td>\n",
       "      <td>0.328378</td>\n",
       "      <td>0.896003</td>\n",
       "      <td>0.945149</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.709646</td>\n",
       "      <td>0.333899</td>\n",
       "      <td>0.375747</td>\n",
       "      <td>0.661493</td>\n",
       "      <td>0.325038</td>\n",
       "      <td>0.336456</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.945912</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.706288</td>\n",
       "      <td>0.333535</td>\n",
       "      <td>0.372753</td>\n",
       "      <td>0.654335</td>\n",
       "      <td>0.320543</td>\n",
       "      <td>0.333793</td>\n",
       "      <td>0.897973</td>\n",
       "      <td>0.946244</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.709586</td>\n",
       "      <td>0.335899</td>\n",
       "      <td>0.373687</td>\n",
       "      <td>0.679863</td>\n",
       "      <td>0.341103</td>\n",
       "      <td>0.338760</td>\n",
       "      <td>0.894414</td>\n",
       "      <td>0.944264</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.703401</td>\n",
       "      <td>0.330270</td>\n",
       "      <td>0.373131</td>\n",
       "      <td>0.654539</td>\n",
       "      <td>0.328769</td>\n",
       "      <td>0.325770</td>\n",
       "      <td>0.895590</td>\n",
       "      <td>0.944920</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.698280</td>\n",
       "      <td>0.327044</td>\n",
       "      <td>0.371236</td>\n",
       "      <td>0.646807</td>\n",
       "      <td>0.323541</td>\n",
       "      <td>0.323266</td>\n",
       "      <td>0.899772</td>\n",
       "      <td>0.947242</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.704881</td>\n",
       "      <td>0.329133</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>0.664194</td>\n",
       "      <td>0.325502</td>\n",
       "      <td>0.338692</td>\n",
       "      <td>0.893705</td>\n",
       "      <td>0.943869</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.697886</td>\n",
       "      <td>0.325846</td>\n",
       "      <td>0.372040</td>\n",
       "      <td>0.655465</td>\n",
       "      <td>0.333751</td>\n",
       "      <td>0.321714</td>\n",
       "      <td>0.898979</td>\n",
       "      <td>0.946802</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.704018</td>\n",
       "      <td>0.330958</td>\n",
       "      <td>0.373061</td>\n",
       "      <td>0.700985</td>\n",
       "      <td>0.355797</td>\n",
       "      <td>0.345188</td>\n",
       "      <td>0.884871</td>\n",
       "      <td>0.938919</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.703050</td>\n",
       "      <td>0.328712</td>\n",
       "      <td>0.374338</td>\n",
       "      <td>0.661739</td>\n",
       "      <td>0.328764</td>\n",
       "      <td>0.332976</td>\n",
       "      <td>0.893641</td>\n",
       "      <td>0.943834</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.692809</td>\n",
       "      <td>0.322972</td>\n",
       "      <td>0.369837</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.339769</td>\n",
       "      <td>0.327289</td>\n",
       "      <td>0.894758</td>\n",
       "      <td>0.944456</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.699280</td>\n",
       "      <td>0.328221</td>\n",
       "      <td>0.371058</td>\n",
       "      <td>0.676359</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.341232</td>\n",
       "      <td>0.888678</td>\n",
       "      <td>0.941058</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.697748</td>\n",
       "      <td>0.328079</td>\n",
       "      <td>0.369669</td>\n",
       "      <td>0.686518</td>\n",
       "      <td>0.337967</td>\n",
       "      <td>0.348552</td>\n",
       "      <td>0.893472</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.332296</td>\n",
       "      <td>0.371930</td>\n",
       "      <td>0.651577</td>\n",
       "      <td>0.319642</td>\n",
       "      <td>0.331935</td>\n",
       "      <td>0.902203</td>\n",
       "      <td>0.948587</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.694168</td>\n",
       "      <td>0.323489</td>\n",
       "      <td>0.370679</td>\n",
       "      <td>0.649216</td>\n",
       "      <td>0.323222</td>\n",
       "      <td>0.325995</td>\n",
       "      <td>0.901670</td>\n",
       "      <td>0.948293</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.697114</td>\n",
       "      <td>0.327327</td>\n",
       "      <td>0.369787</td>\n",
       "      <td>0.669117</td>\n",
       "      <td>0.341357</td>\n",
       "      <td>0.327759</td>\n",
       "      <td>0.895019</td>\n",
       "      <td>0.944602</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.689992</td>\n",
       "      <td>0.323387</td>\n",
       "      <td>0.366605</td>\n",
       "      <td>0.660482</td>\n",
       "      <td>0.330966</td>\n",
       "      <td>0.329517</td>\n",
       "      <td>0.898160</td>\n",
       "      <td>0.946348</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>0.323311</td>\n",
       "      <td>0.368507</td>\n",
       "      <td>0.661301</td>\n",
       "      <td>0.330697</td>\n",
       "      <td>0.330605</td>\n",
       "      <td>0.896725</td>\n",
       "      <td>0.945551</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.691947</td>\n",
       "      <td>0.322963</td>\n",
       "      <td>0.368983</td>\n",
       "      <td>0.689763</td>\n",
       "      <td>0.345001</td>\n",
       "      <td>0.344763</td>\n",
       "      <td>0.887861</td>\n",
       "      <td>0.940600</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.687559</td>\n",
       "      <td>0.318747</td>\n",
       "      <td>0.368812</td>\n",
       "      <td>0.642221</td>\n",
       "      <td>0.319482</td>\n",
       "      <td>0.322739</td>\n",
       "      <td>0.903996</td>\n",
       "      <td>0.949578</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.689193</td>\n",
       "      <td>0.320816</td>\n",
       "      <td>0.368378</td>\n",
       "      <td>0.669288</td>\n",
       "      <td>0.340926</td>\n",
       "      <td>0.328363</td>\n",
       "      <td>0.895724</td>\n",
       "      <td>0.944994</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.686155</td>\n",
       "      <td>0.316947</td>\n",
       "      <td>0.369208</td>\n",
       "      <td>0.677732</td>\n",
       "      <td>0.345655</td>\n",
       "      <td>0.332077</td>\n",
       "      <td>0.893731</td>\n",
       "      <td>0.943884</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.690512</td>\n",
       "      <td>0.321697</td>\n",
       "      <td>0.368815</td>\n",
       "      <td>0.646050</td>\n",
       "      <td>0.318745</td>\n",
       "      <td>0.327304</td>\n",
       "      <td>0.904208</td>\n",
       "      <td>0.949694</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.683047</td>\n",
       "      <td>0.316385</td>\n",
       "      <td>0.366661</td>\n",
       "      <td>0.642287</td>\n",
       "      <td>0.321568</td>\n",
       "      <td>0.320719</td>\n",
       "      <td>0.905089</td>\n",
       "      <td>0.950180</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.689508</td>\n",
       "      <td>0.322088</td>\n",
       "      <td>0.367420</td>\n",
       "      <td>0.677577</td>\n",
       "      <td>0.344311</td>\n",
       "      <td>0.333266</td>\n",
       "      <td>0.894463</td>\n",
       "      <td>0.944292</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.683367</td>\n",
       "      <td>0.317250</td>\n",
       "      <td>0.366116</td>\n",
       "      <td>0.678143</td>\n",
       "      <td>0.346901</td>\n",
       "      <td>0.331242</td>\n",
       "      <td>0.893515</td>\n",
       "      <td>0.943763</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.685720</td>\n",
       "      <td>0.317712</td>\n",
       "      <td>0.368008</td>\n",
       "      <td>0.657250</td>\n",
       "      <td>0.328581</td>\n",
       "      <td>0.328669</td>\n",
       "      <td>0.899318</td>\n",
       "      <td>0.946991</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.689916</td>\n",
       "      <td>0.322999</td>\n",
       "      <td>0.366917</td>\n",
       "      <td>0.644645</td>\n",
       "      <td>0.319428</td>\n",
       "      <td>0.325218</td>\n",
       "      <td>0.904643</td>\n",
       "      <td>0.949934</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.683357</td>\n",
       "      <td>0.316303</td>\n",
       "      <td>0.367054</td>\n",
       "      <td>0.664837</td>\n",
       "      <td>0.332162</td>\n",
       "      <td>0.332674</td>\n",
       "      <td>0.895005</td>\n",
       "      <td>0.944594</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.683324</td>\n",
       "      <td>0.317945</td>\n",
       "      <td>0.365379</td>\n",
       "      <td>0.643850</td>\n",
       "      <td>0.321352</td>\n",
       "      <td>0.322498</td>\n",
       "      <td>0.905859</td>\n",
       "      <td>0.950604</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.681056</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0.364435</td>\n",
       "      <td>0.647816</td>\n",
       "      <td>0.324390</td>\n",
       "      <td>0.323425</td>\n",
       "      <td>0.903147</td>\n",
       "      <td>0.949109</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_coeff value: 0.8227441089833312.\n",
      "Better model found at epoch 1 with jaccard_coeff value: 0.8464834866307889.\n",
      "Better model found at epoch 2 with jaccard_coeff value: 0.8523365431642373.\n",
      "Better model found at epoch 3 with jaccard_coeff value: 0.8641379198988882.\n",
      "Better model found at epoch 4 with jaccard_coeff value: 0.8652161391932162.\n",
      "Better model found at epoch 5 with jaccard_coeff value: 0.8770127844306845.\n",
      "Better model found at epoch 6 with jaccard_coeff value: 0.8772606238648007.\n",
      "Better model found at epoch 7 with jaccard_coeff value: 0.8808982234263589.\n",
      "Better model found at epoch 9 with jaccard_coeff value: 0.8820331783701998.\n",
      "Better model found at epoch 12 with jaccard_coeff value: 0.8883293904168177.\n",
      "Better model found at epoch 21 with jaccard_coeff value: 0.8899241336528324.\n",
      "Better model found at epoch 25 with jaccard_coeff value: 0.8918456044275582.\n",
      "Better model found at epoch 31 with jaccard_coeff value: 0.8922930020496306.\n",
      "Better model found at epoch 32 with jaccard_coeff value: 0.8929631999614088.\n",
      "Better model found at epoch 34 with jaccard_coeff value: 0.8945625380731207.\n",
      "Better model found at epoch 43 with jaccard_coeff value: 0.8980729155979513.\n",
      "Better model found at epoch 45 with jaccard_coeff value: 0.8990953822108356.\n",
      "Better model found at epoch 54 with jaccard_coeff value: 0.8997719814423518.\n",
      "Better model found at epoch 62 with jaccard_coeff value: 0.9022029731182571.\n",
      "Better model found at epoch 68 with jaccard_coeff value: 0.9039963804406548.\n",
      "Better model found at epoch 71 with jaccard_coeff value: 0.9042076708297703.\n",
      "Better model found at epoch 72 with jaccard_coeff value: 0.9050886718834059.\n",
      "Better model found at epoch 78 with jaccard_coeff value: 0.9058586300094931.\n",
      "Fold 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ls_cross_entropy</th>\n",
       "      <th>train_dice_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_ls_cross_entropy</th>\n",
       "      <th>valid_dice_loss</th>\n",
       "      <th>jaccard_coeff</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.873319</td>\n",
       "      <td>0.427019</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.829144</td>\n",
       "      <td>0.410350</td>\n",
       "      <td>0.418794</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>0.898022</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.828578</td>\n",
       "      <td>0.398812</td>\n",
       "      <td>0.429766</td>\n",
       "      <td>0.807554</td>\n",
       "      <td>0.401059</td>\n",
       "      <td>0.406496</td>\n",
       "      <td>0.832752</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.816528</td>\n",
       "      <td>0.393018</td>\n",
       "      <td>0.423510</td>\n",
       "      <td>0.802161</td>\n",
       "      <td>0.404521</td>\n",
       "      <td>0.397640</td>\n",
       "      <td>0.843668</td>\n",
       "      <td>0.915206</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.785320</td>\n",
       "      <td>0.374813</td>\n",
       "      <td>0.410507</td>\n",
       "      <td>0.770521</td>\n",
       "      <td>0.393733</td>\n",
       "      <td>0.376788</td>\n",
       "      <td>0.842937</td>\n",
       "      <td>0.914776</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.766293</td>\n",
       "      <td>0.367464</td>\n",
       "      <td>0.398829</td>\n",
       "      <td>0.751863</td>\n",
       "      <td>0.378924</td>\n",
       "      <td>0.372939</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.921484</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.755731</td>\n",
       "      <td>0.360378</td>\n",
       "      <td>0.395352</td>\n",
       "      <td>0.731309</td>\n",
       "      <td>0.367937</td>\n",
       "      <td>0.363372</td>\n",
       "      <td>0.864156</td>\n",
       "      <td>0.927129</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.753979</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.729396</td>\n",
       "      <td>0.362196</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.869313</td>\n",
       "      <td>0.930088</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.746052</td>\n",
       "      <td>0.354974</td>\n",
       "      <td>0.391078</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>0.373186</td>\n",
       "      <td>0.360439</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>0.926894</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>0.353985</td>\n",
       "      <td>0.392064</td>\n",
       "      <td>0.721599</td>\n",
       "      <td>0.361404</td>\n",
       "      <td>0.360194</td>\n",
       "      <td>0.872899</td>\n",
       "      <td>0.932137</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.749152</td>\n",
       "      <td>0.356166</td>\n",
       "      <td>0.392987</td>\n",
       "      <td>0.719753</td>\n",
       "      <td>0.354176</td>\n",
       "      <td>0.365577</td>\n",
       "      <td>0.872936</td>\n",
       "      <td>0.932158</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.750812</td>\n",
       "      <td>0.358651</td>\n",
       "      <td>0.392161</td>\n",
       "      <td>0.827035</td>\n",
       "      <td>0.412311</td>\n",
       "      <td>0.414724</td>\n",
       "      <td>0.826808</td>\n",
       "      <td>0.905194</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.746875</td>\n",
       "      <td>0.356695</td>\n",
       "      <td>0.390181</td>\n",
       "      <td>0.716516</td>\n",
       "      <td>0.353630</td>\n",
       "      <td>0.362886</td>\n",
       "      <td>0.874432</td>\n",
       "      <td>0.933010</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.745265</td>\n",
       "      <td>0.356645</td>\n",
       "      <td>0.388620</td>\n",
       "      <td>0.714489</td>\n",
       "      <td>0.359930</td>\n",
       "      <td>0.354559</td>\n",
       "      <td>0.874881</td>\n",
       "      <td>0.933266</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.740104</td>\n",
       "      <td>0.353026</td>\n",
       "      <td>0.387078</td>\n",
       "      <td>0.707216</td>\n",
       "      <td>0.346774</td>\n",
       "      <td>0.360441</td>\n",
       "      <td>0.879479</td>\n",
       "      <td>0.935876</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.738708</td>\n",
       "      <td>0.354143</td>\n",
       "      <td>0.384565</td>\n",
       "      <td>0.720604</td>\n",
       "      <td>0.363764</td>\n",
       "      <td>0.356840</td>\n",
       "      <td>0.867964</td>\n",
       "      <td>0.929315</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.732938</td>\n",
       "      <td>0.347598</td>\n",
       "      <td>0.385340</td>\n",
       "      <td>0.711605</td>\n",
       "      <td>0.348184</td>\n",
       "      <td>0.363421</td>\n",
       "      <td>0.876708</td>\n",
       "      <td>0.934304</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.733958</td>\n",
       "      <td>0.350089</td>\n",
       "      <td>0.383869</td>\n",
       "      <td>0.717582</td>\n",
       "      <td>0.349319</td>\n",
       "      <td>0.368263</td>\n",
       "      <td>0.880946</td>\n",
       "      <td>0.936705</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.724944</td>\n",
       "      <td>0.343854</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.369551</td>\n",
       "      <td>0.340975</td>\n",
       "      <td>0.872465</td>\n",
       "      <td>0.931889</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.732748</td>\n",
       "      <td>0.347625</td>\n",
       "      <td>0.385123</td>\n",
       "      <td>0.697485</td>\n",
       "      <td>0.347319</td>\n",
       "      <td>0.350166</td>\n",
       "      <td>0.882131</td>\n",
       "      <td>0.937375</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.728654</td>\n",
       "      <td>0.347327</td>\n",
       "      <td>0.381328</td>\n",
       "      <td>0.706165</td>\n",
       "      <td>0.347245</td>\n",
       "      <td>0.358919</td>\n",
       "      <td>0.881379</td>\n",
       "      <td>0.936950</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.724959</td>\n",
       "      <td>0.345483</td>\n",
       "      <td>0.379476</td>\n",
       "      <td>0.722673</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>0.358328</td>\n",
       "      <td>0.876074</td>\n",
       "      <td>0.933944</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.727501</td>\n",
       "      <td>0.345873</td>\n",
       "      <td>0.381627</td>\n",
       "      <td>0.704391</td>\n",
       "      <td>0.348612</td>\n",
       "      <td>0.355779</td>\n",
       "      <td>0.879821</td>\n",
       "      <td>0.936069</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>0.340552</td>\n",
       "      <td>0.379541</td>\n",
       "      <td>0.699616</td>\n",
       "      <td>0.345989</td>\n",
       "      <td>0.353627</td>\n",
       "      <td>0.877265</td>\n",
       "      <td>0.934620</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.716799</td>\n",
       "      <td>0.340788</td>\n",
       "      <td>0.376012</td>\n",
       "      <td>0.695316</td>\n",
       "      <td>0.351621</td>\n",
       "      <td>0.343694</td>\n",
       "      <td>0.881799</td>\n",
       "      <td>0.937187</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.712621</td>\n",
       "      <td>0.336349</td>\n",
       "      <td>0.376272</td>\n",
       "      <td>0.689176</td>\n",
       "      <td>0.346167</td>\n",
       "      <td>0.343009</td>\n",
       "      <td>0.885114</td>\n",
       "      <td>0.939056</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.719502</td>\n",
       "      <td>0.340169</td>\n",
       "      <td>0.379334</td>\n",
       "      <td>0.702069</td>\n",
       "      <td>0.341676</td>\n",
       "      <td>0.360393</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.936984</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.723321</td>\n",
       "      <td>0.343732</td>\n",
       "      <td>0.379589</td>\n",
       "      <td>0.694023</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>0.347277</td>\n",
       "      <td>0.884359</td>\n",
       "      <td>0.938631</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.707812</td>\n",
       "      <td>0.332732</td>\n",
       "      <td>0.375080</td>\n",
       "      <td>0.691922</td>\n",
       "      <td>0.348077</td>\n",
       "      <td>0.343845</td>\n",
       "      <td>0.881715</td>\n",
       "      <td>0.937140</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.717097</td>\n",
       "      <td>0.337941</td>\n",
       "      <td>0.379157</td>\n",
       "      <td>0.689990</td>\n",
       "      <td>0.346142</td>\n",
       "      <td>0.343848</td>\n",
       "      <td>0.886901</td>\n",
       "      <td>0.940061</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.712322</td>\n",
       "      <td>0.335003</td>\n",
       "      <td>0.377319</td>\n",
       "      <td>0.689853</td>\n",
       "      <td>0.338578</td>\n",
       "      <td>0.351274</td>\n",
       "      <td>0.883482</td>\n",
       "      <td>0.938137</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.711676</td>\n",
       "      <td>0.337950</td>\n",
       "      <td>0.373726</td>\n",
       "      <td>0.692191</td>\n",
       "      <td>0.344741</td>\n",
       "      <td>0.347450</td>\n",
       "      <td>0.884349</td>\n",
       "      <td>0.938626</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.722127</td>\n",
       "      <td>0.341280</td>\n",
       "      <td>0.380848</td>\n",
       "      <td>0.718792</td>\n",
       "      <td>0.354421</td>\n",
       "      <td>0.364371</td>\n",
       "      <td>0.876017</td>\n",
       "      <td>0.933912</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.338644</td>\n",
       "      <td>0.376515</td>\n",
       "      <td>0.732452</td>\n",
       "      <td>0.359424</td>\n",
       "      <td>0.373029</td>\n",
       "      <td>0.871415</td>\n",
       "      <td>0.931290</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.713188</td>\n",
       "      <td>0.337750</td>\n",
       "      <td>0.375438</td>\n",
       "      <td>0.696478</td>\n",
       "      <td>0.346386</td>\n",
       "      <td>0.350092</td>\n",
       "      <td>0.882627</td>\n",
       "      <td>0.937654</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.712211</td>\n",
       "      <td>0.337576</td>\n",
       "      <td>0.374635</td>\n",
       "      <td>0.709905</td>\n",
       "      <td>0.360387</td>\n",
       "      <td>0.349518</td>\n",
       "      <td>0.878168</td>\n",
       "      <td>0.935133</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.706131</td>\n",
       "      <td>0.333746</td>\n",
       "      <td>0.372384</td>\n",
       "      <td>0.714455</td>\n",
       "      <td>0.353856</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>0.877606</td>\n",
       "      <td>0.934814</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.708515</td>\n",
       "      <td>0.333167</td>\n",
       "      <td>0.375348</td>\n",
       "      <td>0.709517</td>\n",
       "      <td>0.351668</td>\n",
       "      <td>0.357849</td>\n",
       "      <td>0.874259</td>\n",
       "      <td>0.932912</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.709274</td>\n",
       "      <td>0.336397</td>\n",
       "      <td>0.372877</td>\n",
       "      <td>0.684561</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>0.346998</td>\n",
       "      <td>0.889296</td>\n",
       "      <td>0.941405</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.704232</td>\n",
       "      <td>0.332332</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.684509</td>\n",
       "      <td>0.343525</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>0.887825</td>\n",
       "      <td>0.940580</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.703927</td>\n",
       "      <td>0.329938</td>\n",
       "      <td>0.373989</td>\n",
       "      <td>0.681901</td>\n",
       "      <td>0.339643</td>\n",
       "      <td>0.342258</td>\n",
       "      <td>0.889808</td>\n",
       "      <td>0.941692</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.706703</td>\n",
       "      <td>0.332832</td>\n",
       "      <td>0.373872</td>\n",
       "      <td>0.696118</td>\n",
       "      <td>0.354426</td>\n",
       "      <td>0.341692</td>\n",
       "      <td>0.884978</td>\n",
       "      <td>0.938979</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.709216</td>\n",
       "      <td>0.335620</td>\n",
       "      <td>0.373595</td>\n",
       "      <td>0.684107</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.350008</td>\n",
       "      <td>0.890671</td>\n",
       "      <td>0.942174</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.714280</td>\n",
       "      <td>0.337820</td>\n",
       "      <td>0.376459</td>\n",
       "      <td>0.679252</td>\n",
       "      <td>0.333901</td>\n",
       "      <td>0.345351</td>\n",
       "      <td>0.891576</td>\n",
       "      <td>0.942681</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.712835</td>\n",
       "      <td>0.336327</td>\n",
       "      <td>0.376507</td>\n",
       "      <td>0.718271</td>\n",
       "      <td>0.362731</td>\n",
       "      <td>0.355539</td>\n",
       "      <td>0.875972</td>\n",
       "      <td>0.933886</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.703282</td>\n",
       "      <td>0.331456</td>\n",
       "      <td>0.371826</td>\n",
       "      <td>0.679556</td>\n",
       "      <td>0.336357</td>\n",
       "      <td>0.343199</td>\n",
       "      <td>0.890500</td>\n",
       "      <td>0.942079</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.702274</td>\n",
       "      <td>0.330028</td>\n",
       "      <td>0.372245</td>\n",
       "      <td>0.681120</td>\n",
       "      <td>0.337263</td>\n",
       "      <td>0.343857</td>\n",
       "      <td>0.888542</td>\n",
       "      <td>0.940982</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.707147</td>\n",
       "      <td>0.333032</td>\n",
       "      <td>0.374115</td>\n",
       "      <td>0.684956</td>\n",
       "      <td>0.337061</td>\n",
       "      <td>0.347895</td>\n",
       "      <td>0.889070</td>\n",
       "      <td>0.941278</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.698699</td>\n",
       "      <td>0.329452</td>\n",
       "      <td>0.369247</td>\n",
       "      <td>0.680537</td>\n",
       "      <td>0.336847</td>\n",
       "      <td>0.343690</td>\n",
       "      <td>0.891480</td>\n",
       "      <td>0.942627</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.703910</td>\n",
       "      <td>0.330620</td>\n",
       "      <td>0.373290</td>\n",
       "      <td>0.685491</td>\n",
       "      <td>0.339120</td>\n",
       "      <td>0.346371</td>\n",
       "      <td>0.886317</td>\n",
       "      <td>0.939733</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.696661</td>\n",
       "      <td>0.328894</td>\n",
       "      <td>0.367766</td>\n",
       "      <td>0.766180</td>\n",
       "      <td>0.403862</td>\n",
       "      <td>0.362318</td>\n",
       "      <td>0.819900</td>\n",
       "      <td>0.901039</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.700526</td>\n",
       "      <td>0.330258</td>\n",
       "      <td>0.370268</td>\n",
       "      <td>0.685044</td>\n",
       "      <td>0.340356</td>\n",
       "      <td>0.344689</td>\n",
       "      <td>0.888231</td>\n",
       "      <td>0.940807</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.699713</td>\n",
       "      <td>0.328799</td>\n",
       "      <td>0.370914</td>\n",
       "      <td>0.686815</td>\n",
       "      <td>0.339164</td>\n",
       "      <td>0.347651</td>\n",
       "      <td>0.889061</td>\n",
       "      <td>0.941273</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.700360</td>\n",
       "      <td>0.331044</td>\n",
       "      <td>0.369316</td>\n",
       "      <td>0.686438</td>\n",
       "      <td>0.336755</td>\n",
       "      <td>0.349683</td>\n",
       "      <td>0.889495</td>\n",
       "      <td>0.941516</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.695874</td>\n",
       "      <td>0.327113</td>\n",
       "      <td>0.368761</td>\n",
       "      <td>0.703402</td>\n",
       "      <td>0.344758</td>\n",
       "      <td>0.358645</td>\n",
       "      <td>0.881129</td>\n",
       "      <td>0.936808</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.691841</td>\n",
       "      <td>0.327438</td>\n",
       "      <td>0.364403</td>\n",
       "      <td>0.693334</td>\n",
       "      <td>0.344872</td>\n",
       "      <td>0.348462</td>\n",
       "      <td>0.882332</td>\n",
       "      <td>0.937488</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.695218</td>\n",
       "      <td>0.326796</td>\n",
       "      <td>0.368422</td>\n",
       "      <td>0.670841</td>\n",
       "      <td>0.332144</td>\n",
       "      <td>0.338697</td>\n",
       "      <td>0.896470</td>\n",
       "      <td>0.945409</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.693899</td>\n",
       "      <td>0.326271</td>\n",
       "      <td>0.367628</td>\n",
       "      <td>0.736802</td>\n",
       "      <td>0.368290</td>\n",
       "      <td>0.368512</td>\n",
       "      <td>0.863377</td>\n",
       "      <td>0.926680</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.701564</td>\n",
       "      <td>0.329162</td>\n",
       "      <td>0.372402</td>\n",
       "      <td>0.699395</td>\n",
       "      <td>0.341281</td>\n",
       "      <td>0.358115</td>\n",
       "      <td>0.879427</td>\n",
       "      <td>0.935846</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.690723</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.366999</td>\n",
       "      <td>0.672199</td>\n",
       "      <td>0.333534</td>\n",
       "      <td>0.338665</td>\n",
       "      <td>0.893708</td>\n",
       "      <td>0.943871</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.701374</td>\n",
       "      <td>0.331495</td>\n",
       "      <td>0.369879</td>\n",
       "      <td>0.675353</td>\n",
       "      <td>0.330405</td>\n",
       "      <td>0.344949</td>\n",
       "      <td>0.893604</td>\n",
       "      <td>0.943813</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.695682</td>\n",
       "      <td>0.326184</td>\n",
       "      <td>0.369499</td>\n",
       "      <td>0.688411</td>\n",
       "      <td>0.343302</td>\n",
       "      <td>0.345109</td>\n",
       "      <td>0.885177</td>\n",
       "      <td>0.939092</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.695393</td>\n",
       "      <td>0.327371</td>\n",
       "      <td>0.368022</td>\n",
       "      <td>0.694968</td>\n",
       "      <td>0.346140</td>\n",
       "      <td>0.348827</td>\n",
       "      <td>0.887197</td>\n",
       "      <td>0.940227</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.687793</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.366813</td>\n",
       "      <td>0.669545</td>\n",
       "      <td>0.329825</td>\n",
       "      <td>0.339720</td>\n",
       "      <td>0.896626</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.685147</td>\n",
       "      <td>0.321160</td>\n",
       "      <td>0.363988</td>\n",
       "      <td>0.686441</td>\n",
       "      <td>0.340916</td>\n",
       "      <td>0.345525</td>\n",
       "      <td>0.885901</td>\n",
       "      <td>0.939499</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.690109</td>\n",
       "      <td>0.323617</td>\n",
       "      <td>0.366492</td>\n",
       "      <td>0.668262</td>\n",
       "      <td>0.330054</td>\n",
       "      <td>0.338208</td>\n",
       "      <td>0.896751</td>\n",
       "      <td>0.945565</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.686138</td>\n",
       "      <td>0.321777</td>\n",
       "      <td>0.364361</td>\n",
       "      <td>0.684809</td>\n",
       "      <td>0.338851</td>\n",
       "      <td>0.345958</td>\n",
       "      <td>0.889584</td>\n",
       "      <td>0.941566</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.692509</td>\n",
       "      <td>0.324054</td>\n",
       "      <td>0.368454</td>\n",
       "      <td>0.678683</td>\n",
       "      <td>0.332589</td>\n",
       "      <td>0.346094</td>\n",
       "      <td>0.892807</td>\n",
       "      <td>0.943368</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.688459</td>\n",
       "      <td>0.322255</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.673589</td>\n",
       "      <td>0.332343</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>0.943694</td>\n",
       "      <td>02:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.684935</td>\n",
       "      <td>0.319520</td>\n",
       "      <td>0.365415</td>\n",
       "      <td>0.678704</td>\n",
       "      <td>0.336429</td>\n",
       "      <td>0.342275</td>\n",
       "      <td>0.891897</td>\n",
       "      <td>0.942860</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.680958</td>\n",
       "      <td>0.315345</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.666217</td>\n",
       "      <td>0.332441</td>\n",
       "      <td>0.333776</td>\n",
       "      <td>0.896727</td>\n",
       "      <td>0.945552</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.678783</td>\n",
       "      <td>0.315365</td>\n",
       "      <td>0.363418</td>\n",
       "      <td>0.667168</td>\n",
       "      <td>0.331609</td>\n",
       "      <td>0.335559</td>\n",
       "      <td>0.896798</td>\n",
       "      <td>0.945592</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.685283</td>\n",
       "      <td>0.319710</td>\n",
       "      <td>0.365574</td>\n",
       "      <td>0.668915</td>\n",
       "      <td>0.333397</td>\n",
       "      <td>0.335519</td>\n",
       "      <td>0.895311</td>\n",
       "      <td>0.944764</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.686207</td>\n",
       "      <td>0.322511</td>\n",
       "      <td>0.363695</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.338317</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>0.941736</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.683034</td>\n",
       "      <td>0.318457</td>\n",
       "      <td>0.364578</td>\n",
       "      <td>0.669191</td>\n",
       "      <td>0.330123</td>\n",
       "      <td>0.339069</td>\n",
       "      <td>0.895408</td>\n",
       "      <td>0.944818</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.685820</td>\n",
       "      <td>0.320533</td>\n",
       "      <td>0.365287</td>\n",
       "      <td>0.666706</td>\n",
       "      <td>0.331644</td>\n",
       "      <td>0.335062</td>\n",
       "      <td>0.897187</td>\n",
       "      <td>0.945808</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.317455</td>\n",
       "      <td>0.364116</td>\n",
       "      <td>0.684823</td>\n",
       "      <td>0.344126</td>\n",
       "      <td>0.340696</td>\n",
       "      <td>0.890761</td>\n",
       "      <td>0.942225</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.676671</td>\n",
       "      <td>0.315396</td>\n",
       "      <td>0.361274</td>\n",
       "      <td>0.675798</td>\n",
       "      <td>0.334522</td>\n",
       "      <td>0.341276</td>\n",
       "      <td>0.893909</td>\n",
       "      <td>0.943983</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.683609</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>0.361537</td>\n",
       "      <td>0.666346</td>\n",
       "      <td>0.330746</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.945958</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.681358</td>\n",
       "      <td>0.319670</td>\n",
       "      <td>0.361688</td>\n",
       "      <td>0.674226</td>\n",
       "      <td>0.333373</td>\n",
       "      <td>0.340853</td>\n",
       "      <td>0.894460</td>\n",
       "      <td>0.944290</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.678503</td>\n",
       "      <td>0.318021</td>\n",
       "      <td>0.360481</td>\n",
       "      <td>0.684247</td>\n",
       "      <td>0.341061</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.891140</td>\n",
       "      <td>0.942437</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_coeff value: 0.8149180399769606.\n",
      "Better model found at epoch 1 with jaccard_coeff value: 0.832752471662957.\n",
      "Better model found at epoch 2 with jaccard_coeff value: 0.8436675371663992.\n",
      "Better model found at epoch 4 with jaccard_coeff value: 0.8544000738546504.\n",
      "Better model found at epoch 5 with jaccard_coeff value: 0.8641562840804397.\n",
      "Better model found at epoch 6 with jaccard_coeff value: 0.869312777424237.\n",
      "Better model found at epoch 8 with jaccard_coeff value: 0.8728992300557086.\n",
      "Better model found at epoch 9 with jaccard_coeff value: 0.8729364929771875.\n",
      "Better model found at epoch 11 with jaccard_coeff value: 0.8744316189295928.\n",
      "Better model found at epoch 12 with jaccard_coeff value: 0.8748808521851301.\n",
      "Better model found at epoch 13 with jaccard_coeff value: 0.8794794547320912.\n",
      "Better model found at epoch 16 with jaccard_coeff value: 0.8809463947459112.\n",
      "Better model found at epoch 18 with jaccard_coeff value: 0.8821311189436605.\n",
      "Better model found at epoch 24 with jaccard_coeff value: 0.8851140064621016.\n",
      "Better model found at epoch 28 with jaccard_coeff value: 0.8869006585491294.\n",
      "Better model found at epoch 37 with jaccard_coeff value: 0.8892958483468328.\n",
      "Better model found at epoch 39 with jaccard_coeff value: 0.889808440698543.\n",
      "Better model found at epoch 41 with jaccard_coeff value: 0.8906707563757454.\n",
      "Better model found at epoch 42 with jaccard_coeff value: 0.8915760331698314.\n",
      "Better model found at epoch 55 with jaccard_coeff value: 0.8964696444716828.\n",
      "Better model found at epoch 62 with jaccard_coeff value: 0.8966260888210956.\n",
      "Better model found at epoch 64 with jaccard_coeff value: 0.8967507219315892.\n",
      "Better model found at epoch 70 with jaccard_coeff value: 0.896798152259338.\n",
      "Better model found at epoch 74 with jaccard_coeff value: 0.897186915570051.\n",
      "Better model found at epoch 77 with jaccard_coeff value: 0.8974568049123718.\n"
     ]
    }
   ],
   "source": [
    "bs=64\n",
    "epochs=80\n",
    "lr=1e-3\n",
    "\n",
    "seg_train('DeepLab xSA-ResNeXt50 80e bs64 Multiclass-to-Seg Base', bs=bs, epochs=epochs, lr=lr, arch=xsa_resnext50, \n",
    "          group='xSA-RNXt50 Multiclass-to-Seg 15-to-80e', wd=1e-4, save=True, pct_start=0.5,\n",
    "          batch_tfms=aug_transforms(flip_vert=True, max_rotate=45, max_zoom=1.2, max_warp=0.1, max_lighting=0,\n",
    "                                    xtra_tfms=[ChannelDrop(channels=4), RandomNoise()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
